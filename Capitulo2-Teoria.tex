\documentclass[12pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Bayesian Core - Capítulo 2 - Normal Models},
            pdfauthor={Daysi Febles y Alexander Ramírez},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{{#1}}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Bayesian Core - Capítulo 2 - Normal Models}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Daysi Febles y Alexander Ramírez}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{30/1/2017}

\usepackage{bbm}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{graphicx}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{4}
\tableofcontents
}
\includegraphics{Capitulo2-Teoria_files/figure-latex/unnamed-chunk-2-1.pdf}

\section{2.2 Hablando Bayesiano}\label{hablando-bayesiano}

\subsection{2.2.1 Bases}\label{bases}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Ejercicio 2.5}\label{ejercicio-2.5}

\textbf{Muestre que
\(\pi(\theta|\mathcal{D})=\frac{l(\theta|\mathcal{D})\pi(\theta)}{\int l(\theta|\mathcal{D})\pi(\theta) d\theta}\)
puede ser calculada por el ajuste de \(\theta\) como una variable
aleatoria con función de densidad \(\pi\) y entonces la distribución
condicional \(D\) en \(\theta\) como una distribución de
\(l(\theta|\mathcal{D})\)}

Por el teorema de Bayes tenemos
\[\pi(\theta| \mathcal{D})=\frac{\pi(\theta, \mathcal{D})}{\pi({\mathcal{D}})}\]
Lo que implica

\[\pi(\theta, \mathcal{D})=\pi(\theta| \mathcal{D})\pi({\mathcal{D}})=\pi(\mathcal{D}|\theta)\pi(\theta)\]

Si usamos \(L(\theta|\mathcal{D})\) como estimador de
\(\pi(\theta|\mathcal{D})\) tenemos:
\[\pi(\theta| \mathcal{D})=\frac{\pi(\mathcal{D}|\theta)\pi(\theta)}{\pi({\mathcal{\mathcal{D}}})}\propto L(\mathcal{D}|\theta)\pi(\theta)\]

\begin{quote}
1.1 \textbf{Si suponemos que el cambio relativo en los reportes de
hurtos tienen una media \(\mu\) entre -0.5 y 0.5 y no provee una
información a priori, cambiaremos la priori por una distribución
Uniforme \(\mathcal{U}(-0.5,0.5)\)}
\end{quote}

La distribución a posteriori viene dada por
\[\pi(\theta|\mathcal{D})=\frac{L(\theta|\mathcal{D})\pi(\theta)}{\int L(\theta|\mathcal{D})\pi(\theta)d\theta}\]

Donde asumiremos que los datos \(x_1,x_2,...,x_n\) son normales
\(\mathcal{N}(\mu,\sigma^2)\) iid, entonces la verosimilitud viene dada
por: \[
\begin{array}{rl}
L(\theta|\mathcal{D}) & = f_\theta(x_1)f_\theta(x_2)...f_\theta(x_n)\\
L(\theta|\mathcal{D}) & = \prod_{i=1}^n f_\theta(x_i)\\
L(\theta|\mathcal{D}) & = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}e^{\frac{1}{2\sigma^2}(x_i-\mu)^2}\\
L(\theta|\mathcal{D}) & = \left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2}
\end{array}
\]

Desarrollando \(\sum_{i=1}^n(x_i-\mu)^2\) tenemos \[
\begin{array}{rl}
\sum_{i=1}^n(x_i-\mu)^2 & = \sum_{i=1}^n (x_i^2-2\mu x_i +\mu^2)\\
\sum_{i=1}^n(x_i-\mu)^2 & = \sum_{i=1}^n x_i^2-2\mu \sum_{i=1}^n x_i +\sum_{i=1}^n\mu^2\\
\sum_{i=1}^n(x_i-\mu)^2 & = \sum_{i=1}^n x_i^2-2\mu n\frac{\sum_{i=1}^n x_i}{n} +n\mu^2\\
\sum_{i=1}^n(x_i-\mu)^2 & = \sum_{i=1}^n x_i^2-2\mu n\bar x +n\mu^2\\
\sum_{i=1}^n(x_i-\mu)^2 & = \sum_{i=1}^n x_i^2-2\mu n\bar x +n\mu^2 +n\bar x^2 - n\bar x^2\\
\sum_{i=1}^n(x_i-\mu)^2 & = \sum_{i=1}^n x_i^2+n(-2\mu \bar x +\mu^2 +\bar x^2) - n\bar x^2\\
\sum_{i=1}^n(x_i-\mu)^2 & = \sum_{i=1}^n x_i^2 - n\bar x^2 + n(\mu -\bar x)^2 \\
\end{array}
\]

Sustituyendo esto tenemos: \[
\begin{array}{rl}
L(\theta|\mathcal{D}) & = \left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n e^{-\frac{1}{2\sigma^2}\left(\sum_{i=1}^n x_i^2 - n\bar x^2 + n(\mu -\bar x)^2\right)}\\
L(\theta|\mathcal{D}) & =\left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2 - n\bar x^2 }e^{\frac{1}{2\sigma^2} n(\mu -\bar x)^2}\\
\end{array}
\]

Sustituyendo esta verosimilitud en la distribución a posteriori tenemos:
\[
\begin{array}{rl}
\pi(\theta|\mathcal{D}) & = \displaystyle \frac{\left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2 - n\bar x^2 }e^{-\frac{1}{2\sigma^2} n(\mu -\bar x)^2}\mathbb{1}_{[-0.5,0.5]}}{\int \left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2 - n\bar x^2 }e^{-\frac{1}{2\sigma^2} n(\theta -\bar x)^2}\mathbb{1}_{[-0.5,0.5]}d\theta}\\
\pi(\theta|\mathcal{D}) &= \displaystyle \frac{\left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2 - n\bar x^2 }e^{-\frac{1}{2\sigma^2} n(\mu -\bar x)^2}\mathbb{1}_{[-0.5,0.5]}}{\left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2 - n\bar x^2 }\int_{-0.5}^{0.5} e^{-\frac{1}{2\sigma^2} n(\theta -\bar x)^2}d\theta}\\
\pi(\theta|\mathcal{D}) &=\displaystyle \frac{e^{-\frac{1}{2\sigma^2} n(\mu -\bar x)^2}\mathbb{1}_{[-0.5,0.5]}}{\int_{-0.5}^{0.5} e^{-\frac{1}{2\sigma^2} n(\theta -\bar x)^2}d\theta}\\
\end{array}
\] La cual es una distribución normal truncada.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Ejercicio 2.6}\label{ejercicio-2.6}

\textbf{Muestre que la minimización de \(\hat{\theta}\) en la esperanza
\(E[L(\theta,\hat{\theta})|D]\), es la esperanza de la función de
perdida cuadrática sobre la distribución con función de densidad
\(\pi(\theta|D)\), produce el valor posterior esperado de la solución
\(\hat\theta\)}

La función de perdida se define como
\(L(\theta, \hat{\theta})=\|\theta, \hat{\theta}\|^2\), entonces
tenemos: \[
\begin{array}{rl}
\mathbb{E}(L(\theta, \hat{\theta})|\mathcal{D}) & = \mathbb{E}(\|\theta- \hat{\theta}\|^2|\mathcal{D})\\
\mathbb{E}(L(\theta, \hat{\theta})|\mathcal{D}) & = \mathbb{E}(<\theta-\hat{\theta}><\theta-\hat{\theta}>|\mathcal{D})\\
\mathbb{E}(L(\theta, \hat{\theta})|\mathcal{D}) & = \mathbb{E}(\theta^t\theta-\theta^t\hat{\theta}-\hat{\theta}^t\theta+\hat{\theta}^t\hat{\theta}|\mathcal{D})\\
\mathbb{E}(L(\theta, \hat{\theta})|\mathcal{D}) & = \mathbb{E}(\|\theta\|^2-2\hat{\theta}^t\theta+\|\hat{\theta}\|^2|\mathcal{D})\\
\mathbb{E}(L(\theta, \hat{\theta})|\mathcal{D}) & = \mathbb{E}(\|\theta\|^2|\mathcal{D})-2\mathbb{E}(\hat{\theta}^t\theta|\mathcal{D})+\mathbb{E}(\|\hat{\theta}\|^2|\mathcal{D})\\
\mathbb{E}(L(\theta, \hat{\theta})|\mathcal{D}) & = \| \mathbb{E}(\theta|\mathcal{D})\|^2-2\hat{\theta}^t\mathbb{E}(\theta|\mathcal{D})+\|\hat{\theta}\|^2\\
\mathbb{E}(L(\theta, \hat{\theta})|\mathcal{D}) & = \| \mathbb{E}(\theta|\mathcal{D})-\hat{\theta}\|^2
\end{array}
\]

\subsection{2.2.2 Distribuciones a
Priori}\label{distribuciones-a-priori}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Ejercicio 2.7}\label{ejercicio-2.7}

\textbf{Muestre que la distribución Normal es de la familia exponencial}

La función de densidad de una distribución normal es
\(f_\theta(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}\),
para probar que esta función es de la familia exponencial tenemos que
escribirla de la forma
\[f_\theta(x)=h(x)e^{\theta . R(x)-\psi(\theta)}\] Entonces tenemos: \[
\begin{array}{rl}
f_\theta(x) & =\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}\\
f_\theta(x) & =\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^2}(x^2-2x\mu+\mu^2)}\\
f_\theta(x) & =\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2\sigma^2}(x^2-2x\mu)}e^{-\frac{1}{2\sigma^2} \mu^2}\\
f_\theta(x) & =\frac{1}{\sqrt{2\pi}}\frac{e^{-\frac{1}{2\sigma^2} \mu^2}}{\sigma} e^{-\frac{1}{2\sigma^2}(x^2-2x\mu)}\\
f_\theta(x) & =\frac{1}{\sqrt{2\pi}}\frac{e^{-\frac{1}{2\sigma^2} \mu^2}}{\sigma} e^{-\frac{1}{2\sigma^2}x^2+\frac{1}{2\sigma^2}2x\mu}\\
f_\theta(x) & =\frac{1}{\sqrt{2\pi}}\frac{e^{-\frac{1}{2\sigma^2} \mu^2}}{\sigma} e^{-\frac{1}{2\sigma^2}x^2+\frac{1}{\sigma^2}x\mu}\\
\end{array}
\]

Notemos que: \[
\displaystyle -\frac{1}{2\sigma^2}x^2+\frac{1}{\sigma^2}x\mu = \begin{bmatrix}
-\frac{1}{2\sigma^2} & \frac{\mu}{\sigma^2}
\end{bmatrix} 
\begin{bmatrix}
x^2 \\ x
\end{bmatrix} = \theta. R(x)
\]

\[\displaystyle \frac{e^{-\frac{1}{2\sigma^2} \mu^2}}{\sigma}=\frac{e^{-\frac{1}{2\sigma^2} \mu^2}}{e^{\ln(\sigma)}}= e^{-\frac{1}{2\sigma^2} \mu^2-\ln(\sigma)}=e^{-\left(\frac{1}{2\sigma^2} \mu^2+\ln(\sigma)\right)}= e^{-\psi(\theta)}\]

\[\frac{1}{\sqrt{2\pi}}=h(x)\]

Asi tenemos la normal es de la familia exponencial.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Ejercicio 2.8}\label{ejercicio-2.8}

\textbf{Muestre que para una familia exponencial \(\psi(\theta)\) es
definida por la restricción de que \(f_\theta\) es una función de
densidad y que el valor esperado de esta distribución puede ser escrito
como \(\frac{\partial \psi(\theta)}{\partial \theta}\) el vector de las
derivadasde \(\psi (\theta)\) con respecto a la componente de
\(\theta\)}

Por ser \(f_\theta\) una función de densidad de la familia exponencial
tenemos:

\(\int f_\theta (y) dy = \int h(y)e^{\theta . R(y)-\psi(\theta)} dy =1\)

Así: \[
\begin{array}{rl}
\int h(y)e^{\theta R(y)-\psi(\theta)} dy =1\\
\int h(y)e^{\theta R(y)}e^{-\psi(\theta)} dy =1\\
e^{-\psi(\theta)} \int h(y)e^{\theta R(y)} dy =1\\
e^{\psi(\theta)} = \int h(y)e^{\theta R(y)} dy
\end{array}
\]

\(R(y)\) es un estadístico suficiente tenemos:
\[\mathbb{E}(R(y)) =\int R(y)h(y)e^{\theta . R(y)-\psi(\theta)} dy\]

Observemos que \(\frac{\partial (\theta R(y))}{\partial \theta}=R(y)\),
asi: \[
\begin{array}{rl}
\mathbb{E}(R(y)) & =\displaystyle \int \frac{\partial (\theta R(y))}{\partial \theta} h(y)e^{\theta  R(y)-\psi(\theta)} dy\\
\mathbb{E}(R(y)) & = \displaystyle \int \frac{\partial (\theta R(y)+\psi(\theta)-\psi(\theta))}{\partial \theta} h(y)e^{\theta  R(y)-\psi(\theta)} dy\\
\mathbb{E}(R(y)) & = \displaystyle \int \frac{\partial (\theta R(y)-\psi(\theta))}{\partial \theta} h(y)e^{\theta  R(y)-\psi(\theta)} dy\\
 & +\displaystyle \int \frac{\partial (\psi(\theta))}{\partial \theta} h(y)e^{\theta  R(y)-\psi(\theta)} dy\\
\mathbb{E}(R(y)) & = \displaystyle \int \frac{\partial (\theta R(y)-\psi(\theta))}{\partial \theta} h(y)e^{\theta  R(y)-\psi(\theta)} dy\\
& +\displaystyle \frac{\partial (\psi(\theta))}{\partial \theta} \int h(y)e^{\theta  R(y)-\psi(\theta)} dy\\
\end{array}
\]

Observemos que
\(\frac{\partial (e^{\theta R(y)-\psi(\theta))}}{\partial \theta}=e^{\theta R(y)-\psi(\theta)}\frac{\partial (\theta R(y)-\psi(\theta))}{\partial \theta}\)
asi: \[
\begin{array}{rl}
\mathbb{E}(R(y)) & = \displaystyle \int  h(y) \frac{\partial (e^{\theta R(y)-\psi(\theta)})}{\partial \theta} dy +\displaystyle \frac{\partial (\psi(\theta))}{\partial \theta}  1\\
\mathbb{E}(R(y)) & = \displaystyle \frac{\partial}{\partial \theta} \int  h(y)  e^{\theta R(y)-\psi(\theta)} dy +\displaystyle \frac{\partial (\psi(\theta))}{\partial \theta}\\
\mathbb{E}(R(y))& = \displaystyle \frac{\partial}{\partial \theta} 1 +\displaystyle \frac{\partial (\psi(\theta))}{\partial \theta}\\
\mathbb{E}(R(y))& = \displaystyle \frac{\partial (\psi(\theta))}{\partial \theta}\\
\end{array}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Ejercicio 2.9}\label{ejercicio-2.9}

\textbf{Muestre que los hiperparametros actualizados estan dado por
\(\xi'(y) = \xi+R(y)\) y \(\lambda'(y)=\lambda +1\), encuentre la
expresi?n correspondiente para
\(\pi (\theta | \xi, \lambda, y_1,...,y_n)\)}

Cuando el modelo es de la familia de distribuciones exponencial, existe
una clase genérica de prioris llamadas clase de prioris conjugadas
\(\pi (\theta| \xi, \lambda)\propto e^{\theta.\xi -\lambda\psi(\theta)}\).
Esta distribución a priori parametrizada en \(\theta\) es tal que la
distribución a posteriori tiene la misma forma, veamos esto: \[
\begin{array}{rl}
\pi(\theta|\xi, \lambda) & \propto f_\theta(y)\pi(\theta|\xi,\lambda)\\
& = h(y)e^{\theta R(y)-\psi(\theta)} e^{\theta\xi -\lambda\psi(\theta)}\\
& \propto e^{\theta R(y)-\psi(\theta)} e^{\theta\xi -\lambda\psi(\theta)}\\
& = e^{\theta R(y)-\psi(\theta)+\theta\xi -\lambda\psi(\theta)}\\
& = e^{\theta(R(y)-\xi) -\psi(\theta)(1+\lambda)}\\
& = e^{\theta\xi'(y) -\psi(\theta)\lambda'(y)}
\end{array}
\]

Entonces la distribución a posteriori será: \[
\begin{array}{rl}
\pi (\theta | \xi, \lambda, y_1,...,y_n) & \propto f_\theta(y)\pi(\theta|\xi, \lambda)\\
\pi (\theta | \xi, \lambda, y_1,...,y_n)& = \prod_{i=1}^n h(y_i)e^{\theta R(y_i)-\psi(\theta)}\pi(\theta|\xi, \lambda)\\
\pi (\theta | \xi, \lambda, y_1,...,y_n)& \propto \prod_{i=1}^n e^{\theta R(y_i)-\psi(\theta)}\pi(\theta|\xi, \lambda)\\
\pi (\theta | \xi, \lambda, y_1,...,y_n)& = e^{\sum_{i=1}^n (\theta R(y_i)-\psi(\theta))}\pi(\theta|\xi, \lambda)\\
\pi (\theta | \xi, \lambda, y_1,...,y_n)& = e^{\sum_{i=1}^n (\theta R(y_i)-\psi(\theta))}e^{\theta\xi -\lambda\psi(\theta)}\\
\pi (\theta | \xi, \lambda, y_1,...,y_n)& = e^{\sum_{i=1}^n (\theta R(y_i)-\psi(\theta))+\theta\xi -\lambda\psi(\theta)}\\
\pi (\theta | \xi, \lambda, y_1,...,y_n)& = e^{ \theta\sum_{i=1}^n R(y_i)-n\psi(\theta)+\theta\xi -\lambda\psi(\theta)}\\
\pi (\theta | \xi, \lambda, y_1,...,y_n) & = e^{ \theta(\sum_{i=1}^n R(y_i)+\xi)-\psi(\theta)(n+\lambda)}\\
\end{array}
\]

Asi tenemos que \(\xi(y_1,...,y_n)=\sum_{i=1}^n R(y_i)+\xi\) y
\(\lambda(y_1,...,y_n)=n+\lambda\)
\[\pi (\theta | \xi, \lambda, y_1,...,y_n)\propto e^{ \theta\xi(y_1,...,y_n)-\psi(\theta)\lambda(y_1,...,y_n)}\]

Si tenemos una normal \(\mathcal{N}(\mu,1)\) este es un caso de la
familia exponencial, es decir: \[
\begin{array}{rl}
f_\theta(x)&=\frac{1}{\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2}}\\
f_\theta(x)&=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2-2x\mu+ \mu^2}{2}}\\
f_\theta(x)&=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}+\frac{2x\mu- \mu^2}{2}}\\
f_\theta(x)&=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} e^{x\mu-\frac{\mu^2}{2}}\\
\end{array}
\] donde \(h(x)=\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}\),
\(\psi(\theta)=\frac{\mu^2}{2}\), \(R(x)=x\) y \(\theta=\mu\). Asi
tenemos que la priori puede escribirse como: \[
\begin{array}{rl}
\pi(\mu|\xi,\lambda) & \propto e^{\theta\xi-\lambda\psi(\theta)}\\
\pi(\mu|\xi,\lambda) & = e^{\mu\xi-\lambda\frac{\mu^2}{2}}
\end{array}
\]

Veamos que esto implica que la priori conjugada de \(\mu\) es tambi?n
una normal \(\mathcal{N}(\lambda^{-1}\xi,\lambda^{-1})\) \[
\begin{array}{rl}
\pi(\mu|\xi,\lambda) & \propto  e^{\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda) & =  e^{-\frac{\lambda}{2}[\mu^2-2\mu\frac{\xi}{\lambda} +(\frac{\xi}{\lambda})^2- (\frac{\xi}{\lambda})^2]}\\
\pi(\mu|\xi,\lambda) & =  e^{-\frac{\lambda}{2}[\mu^2-2\mu\frac{\xi}{\lambda} +(\frac{\xi}{\lambda})^2] +\frac{\lambda}{2}(\frac{\xi}{\lambda})^2}\\
\pi(\mu|\xi,\lambda) & =  e^{-\frac{\lambda}{2}(\mu-\frac{\xi}{\lambda})^2} e^{\frac{\lambda}{2}\frac{\xi^2}{\lambda^2}}\\
\pi(\mu|\xi,\lambda) & =  e^{-\frac{1}{2(\frac{1}{\lambda})}(\mu-\frac{\xi}{\lambda})^2} e^{\frac{\xi^2}{2\lambda}}\\
\pi(\mu|\xi,\lambda) & \propto  e^{-\frac{1}{2(\frac{1}{\lambda})}(\mu-\frac{\xi}{\lambda})^2} \\
\end{array}
\] Esto es una normal con media \(\frac{\xi}{\lambda}\) y varianza
\(\frac{1}{\lambda}\).

Para calcular la distribuci?n a priori de \(\mu\) usaremos el teorema de
Bayes, recordemos que estamos suponiendo que
\(X\sim \mathcal{N}(\mu,1)\) por lo tanto \[
\begin{array}{rl}
f_\theta(x) & =\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x-\mu)^2}\\
f_\theta(x) & =\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x^2-2x\mu+\mu^2)}\\
f_\theta(x) & =\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2} e^{\frac{1}{2}(2x\mu-\mu^2)}\\
f_\theta(x) & =\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2} e^{x\mu-\frac{\mu^2}{2}}\\
\end{array}
\]

Por lo tanto la distribución a priori de \(\mu\) es: \[
\begin{array}{rl}
\pi(\mu|x,\xi,\lambda) & \propto \pi(x|\mu)\pi(\mu|\xi,\lambda)\\
\pi(\mu|x,\xi,\lambda) & = e^{x\mu-\frac{\mu^2}{2}}e^{\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|x,\xi,\lambda) & = e^{x\mu-\frac{\mu^2}{2}+\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|x,\xi,\lambda) & = e^{-\frac{\mu^2}{2}(1+\lambda)+ x\mu +\mu\xi}\\
\pi(\mu|x,\xi,\lambda) & = e^{-\frac{(1+\lambda)}{2}[\mu^2- x\mu\frac{2}{1+\lambda} -\mu\xi\frac{2}{1+\lambda}]}\\
\pi(\mu|x,\xi,\lambda) & = e^{-\frac{(1+\lambda)}{2}[\mu^2- 2\mu(\frac{x}{1+\lambda} + \frac{\xi}{1+\lambda})]}\\
\pi(\mu|x,\xi,\lambda) & = e^{-\frac{(1+\lambda)}{2}[\mu^2- 2\mu(\frac{x+\xi}{1+\lambda})+ (\frac{x+\xi}{1+\lambda})^2- (\frac{x+\xi}{1+\lambda})^2]}\\
\pi(\mu|x,\xi,\lambda) & = e^{-\frac{(1+\lambda)}{2}[(\mu- \frac{x+\xi}{1+\lambda})^2- (\frac{x+\xi}{1+\lambda})^2]}\\
\pi(\mu|x,\xi,\lambda) & = e^{-\frac{(1+\lambda)}{2}(\mu- \frac{x+\xi}{1+\lambda})^2}e^{- (\frac{x+\xi}{1+\lambda})^2}\\
\pi(\mu|x,\xi,\lambda) & \propto e^{-\frac{(1+\lambda)}{2}(\mu- \frac{x+\xi}{1+\lambda})^2}
\end{array}
\]

Con lo cual tenemos una distribuci?n normal con media
\(\frac{x+\xi}{1+\lambda}\) y varianza \(\frac{1}{(1+\lambda)}\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Ejecicio 2.10}\label{ejecicio-2.10}

\textbf{Calcular la distribución posterior para una muestra simple iid
\(\mathcal{D}=(x_1,x_2,...,x_n)\) de \(\mathcal{N}(\mu,1)\) y muestre
que esto solo depende del estadístico suficiente
\(\bar{x}=\frac{\sum_{i=1}^n x_{i}}{n}\)}

De lo anterior sabemos que la priori conjugada es
\(\pi(\mu|\xi,\lambda) = e^{\mu\xi-\lambda\frac{\mu^2}{2}}\) y la
verosimilitud de los datos es
\(L(\mu|\mathcal{D})=\prod_{i=1}^n \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x_i-\mu)^2}\),
sustituyendo este tenemos: \[
\begin{array}{rl}
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto L(\mu|\mathcal{D})\pi(\mu|\xi,\lambda)\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x_i-\mu)^2}e^{\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto  \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} \sum_{i=1}^{n}(x_i-\mu)^2}e^{\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} \sum_{i=1}^{n}(x_i-\mu)^2+\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} \sum_{i=1}^{n}(x_i^2-2\mu x_i +\mu^2)+\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} (\sum_{i=1}^{n}x_i^2-2\mu n \bar{x} +n\mu^2)+\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & = \frac{1}{\sqrt{2\pi}}e^{-\frac{\sum_{i=1}^{n}x_i^2}{2} +n \mu \bar{x} -\frac{n\mu^2}{2}+\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & = \frac{1}{\sqrt{2\pi}}e^{-\frac{\sum_{i=1}^{n}x_i^2}{2} +n \mu \bar{x} -\frac{n\mu^2}{2}+\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & = \frac{1}{\sqrt{2\pi}}e^{-\frac{\sum_{i=1}^{n}x_i^2}{2} +n \mu \bar{x} -\frac{n\mu^2}{2}+\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ n \mu \bar{x} -\frac{n\mu^2}{2}+\mu\xi-\lambda\frac{\mu^2}{2}}\\
\end{array}
\]

Completando cuadrados tenemos: \[
\begin{array}{rl}
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ \mu^2(-\frac{n}{2}-\frac{\lambda}{2})+\mu(n\bar{x} +\xi)}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ -\frac{1}{2}(n+\lambda)\mu^2+\mu(n\bar{x} +\xi)}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ -\frac{1}{2}(n+\lambda)[\mu^2-\mu(n\bar{x} +\xi)\frac{2}{(n+\lambda)}]}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ -\frac{1}{2}(n+\lambda)[\mu^2-2\mu\frac{n\bar{x}+ \xi}{n+\lambda}]}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ -\frac{n+\lambda}{2}[\mu^2-2\mu\frac{n\bar{x} +\xi}{n+\lambda}+(\frac{n\bar{x}+ \xi}{n+\lambda})^2-(\frac{n\bar{x} +\xi}{n+\lambda})^2]}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ -\frac{n+\lambda}{2}[\mu-\frac{n\bar{x}+ \xi}{n+\lambda}]^2+\frac{n+\lambda}{2}(\frac{n\bar{x} +\xi}{n+\lambda})^2}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ -\frac{n+\lambda}{2}[\mu-\frac{n\bar{x} +\xi}{n+\lambda}]^2+\frac{1}{2}\frac{(n\bar{x} +\xi)^2}{n+\lambda}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ -\frac{n+\lambda}{2}[\mu-\frac{n\bar{x} +\xi}{n+\lambda}]^2}\\
\end{array}
\]

La cual es una normal con media \(\frac{n\bar{x} +\xi}{n+\lambda}\) y
varianza \(\frac{1}{n+\lambda}\), notemos que la media solo depende del
estadístico de prueba \(\bar x\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Ejercicio 2.11}\label{ejercicio-2.11}

\textbf{De el rango de valores de la media posteriori, donde el par
\((\lambda,\lambda^{-1}\xi)\) varia sobre
\(\mathbb{R}^+\times\mathbb{R}\)}

La media de la distribución a priori de \(\mu\) es:
\[\frac{x+\xi}{1+\lambda}=\frac{1}{1+\lambda}x+\frac{1}{1+\lambda}\xi\]

Lo que es igual
\[\frac{\lambda^{-1}}{1+\lambda^{-1}}x+\frac{\xi\lambda^{-1}}{1+\lambda^{-1}}\]

Estamos suponiendo que \(\lambda>0\)

\begin{itemize}
\tightlist
\item
  Si \(\lambda\to \infty\) entonces
  \(\frac{\lambda^{-1}}{1+\lambda^{-1}}\to 0\).
\item
  Si \(\lambda\to 0\) entonces
  \(\frac{\lambda^{-1}}{1+\lambda^{-1}}\to 1\), ya que el valor de
  arriba será igual que el denominador.
\end{itemize}

Por lo tanto \[\frac{\lambda^{-1}}{1+\lambda^{-1}}\le 1\]

\textbf{Si tengo una muestra simple idd \(\mathcal{D}=(x_1,...,x_n)\) de
una distribución normal \(\mathcal{N}(\mu,\sigma^2)\) y
\(\theta=(\mu, \sigma^2)\), la priori de \(\theta\) es el producto de
una inversa gamma en \(\sigma^2\),
\(\mathcal{IG}(\lambda_\sigma-1,\frac{\alpha}{2})\) y \(\mu\)
condicionada a \(\sigma^2\) como
\(\mathcal{N}(\xi,\frac{\sigma^2}{\lambda_\mu})\)} \[
\begin{array}{rl}
\pi(\theta|\mathcal{D}) & \propto L(\theta|\mathcal{D})\pi(\theta)\\
\pi(\theta|\mathcal{D}) & = L(\theta|\mathcal{D})\pi(\mu, \sigma^2)\\
\pi(\theta|\mathcal{D}) & = L(\theta|\mathcal{D})\pi(\mu| \sigma^2)\pi(\sigma^2)\\
\end{array}
\]

Donde \[
\begin{array}{rl}
L(\theta|\mathcal{D})&=\prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^2}(x_i-\mu)^2}\\
L(\theta|\mathcal{D})&= \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2}\\
L(\theta|\mathcal{D})&= \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i^2-2x_i\mu+\mu^2)}\\
L(\theta|\mathcal{D})&= \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}(\sum_{i=1}^n x_i^2-2n\mu\frac{\sum_{i=1}^n
x_i}{n}+n\mu^2)}\\
L(\theta|\mathcal{D})&= \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}(\sum_{i=1}^n x_i^2-2n\mu\bar x+n\mu^2)}\\
L(\theta|\mathcal{D})&= \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}(\sum_{i=1}^n x_i^2+n(\mu^2-2\mu\bar x+\bar x^2-\bar x^2))}\\
L(\theta|\mathcal{D})&= \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}(\sum_{i=1}^n x_i^2+n(\mu-\bar x)^2-n\bar x^2)}\\
L(\theta|\mathcal{D})&= \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}(n(\mu-\bar x)^2 + \sum_{i=1}^n x_i^2-n\bar x^2)}\\
L(\theta|\mathcal{D})&= \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}(n(\mu-\bar x)^2 + S_x)}
\end{array}
\]

Las prioris son: \[
\begin{array}{rl}
\pi(\mu| \sigma^2) & = \frac{1}{\sqrt{2\pi\frac{\sigma^2}{\lambda_\mu}}}e^{-\frac{1}{2\frac{\sigma^2}{\lambda_\mu}}(\mu-\xi)^2}\\
& =\frac{\sqrt{\lambda_\mu}}{\sqrt{2\pi \sigma^2}}e^{-\frac{\lambda_\mu}{2\sigma^2}(\mu-\xi)^2}
\end{array}
\]

\[\pi(\sigma^2)=\frac{\left(\frac{\alpha}{2}\right)^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)}\frac{e^{-\frac{\alpha}{2\sigma^2}}}{{\sigma^2}^{\lambda_\sigma+1}}\]

Asi tenemos: \[
\begin{array}{rl}
\pi(\theta|\mathcal{D}) & =  \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}[n(\mu-\bar x)^2 + S_x]}\frac{\sqrt{\lambda_\mu}}{\sqrt{2\pi \sigma^2}}e^{-\frac{\lambda_\mu}{2\sigma^2}(\mu-\xi)^2}\frac{\left(\frac{\alpha}{2}\right)^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)}\frac{e^{-\frac{\alpha}{2\sigma^2}}}{{\sigma^2}^{\lambda_\sigma+1}}\\
\pi(\theta|\mathcal{D}) & \propto \left( \frac{1}{\sigma}\right)^ne^{-\frac{1}{2\sigma^2}[n(\mu-\bar x)^2 + S_x]}\frac{1}{\sigma}e^{-\frac{\lambda_\mu}{2\sigma^2}(\mu-\xi)^2}\frac{e^{-\frac{\alpha}{2\sigma^2}}}{{\sigma^2}^{\lambda_\sigma+1}}\\
\pi(\theta|\mathcal{D}) & = \frac{1}{\sigma^n} e^{-\frac{1}{2\sigma^2}[n(\mu-\bar x)^2 + S_x]}\frac{1}{\sigma}e^{-\frac{\lambda_\mu}{2\sigma^2}(\mu-\xi)^2}e^{-\frac{\alpha}{2\sigma^2}}\frac{1}{{\sigma}^{2\lambda_\sigma+2}}\\
\pi(\theta|\mathcal{D}) & = \frac{1}{\sigma^{n+1+2\lambda_\sigma+2}} e^{-\frac{1}{2\sigma^2}[n(\mu-\bar x)^2 + S_x]}e^{-\frac{\lambda_\mu}{2\sigma^2}(\mu-\xi)^2}e^{-\frac{\alpha}{2\sigma^2}}\\
\pi(\theta|\mathcal{D}) & = \frac{1}{\sigma^{n+2\lambda_\sigma+3}} e^{-\frac{1}{2\sigma^2}[n(\mu-\bar x)^2 + S_x]-\frac{\lambda_\mu}{2\sigma^2}(\mu-\xi)^2-\frac{\alpha}{2\sigma^2}}\\
\pi(\theta|\mathcal{D}) & = \frac{1}{\sigma^{n+2\lambda_\sigma+3}} e^{-\frac{1}{2\sigma^2}[n(\mu-\bar x)^2 + S_x+\lambda_\mu(\mu-\xi)^2+\alpha]}\\
\pi(\theta|\mathcal{D}) & = \frac{1}{(\sigma^2)^{\frac{n}{2}+\lambda_\sigma+\frac{3}{2}}} e^{-\frac{1}{2\sigma^2}[n(\mu-\bar x)^2 + S_x+\lambda_\mu(\mu-\xi)^2+\alpha]}\\
\end{array}
\]

Si hacemos
\(\lambda_\sigma(\mathcal{D})=\frac{n}{2}+\lambda_\sigma+\frac{3}{2}\),
\(\lambda_\mu(\mathcal{D})=\lambda_\mu\) y
\(\alpha(\mathcal{D})=n(\mu-\bar x)^2 + S_x+\alpha\)

\[\pi(\theta|\mathcal{D}) = \frac{1}{(\sigma^2)^{\lambda_\sigma(\mathcal{D})}} e^{-\frac{1}{2\sigma^2}[\lambda_\mu(\mathcal{D})(\mu-\xi)^2+\alpha(\mathcal{D})]}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Ejercicio 2.12}\label{ejercicio-2.12}

\textbf{Una distribución Weibull \(\mathcal{W}(\alpha, \beta,\gamma)\)
es definida como una potencia de una gamma
\(\mathcal{G}(\alpha, \beta)\): Si
\(X\sim\mathcal{W}(\alpha, \beta,\gamma)\) entonces
\(X^\gamma\sim\mathcal{G}(\alpha, \beta)\). Muestre que cuando
\(\gamma\) es conocida \(\mathcal{W}(\alpha, \beta,\gamma)\) es una
familia exponencial pero no lo es cuando \(\gamma\) es desconocida.}

La distribución Weibull es:

\[f_\theta(x)=\frac{\gamma \alpha^\beta}{\Gamma(\beta)}x^{(\beta+1)\gamma -1}e^{-x^\gamma\alpha}\]
Supongamos que \(\gamma\) es conocido y probemos que la Weibull es de la
familia exponencial: \[
\begin{array}{rl}
f_\theta(x) & = \frac{\gamma \alpha^\beta}{\Gamma(\beta)}x^{(\beta+1)\gamma -1}e^{-x^\gamma\alpha}\\
f_\theta(x) & = e^{\ln(\frac{\gamma \alpha^\beta}{\Gamma(\beta)}x^{(\beta+1)\gamma -1})}e^{-x^\gamma\alpha}\\
f_\theta(x) & = e^{\ln(\gamma \alpha^\beta) -\ln(\Gamma(\beta))+((\beta+1)\gamma -1)\ln(x)}e^{-x^\gamma\alpha}\\
f_\theta(x) & = e^{\ln(\gamma \alpha^\beta) -\ln(\Gamma(\beta))+((\beta+1)\gamma -1)\ln(x)-x^\gamma\alpha}\\
f_\theta(x) & = e^{\ln(\gamma \alpha^\beta) -\ln(\Gamma(\beta))+(\beta+1)\gamma\ln(x)-\ln(x)-x^\gamma\alpha}\\
f_\theta(x) & = e^{\ln(\gamma \alpha^\beta) -\ln(\Gamma(\beta))+\beta\gamma\ln(x)+\gamma\ln(x)+\gamma\ln(x)-\ln(x)-x^\gamma\alpha}\\
f_\theta(x) & = e^{\ln(\gamma \alpha^\beta) -\ln(\Gamma(\beta))+\beta\gamma\ln(x)+\gamma\ln(x)+\gamma\ln(x)-\ln(x)-x^\gamma\alpha}\\
\end{array}
\]

Si hacemos
\(\psi(\theta)= -\ln(\gamma \alpha^\beta) +\ln(\Gamma(\beta))\) \[
\begin{array}{rl}
f_\theta(x) & = e^{-\psi(\theta)+\beta\gamma\ln(x)-x^\gamma\alpha+\gamma\ln(x)-\ln(x)} \\
f_\theta(x) & = e^{-\psi(\theta)+\beta\gamma\ln(x)-x^\gamma\alpha+(\gamma-1)\ln(x)} \\
\end{array}
\]

Notemos que:
\[\beta\gamma\ln(x)-x^\gamma\alpha=[\beta, \alpha]\begin{bmatrix} \gamma\ln(x) \\ -x^\gamma \end{bmatrix}\]

Entonces tenemos que \(\theta=[\beta, \alpha]\),
\(R(x)=[\gamma\ln(x), -x^\gamma]^t\) \[
\begin{array}{rl}
f_\theta(x) & = e^{\theta R(x)-\psi(\theta)+(\gamma-1)\ln(x)} \\
f_\theta(x) & = e^{\theta R(x)-\psi(\theta)}e^{(\gamma-1)\ln(x)} \\
f_\theta(x) & = e^{\theta R(x)-\psi(\theta)}e^{\ln(x^{(\gamma-1)})} \\
f_\theta(x) & = x^{(\gamma-1)} e^{\theta R(x)-\psi(\theta)}\\
\end{array}
\]

Finalmente \(h(x)=x^{(\gamma-1)}\), por lo tanto
\[f_\theta(x) = h(x) e^{\theta R(x)-\psi(\theta)}\] Así la función
Weibull es de la familia exponencial cuando \(\gamma\) es conocido.
Cuando \(\gamma\) es desconocido es imposible conseguir la forma de la
familia exponencial.

Además notemos que si \(y=x^\gamma\) entonces \(y'=\gamma x^{\gamma-1}\)
y \(x=y^{\frac{1}{\gamma}}\) \[
\begin{array}{rl}
f_\theta(x) & = \frac{\gamma}{\gamma}x^{(\gamma-1)} e^{[\beta, \gamma]\begin{bmatrix}\ln(x) \\ -x^\gamma \end{bmatrix} +\ln(\gamma \alpha^\beta) -\ln(\Gamma(\beta))+(\gamma-1)\ln(x)}\\
f_\theta(x) & = \frac{y'}{\gamma} e^{[\beta, \alpha]\begin{bmatrix} \ln(y^{\frac{1}{\gamma}}) \\ -y \end{bmatrix} +\ln(\gamma \alpha^\beta) -\ln(\Gamma(\beta))+(\gamma-1)\ln(y^{\frac{1}{\gamma}})}\\
f_\theta(x) & = \frac{y'}{\gamma} e^{[\beta, \alpha]\begin{bmatrix} \frac{1}{\gamma}\ln(y) \\ -y \end{bmatrix} +\ln(\gamma \alpha^\beta) -\ln(\Gamma(\beta))+\frac{\gamma-1}{\gamma}\ln(y)}\\
f_\theta(x) & = \frac{y'}{\gamma} e^{\frac{\beta}{\gamma}\ln(y) -y\alpha +\ln(\frac{\gamma \alpha^\beta}{\Gamma(\beta)}) +\frac{\gamma-1}{\gamma}\ln(y)}\\
f_\theta(x) & = \frac{y'}{\gamma} e^{\frac{\beta}{\gamma}\ln(y)}e^{-y\alpha}e^{\ln(\frac{\gamma \alpha^\beta}{\Gamma(\beta)})} e^{\frac{\gamma-1}{\gamma}\ln(y)}\\
f_\theta(x) & = \frac{y'}{\gamma} e^{\ln(y^{\frac{\beta}{\gamma}})}e^{-y\alpha}e^{\ln(\frac{\gamma \alpha^\beta}{\Gamma(\beta)})} e^{\ln(y^\frac{\gamma-1}{\gamma})}\\
f_\theta(x) & = \frac{y'}{\gamma} y^{\frac{\beta}{\gamma}}e^{-y\alpha} \frac{\gamma \alpha^\beta}{\Gamma(\beta)} y^\frac{\gamma-1}{\gamma}\\
f_\theta(x) & = \frac{y'}{\gamma} y^{\frac{\beta}{\gamma}+\frac{\gamma-1}{\gamma}}e^{-y\alpha} \frac{\gamma \alpha^\beta}{\Gamma(\beta)} \\
f_\theta(x) & = \frac{y'}{\gamma} y^{\frac{\beta-1+2\gamma}{\gamma}-1}e^{-y\alpha} \frac{\gamma \alpha^\beta}{\Gamma(\beta)} \\
\end{array}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Ejercicio 2.13}\label{ejercicio-2.13}

\textbf{Muestre que cuando la priori de \(\theta=(\mu, \sigma^2)\) es
\(\mathcal{N}(\xi,\frac{\sigma^2}{\lambda_\mu})\times \mathcal{IG}(\lambda_s,\alpha)\),
la marginal de \(\mu\) es una distribuci?n \emph{t-Student}
\(\mathcal{T}(1\lambda_s,\xi,2\alpha)\). Dar la correspondiente marginal
a priori de \(\sigma^2\). Para una muestra simple iid
\(\mathcal{D}=(x_1,...x_n)\) de \(\mathcal{N}(\mu,\sigma^2)\)}

La priori de \(\theta\) es: \[
\begin{array}{rl}
\pi(\theta) & =\frac{\sqrt{\lambda_\mu}}{\sqrt{2\pi \sigma^2}}e^{-\frac{\lambda_\mu}{2\sigma^2}(\mu-\xi)^2}\frac{\alpha^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)}\frac{e^{-\frac{\alpha}{\sigma^2}}}{{\sigma^2}^{\lambda_\sigma+1}}\\
\pi(\theta) & =\frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\sqrt{2\pi }\Gamma(\lambda_\sigma)}\frac{1}{{\sigma^2}^{(\frac{1}{2}+\lambda_\sigma+1)}}e^{-\frac{\lambda_\mu}{2\sigma^2}(\mu-\xi)^2-\frac{\alpha}{\sigma^2}}\\
\pi(\theta) & =\frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\sqrt{2\pi }\Gamma(\lambda_\sigma)}\frac{1}{{\sigma^2}^{(\frac{3}{2}+\lambda_\sigma)}}e^{-\frac{1}{2\sigma^2}[\lambda_\mu(\mu-\xi)^2-2\alpha]}\\
\end{array}
\]

Entonces tenemos que la priori de \(\mu\) es: \[
\begin{array}{rl}
\pi(\mu) & =\displaystyle \int \pi(\theta)d\sigma^2\\
\pi(\mu) & =\displaystyle \int \frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\sqrt{2\pi }\Gamma(\lambda_\sigma)}\frac{1}{{\sigma^2}^{(\frac{3}{2}+\lambda_\sigma)}}e^{-\frac{1}{2\sigma^2}[\lambda_\mu(\mu-\xi)^2-2\alpha]}d\sigma^2\\
\pi(\mu) & =\displaystyle \frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\sqrt{2\pi }\Gamma(\lambda_\sigma)} \int \frac{1}{{\sigma^2}^{(\frac{3}{2}+\lambda_\sigma)}}e^{-\frac{1}{2\sigma^2}[\lambda_\mu(\mu-\xi)^2-2\alpha]}d\sigma^2
\end{array}
\]

Si llamamos \(A=\frac{3}{2}+\lambda_\sigma\),
\(C=\frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\sqrt{2\pi }\Gamma(\lambda_\sigma)}\)
y \(B=\frac{1}{2}[\lambda_\mu(\mu-\xi)^2-2\alpha]\) tenemos \[
\begin{array}{rl}
\pi(\mu) & =\displaystyle C \int \frac{1}{{\sigma^2}^A}e^{-\frac{1}{\sigma^2}B}d\sigma^2\\
\pi(\mu) & =\displaystyle C \int \left(\frac{1}{{\sigma^2}}\right)^Ae^{-(\frac{1}{\sigma^2})B}d\sigma^2\\
\pi(\mu) & =\displaystyle C \int \left(\frac{1}{{\sigma^2}}\right)^{A+2-2}e^{-(\frac{1}{\sigma^2})B}d\sigma^2\\
\pi(\mu) & =\displaystyle C \int \left(\frac{1}{{\sigma^2}}\right)^{A-2}e^{-(\frac{1}{\sigma^2})B}\left(\frac{1}{{\sigma^2}}\right)^2 d\sigma^2\\
\end{array}
\]

Haciendo el cambio de variable \(u=\frac{1}{\sigma^2}\)
\(du=-\left( \frac{1}{\sigma^2} \right)^2d\sigma^2\) \[
\begin{array}{rl}
\pi(\mu) & =\displaystyle -C \int u^{A-2}e^{-uB} du\\
\pi(\mu) & =\displaystyle -C \int u^{(A-1)-1}e^{-uB} \frac{B^{A-1}}{\Gamma(A-1)}\frac{\Gamma(A-1)}{B^{A-1}}du\\
\pi(\mu) & =\displaystyle -C\frac{\Gamma(A-1)}{B^{A-1}}
\int u^{(A-1)-1}e^{-uB} \frac{B^{A-1}}{\Gamma(A-1)}du\\
\pi(\mu) & =\displaystyle -C\frac{\Gamma(A-1)}{B^{A-1}}
\end{array}
\]

Ya que \(u^{(A-1)-1}e^{-uB} \frac{A^{B-1}}{\Gamma(B-1)}\) es la funci?n
de densidad de una distribuci?n Gamma con par?metros \(A-1\) y \(B\) \[
\begin{array}{rl}
\pi(\mu) & \propto B^{-(A-1)} \\
\pi(\mu) & \propto B^{1-A} \\
\pi(\mu) & = (\frac{1}{2}[\lambda_\mu(\mu-\xi)^2-2\alpha])^{1-[\frac{3}{2}+\lambda_\sigma]}\\
\pi(\mu) & \propto [\lambda_\mu(\mu-\xi)^2-2\alpha]^{-\frac{1}{2}-\lambda_\sigma}\\
\pi(\mu) & \propto [-2\alpha(1-\frac{\lambda_\mu(\mu-\xi)^2}{2\alpha})]^{-\frac{1}{2}-\lambda_\sigma}\\
\pi(\mu) & \propto (1-\frac{\lambda_\mu\lambda_\sigma(\mu-\xi)^2}{2\alpha\lambda_\sigma})^{-\frac{1+2\lambda_\sigma}{2}}
\end{array}
\]

La priori marginal correspondiente para \(\sigma^2\) será: \[
\begin{array}{rl}
\pi(\mu) & =\displaystyle \int \pi(\theta)d\mu\\
\pi(\mu) & =\displaystyle \int \frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\sqrt{2\pi }\Gamma(\lambda_\sigma)}\frac{1}{{\sigma^2}^{(\frac{3}{2}+\lambda_\sigma)}}e^{-\frac{1}{2\sigma^2}[\lambda_\mu(\mu-\xi)^2-2\alpha]}d\mu\\
\pi(\mu) & =\displaystyle \frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\sqrt{2\pi }\Gamma(\lambda_\sigma)} \frac{1}{{\sigma^2}^{(\frac{3}{2}+\lambda_\sigma)}}e^{\frac{\alpha}{\sigma^2}}\int e^{-\frac{1}{2\sigma^2}\lambda_\mu(\mu-\xi)^2}d\mu\\
\pi(\mu) & =\displaystyle \frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)} \frac{1}{{\sigma^2}^{(\frac{3}{2}+\lambda_\sigma)}}e^{\frac{\alpha}{\sigma^2}}\int \frac{1}{\sqrt{2\pi }}e^{-\frac{1}{2\sigma^2}\lambda_\mu(\mu-\xi)^2}d\mu
\end{array}
\]

Ahora llamemos
\(D=\frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)} \frac{1}{{\sigma^2}^{(\frac{3}{2}+\lambda_\sigma)}}e^{\frac{\alpha}{\sigma^2}}\)
\[
\begin{array}{rl}
\pi(\mu) & =\displaystyle D\int \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2\sigma^2}\lambda_\mu(\mu-\xi)^2}d\mu\\
\pi(\mu) & =\displaystyle D\int \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2\frac{\sigma^2}{\lambda_\mu}} (\mu-\xi)^2}d\mu\\
\pi(\mu) & =\displaystyle D\int \frac{\sqrt{\frac{\sigma^2}{\lambda_\mu}}}{\sqrt{2\pi\frac{\sigma^2}{\lambda_\mu}}}e^{-\frac{1}{2\frac{\sigma^2}{\lambda_\mu}} (\mu-\xi)^2}d\mu\\
\pi(\mu) & = D\sqrt{\frac{\sigma^2}{\lambda_\mu}} \displaystyle \int \frac{1}{\sqrt{2\pi\frac{\sigma^2}{\lambda_\mu}}}e^{-\frac{1}{2\frac{\sigma^2}{\lambda_\mu}} (\mu-\xi)^2}d\mu\\
\pi(\mu) & = D\sqrt{\frac{\sigma^2}{\lambda_\mu}}
\end{array}
\]

Ya que
\(\frac{1}{\sqrt{2\pi\frac{\sigma^2}{\lambda_\mu}}}e^{-\frac{1}{2\frac{\sigma^2}{\lambda_\mu}} (\mu-\xi)^2}\)
es una función de densidad de una normal con media \(\xi\) y varianza
\(\frac{\sigma^2}{\lambda_\mu}\)

Por lo tanto: \[
\begin{array}{rl}
\pi(\mu) & = \frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)} \frac{1}{{\sigma^2}^{(\frac{3}{2}+\lambda_\sigma)}}e^{\frac{\alpha}{\sigma^2}} \sqrt{\frac{\sigma^2}{\lambda_\mu}}\\
\pi(\mu) & = \frac{\alpha^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)}e^{\frac{\alpha}{\sigma^2}} (\sigma^2)^{\frac{1}{2}-\frac{3}{2}-\lambda_\sigma}\\
\pi(\mu) & = \frac{\alpha^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)}e^{\frac{\alpha}{\sigma^2}} (\sigma^2)^{-1-\lambda_\sigma}\\
\pi(\mu) & = \frac{\alpha^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)}e^{\frac{\alpha}{\sigma^2}}\frac{1}{(\sigma^2)^{\lambda_\sigma+1}}
\end{array}
\]

La cual es la función de densidad de una Inversa Gamma con parámetros
\(\lambda_\sigma\) y \(\alpha\).

La distribución a posteriori para la muestra normal será entonces:

\subsection{2.2.3 Intervalos de
Confianza}\label{intervalos-de-confianza}

Un punto que debe quedar claro desde el principio es que el enfoque
Bayesiano es un enfoque/acercamiento inferencial completo. Entonces,
cubre la evaluación de confianza, pruebas, predicción, chequeo del
modelo y estimación puntual. Vamos a cubrir progresivamente las
diferentes facetas del Análisis Bayesiano en otros capítulos de este
libro, pero vamos a refefirnos aquí sobre los intervalos de confianza.

Como en todo lo demás, la obtención/derivación de los intervalos de
confianza (o regiones de confianza en una configuración más general)
está basada en la distribución posterior \(\pi(\theta,\mathcal{D})\).
Dado que el enfoque Bayesiano trata/procesa al parámetro \(\theta\) como
una variable aleatoria (se le asigna una distribución), una definición
``natural'' de la región de confianza sobre \(\theta\) es determinar
\(C(\mathcal{D})\) tal que \[
\tag{2.9}
\pi(\theta \in C(\mathcal{D})\ |\ \mathcal{D}) = 1 - \alpha
\] donde \(\alpha\) es un nivel predeterminado así como \(0.05\).

\textbf{La grán diferencia, desde una perspectiva tradicional, es que la
integración se hace sobre el espacio de parámetros, en lugar del espacio
muestral. La cantidad \(1-\alpha\) entonces corresponde a la
probabilidad de que un \(\theta\) aleatorio pertenece al conjunto
\(C(\mathcal{D})\), en lugar de la probabilidad de que un conjunto
(intervalo) aleatorio contiene el ``verdadero'' valor del parámetro
\(\theta\).}

Dado este cambio en la interpretación de un conjunto de confianza
(también llamado conjunto con credibilidad/creible por los Bayesianos),
la determinación del mejor intervalo resulta más fácil que en el sentido
clásico: simplemente corresponde al valor de \(\theta\) con el mayor
valor para la distribución posterior. \[
C(\mathcal{D})=\{\theta; \pi(\theta\ |\ \mathcal{D})\ge k_{\alpha}\},
\] donde \(k_\alpha\) se determina por la restricción de cobertura
(2.9). Esta región es llamada \emph{highest posterior density (HPD)
region} (región de mayor densidad posterior).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Para \textbf{normaldata}, utilizando el parámetro
\(\theta=(\mu,\sigma^2)\), la distribución posterior marginal sobre
\(\mu\) utilizando el \emph{a priori} \(\pi(\theta)=1/\sigma^2\) es una
distribución \emph{t-Student} \[
\pi(\mu|\mathcal{D})\propto [n(\mu-\bar{x})^2+s_x^2]^{-n/2}
\] Veamos como llegan a esto:

La distribución a posteriori viene dada por
\(\pi(\theta|\mathcal{D})\propto l(\theta|\mathcal{D})\pi(\theta)\)

Como los datos se distribuyen normal con media \(\mu\) y varianza
\(\sigma^2\) tenemos que la verosimilitud de los datos viene dada por:
\[
\begin{array}{rl}
l(\theta|\mathcal{D}) & = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{1}{2\sigma^2}(x_i-\mu)^2}\\
& =  \left( \frac{1}{\sqrt{2\pi\sigma^2}}\right)^n e^{\frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2}\\
\end{array}
\]

Donde \[
\begin{array}{rl}
\sum_{i=1}^n (x_i-\mu)^2 & = \sum_{i=1}^n x_i^2 -2x_i\mu +\mu^2\\
& = \sum_{i=1}^n x_i^2 -2\mu\sum_{i=1}^n x_i +\sum_{i=1}^n \mu^2\\
& = \sum_{i=1}^n x_i^2 -2\mu\frac{n\sum_{i=1}^nx_i}{n}  +n \mu^2\\
& = \sum_{i=1}^n x_i^2 -2n\mu\bar x  +n \mu^2\\
& = \sum_{i=1}^n x_i^2 + n(\mu^2-2\mu\bar x )\\
& = \sum_{i=1}^n x_i^2 + n(\mu^2-2\mu\bar x +\bar x^2-\bar x^2)\\
& = \sum_{i=1}^n x_i^2 + n((\mu-\bar x)^2 -\bar x^2)\\
& = \sum_{i=1}^n x_i^2 - n\bar x^2 + n(\mu-\bar x)^2\\
& = S_x + n(\mu-\bar x)^2
\end{array}
\]

Con \(S_x=\sum_{i=1}^n x_i^2 - n\bar x^2\). Sustituyendo este nos queda:
\[l(\theta|\mathcal{D})= \left( \frac{1}{\sqrt{2\pi\sigma^2}}\right)^n e^{\frac{1}{2\sigma^2}\left[ S_x + n(\mu-\bar x)^2\right]}\]

Por lo tanto \[
\begin{array}{rl}
\pi(\theta|\mathcal{D}) & \propto \left( \frac{1}{\sqrt{2\pi\sigma^2}}\right)^n e^{-\frac{1}{2\sigma^2}\left[ S_x + n(\mu-\bar x)^2\right]}\frac{1}{\sigma^2}\\
\pi(\theta|\mathcal{D}) & \propto \left( \frac{1}{\sqrt{\sigma^2}}\right)^n \frac{1}{\sigma^2} e^{-\frac{1}{2\sigma^2}\left[ S_x + n(\mu-\bar x)^2\right]}\\
\pi(\theta|\mathcal{D}) & \propto \left( \frac{1}{\sigma^2}\right)^{\frac{n}{2}+1}  e^{-\frac{1}{2\sigma^2}\left[ S_x + n(\mu-\bar x)^2\right]}
\end{array}
\]

Para conseguir la distribución a posteriori del parámetro \(\mu\)
tenemos que integrar esta última función con respecto a \(\sigma^2\) \[
\begin{array}{rl}
\pi(\mu|\mathcal{D}) & = \displaystyle \int \pi(\theta|\mathcal{D}) d\sigma^2\\
\pi(\mu|\mathcal{D}) & = \displaystyle \int \left( \frac{1}{\sigma^2}\right)^{\frac{n}{2}+1}  e^{-\frac{1}{2\sigma^2}\left[ S_x + n(\mu-\bar x)^2\right]}d\sigma^2\\
\pi(\mu|\mathcal{D}) & = \displaystyle \int \left( \frac{1}{\sigma^2}\right)^{\frac{n}{2}+1}e^{-\frac{1}{2\sigma^2}A}d\sigma^2\\
\pi(\mu|\mathcal{D}) & = \displaystyle \int \left( \frac{1}{\sigma^2}\right)^{\frac{n}{2}-1}\left( \frac{1}{\sigma^2}\right)^{2}e^{-\frac{1}{2\sigma^2}A}d\sigma^2
\end{array}
\] Haciendo el cambio de variable \(u=\frac{1}{\sigma^2}\) entonces
\(du=-\left(\frac{1}{\sigma^2}\right)^2 d\sigma^2\) \[
\begin{array}{rl}
\pi(\mu|\mathcal{D}) & = \displaystyle \int (u)^{\frac{n}{2}-1}e^{-\frac{1}{2}uA}du\\
\pi(\mu|\mathcal{D}) & = \displaystyle \int (u)^{\frac{n}{2}-1}e^{-\frac{1}{2}Au}\frac{(\frac{A}{2})^{\frac{n}{2}}}{\Gamma{(\frac{n}{2}})}\frac{\Gamma{(\frac{n}{2}})}{(\frac{A}{2})^{\frac{n}{2}}}du\\
\pi(\mu|\mathcal{D}) & = \displaystyle \frac{\Gamma{(\frac{n}{2}})}{(\frac{A}{2})^{\frac{n}{2}}} \int \frac{(\frac{A}{2})^{\frac{n}{2}}}{\Gamma{(\frac{n}{2}})} (u)^{\frac{n}{2}-1}e^{-\frac{1}{2}Au}du\\
\end{array}
\]

Donde
\(\frac{(\frac{A}{2})^{\frac{n}{2}}}{\Gamma{(\frac{n}{2}})}(u)^{\frac{n}{2}-1}e^{-\frac{1}{2}Au}\)
es la función de densidad de una distribución Gamma con parámetros
\(\frac{n}{2}\) y \(\frac{A}{2}\), así esta intendral da 1, por lo
tanto: \[
\begin{array}{rl}
\pi(\mu|\mathcal{D}) & = \displaystyle \frac{\Gamma{(\frac{n}{2}})}{(\frac{A}{2})^{\frac{n}{2}}}\\
\pi(\mu|\mathcal{D}) & \propto \left( \frac{A}{2}\right)^{-\frac{n}{2}}\\
\pi(\mu|\mathcal{D}) & \propto  \frac{\left(S_x + n(\mu-\bar x)^2\right)^{-\frac{n}{2}}}{\left(2\right)^{-\frac{n}{2}}}\\
\pi(\mu|\mathcal{D}) & \propto  \left(S_x + n(\mu-\bar x)^2\right)^{-\frac{n}{2}}\\
\end{array}
\]

con \(n-1=89\) grados de libertad, como se puede ver en (2.7), la región
de confianza correspondiente al \(95\%\) para \(\mu\) es el intervalo
\([-0.070,-0.013]\). Note que como el \(0\) no pertenece al intervalo,
uno puede sentir justificado el reporte de un decrecimiento en el número
robos entre 1991 y 1995.

Intervalo sobre \(\mu\) \[
[\bar{x}\ -\ t_{\alpha/2,n-1}s_x/\sqrt{n(n-1)},\ \bar{x}\ +\ t_{\alpha/2,n-1}s_x/\sqrt{n(n-1)}]
\]

\begin{itemize}
\tightlist
\item
  La región de confianza correspondiente al \(95\%\) para \(\mu\) es el
  intervalo \([-0.070,-0.013]\). Note que como el \(0\) no pertenece al
  intervalo, uno puede sentir justificado el reporte de un decrecimiento
  en el número robos entre 1991 y 1995.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alpha<-.}\DecValTok{05}
\NormalTok{n<-}\KeywordTok{length}\NormalTok{(larcenies)}
\NormalTok{xbar<-}\StringTok{ }\KeywordTok{mean}\NormalTok{(larcenies)}
\NormalTok{sx<-}\StringTok{ }\KeywordTok{sd}\NormalTok{(larcenies)}
\NormalTok{prob<-}\KeywordTok{qt}\NormalTok{(}\DecValTok{1}\NormalTok{-alpha/}\DecValTok{2}\NormalTok{,}\DataTypeTok{df=}\NormalTok{n}\DecValTok{-1}\NormalTok{)}
\KeywordTok{c}\NormalTok{(xbar-prob*sx/}\KeywordTok{sqrt}\NormalTok{(n),xbar+prob*sx/}\KeywordTok{sqrt}\NormalTok{(n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.06966541 -0.01285346
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Mientras que la forma de un intervalo de confianza Bayesiano óptimo es
fácilmente derivable, el cálculo de la cota \(k_\alpha\) o el conjunto
\(C(\mathcal{D})\) puede ser muy retador ``to allow an analytic
construction outside conjugate setups''

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Ejemplo 2.1}\label{ejemplo-2.1}

Cuando la distribución \emph{a priori} no es conjugada, la distribución
posterior no se comporta bien necesariamente. Por ejemplo, si la
distribución normal \(\mathcal{N}(\mu,1)\) es reemplazada por la
distribución de Cauchy, \(\mathcal{C}(\mu,1)\), en la verosimilitud \[
l(\mu|\mathcal{D})=\pi(\mathcal{D}\ |\ \theta)=\prod_{i=1}^{n}f_\mu(x_i)=\frac{1}{\pi^n\prod_{i=1}^n(1+(x_i-\mu)^2)},
\] no hay un \emph{a priori} conjugado disponible y podemos, por
ejemplo, considerar un \emph{a priori} normal para \(\mu\), digamos
\(\mathcal{N}(0,10)\). La distribución posterior, entonces, es
proporcional a \[
\tilde\pi(\mu|\mathcal{D})=\frac{\exp(-\mu^2/20)}{\prod_{i=1}^{n}(1+(x_i-\mu)^2)}.
\] Una solución analítica de \(\tilde\pi(\mu|\mathcal{D})\) no es
posible, sólo numérica, y el cálculo de una cota apropiada \(k_\alpha\)
requiere un nivel extra de cómputos numéricos para obtener la cobertura
correcta. La Figura 2.5 nos dá la distribución posterior de \(\mu\) para
los datos (observaciones) \(x_1=-4.3\) y \(x_2=3.2\), normalizado por
una simple integración trapezoidal, que es, calculando
\(\tilde\pi(\mu|\mathcal{D})\) en una grilla de ancho \(\triangle\) y
``and summing up''. Para un valor \(k\) dado, la misma aproximación
trapezoidal se puede utilizar para calcular la cobertura aproximada de
la región HDP. Para \(\alpha=0.95\), una prueba de ensayo y error
(tanteo) del rango de \(k\) nos proporciona una aproximación de
\(k_\alpha=0.0415\) y la región HPD, se representa como en la figura.
\(\lhd\)

\textbf{Introducir la figura}

\textbf{Fig. 2.5.} Distribución posterior de \(\mu\) para una
distribución \emph{a priori} \(\mathcal{N}(0,10)\) y una región HPD de
\(95\%\).

\begin{quote}
Dado que la distribución posterior no es necesariamente unimodal, la
región HPD puede incluir varios conjuntos no relacionados
(disconnected), como se ilustra en el Ejemplo 2.1. Esto puede parecer
contraintuitivo desde el punto de vista clásico, pero se debe
interpretar como un indicador de indeterminación, ya sea en los datos o
en la distribución \emph{a priori}, acerca de los posibles valores del
parámetro \(\theta\). Note también que las regiones HPD no son
independientes de la selección de la medida de referencia que define el
volumen (o superficie).
\end{quote}

\section{2.3 Prueba de Hipótesis}\label{prueba-de-hipotesis}

Decidir la validez de algunas premisas o restricciones sobre el
parámetro \(\theta\) es una gran parte del trabajo del estadístico.
(Vamos a lidiar con la validez del modelo completo, por ejemplo, si es o
no es apropiada una distribución normal para los datos provistos en la
sección 2.5.2 y más generalmente en la sección 6.7.3.). Como el
resultado del proceso de decisión es claro (\emph{clearcut}), aceptar
(representado por 1) o rechazar (representado por 0), la construcción y
la evaluación de los procedimientos es crucial. Mientras que la solución
Bayesiana es formalmente muy cercana a un estadístico de la razón de la
verosimilitud, sus valores numéricos frecuentemente difieren fuertemente
de las soluciones clásicas.

\subsection{2.3.1 Decisiones Cero-Uno}\label{decisiones-cero-uno}

Vamos a formalizar la premisa como espacios de parámetros restringidos,
denotado como \(\theta\in\Theta_0\). Por ejemplo, \(\theta>0\)
corresponde a \(\Theta_0=\mathbb{R}^+\).

El enfoque estándar de Neyman Pearson para pruebas de hipótesis se basa
en la función de pérdida \(0-1\) que penaliza todos los errores
igualmente: Si consideramos la prueba de la Hipótesis Nula
\(H_0: \theta\in\Theta_0\) versus \(H_1: \theta\notin\Theta_0\) y
denotamos por \(d\in\{0,1\}\) la decisión que toma el estadístico y por
\(\delta\) el procedimiento de decisión correspondiente, la pérdida \[
L(\theta,d)=\Bigg\{
\begin{array}{ll}
1-d &\text{si } \theta\in\Theta_0\\
d &\text{si no}
\end{array}
\] y el riesgo asociado \[
\begin{array}{rl}
R(\theta,\delta) = & \mathbb{E}_\theta[L(\theta,\delta(x))]\\
= & \displaystyle\int_\chi L(\theta,\delta(x))f_\theta(x)dx\\
= & \bigg\{
\displaystyle\begin{array}{lr}
P_\theta(\delta(x)=0), \text{ si }\theta\in\Theta_0\\
P_\theta(\delta(x)=1), \text{ si no}
\end{array}
\end{array}
\] donde \(x\) denota los datos \(\mathcal{D}\) disponibles. Bajo la
función de pérdida \(0-1\), el criterio de decisión de Bayes (estimador)
asociado con la distribución \emph{a priori} \(\pi\) es \[
\delta^\pi(x)=\Bigg\{
\begin{array}{lr}
1, \text{ si }P^\pi(\theta\in\Theta_0|x)>P^\pi(\theta\notin\Theta_0|x)\\
0, \text{ si no}
\end{array}
\] Este estimador se justifica fácilmente sobre una base intuitiva dado
que se escoge la hipótesis con la probabilidad posterior mayor.

Una generalización de la pérdida mencionada anteriormente es la
penalización de los errores de una manera diferente cuando la hipótesis
nula es verdadera en lugar de cuando es falsa a través de la función de
pérdida \(0-1\) ponderada (\(a_0,a_1>0\)), \[
L_{a_0,a_1}(\theta,d)= \Bigg \{
\begin{array}{ll}
a_0, &\text{ si }\ \theta\in\Theta_0\ \text{ y }\ d=0,\\
a_1, &\text{ si }\ \theta\in\Theta_1\ \text{ y }\ d=1,\\
0,&\text{ si no }
\end{array}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Ejercicio 2.18.}\label{ejercicio-2.18.}

Muestre que, bajo la función de pérdida \(L_{a_0,a_1}\), el estimador de
Bayes asociado con la distribución \emph{a priori} \(\pi\) es dada por
\[
\delta^\pi(x)=\Bigg\{
\begin{array}{ll}
1, &\text{ si }\ P^\pi(\theta\in\Theta_0|x)>\displaystyle \frac{a_1}{a_0+a_1},\\
0,&\text{ si no }
\end{array}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Sea \[
L_{a_0,a_1}(\theta,d)= \Bigg \{
\begin{array}{ll}
a_0, &\text{ si }\ \theta\in\Theta_0\ \text{ y }\ d=0,\\
a_1, &\text{ si }\ \theta\in\Theta_1\ \text{ y }\ d=1,\\
0,&\text{ si no }
\end{array}
\] siendo \(d\) la decisión que toma el estadístico (analista) y
\(\delta\) es el proceso de decisión. La pérdida esperada de la densidad
posterior es \[
\mathbb{E}[L_{a_0,a_1}(\theta,d)|x] = \bigg\{
\begin{array}{rl}
a_0\ P^\pi(\theta\in\Theta_0|x)\text{, si } d=0\\
a_1\ P^\pi(\theta\in\Theta_1|x)\text{, si } d=1\\
\end{array}
\] la decisión que minimiza la pérdida posterior es \(d=1\) cuando \[
\begin{array}{rl}
a_0\ P^\pi(\theta\in\Theta_0|x) > & a_1\ P^\pi(\theta\in\Theta_1|x) \\
a_0\ P^\pi(\theta\in\Theta_0|x) > & a_1\ [1-P^\pi(\theta\in\Theta_0|x)]\\
a_0\ P^\pi(\theta\in\Theta_0|x) > & a_1\ -\ a_1P^\pi(\theta\in\Theta_0|x)\\
a_0\ P^\pi(\theta\in\Theta_0|x) + a_1P^\pi(\theta\in\Theta_0|x) > & a_1\ \\
(a_0+a_1)\ P^\pi(\theta\in\Theta_0|x) > & a_1\ \\
\ P^\pi(\theta\in\Theta_0|x) > & \displaystyle\frac{a_1}{a_0+a_1}\ \\
\end{array}
\] y \(0\) en otro caso.

Por lo tanto nos queda \[
\delta^\pi(x)=\Bigg\{
\begin{array}{ll}
1, &\text{ si }\ P^\pi(\theta\in\Theta_0|x)>\displaystyle \frac{a_1}{a_0+a_1},\\
0,&\text{ si no }
\end{array}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Para esta clase de funciones de pérdida, la hipótesis nula \(H_0\) se
rechaza cuando la probabilidad posterior de \(H_0\) es muy pequeña, el
nivel de aceptación \(a_1/(a_0+a_1)\) está determinando por la
escogencia de (\(a_0,a_1\)).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Ejercicio 2.19.}\label{ejercicio-2.19.}

Cuando \(\theta\in\{\theta_0,\theta_1\}\), muestre que el procedimiento
Bayesiano sólo depende de la razón/ratio
\(\frac{(1-\varrho_0) f_{\theta_1}(x)}{\varrho_0 f_{\theta_0}(x)}\),
donde \(\varrho_0\) es el peso \emph{a priori} sobre \(\theta_0\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

En este caso \(\pi\) le asigna masa \(\varrho_0\) a un punto
\(\theta_0\) y le asigna por otra parte la masa \((1-\varrho_0)\) a
\(\theta_1\), entonces \[
\begin{array}{rl}
P^\pi(\theta=\theta_0)=&\displaystyle \frac{\varrho_0 f_{\theta_0}(x)}{\varrho_0 f_{\theta_0}(x)\ +\ \varrho_1 f_{\theta_1}(x)}\\
=&\displaystyle \frac{\varrho_0 f_{\theta_0}(x)}{\varrho_0 f_{\theta_0}(x)\ +\ (1-\varrho_0) f_{\theta_1}(x)}\\
=&\displaystyle \frac{1}{1\ +\ \displaystyle \frac{(1-\varrho_0) f_{\theta_1}(x)}{\varrho_0 f_{\theta_0}(x)}}
\end{array}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Para \(x\sim\mathcal{N}(\mu,\sigma^2)\) y
\(\mu\sim\mathcal{N}(\xi,\tau^2)\), recordemos que \(\pi(\mu|x)\) es la
distribución normal con \(\mathcal{N}(\xi(x),w^2)\) con \[
\xi(x)=\frac{\sigma^2\xi+\tau^2x}{\sigma^2+\tau^2}\ \text{ y }\ \omega^2=\frac{\sigma^2\tau^2}{\sigma^2+\tau^2}
\]

Para probar \(H_0:\mu<0\) calculamos \[
\begin{array}{rl}
P^\pi(\mu<0|x) = &  \displaystyle P^\pi\Bigg(\frac{\mu-\xi(x)}{\omega}<\frac{-\xi(x)}{\omega} \Bigg) \sim \mathcal{N}(0,1) \\
= & \displaystyle   \Phi\bigg(\frac{-\xi(x)}{\omega}\bigg)
\end{array}
\]

Si \(z_{a_0,a_1}\) es el cuantil \(\frac{a_1}{a_0+a_1}\) de una
distribución normal \(\mathcal{N}(0,1)\) (por ejemplo,
\(\Phi(z_{a_0,a_1})=\frac{a_1}{a_0+a_1}\)), aceptamos la hipótesis nula
\(H_0\) cuando \[
\begin{array}{rl}
-\xi(x) > & z_{a_0,a_1} \omega \\
-\displaystyle \frac{\sigma^2\xi+\tau^2 x}{\sigma^2+\tau^2}>& z_{a_0,a_1}\omega \\
-\displaystyle \sigma^2\xi-\tau^2x>& z_{a_0,a_1}\omega(\sigma^2+\tau^2) \\
\displaystyle -\tau^2x>& - z_{a_0,a_1}\omega(\sigma^2+\tau^2) - \sigma^2\xi \\
x<& \displaystyle \frac{-z_{ a_0,a_1}\omega(\sigma^2+\tau^2) - \sigma^2\xi}{\tau^2} \\
x<& \displaystyle -\frac{z_{ a_0,a_1}\omega(\sigma^2+\tau^2)}{\tau^2} - \frac{\sigma^2\xi}{\tau^2}
\end{array}
\]

La cota de aceptación superior es \[
x<-\frac{\sigma^2}{\tau^2}\xi - \bigg(1+ \frac{\sigma^2}{\tau^2} \bigg)z_{a_0,a_1}\omega
\]

Esto ilustra una vez más la falta de robustez de la distribución \emph{a
priori} conjugada: Esta cota puede ser cualquier real cuando \(\xi\)
varía en \(\mathbb{R}\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Ejercicio 2.20.}\label{ejercicio-2.20.}

Muestre que el límite de la probabilidad posterior \(P^\pi(\mu<0|x)\)
cuando \(\xi\) tiende a \(0\) y \(\tau\) tiende a \(\infty\) es
\(\Phi(-x/\sigma)\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Para \(x\sim\mathcal{N}(\mu,\sigma^2)\) y
\(\mu\sim\mathcal{N}(\xi,\tau^2)\), probemos que \(\pi(\mu|x)\) es la
distribución normal con \(\mathcal{N}(\xi(x),w^2)\) con
\(P^\pi(\mu<0|x)=\Phi\bigg(\frac{-\xi(x)}{\omega}\bigg)\) \[
\xi(x)=\frac{\sigma^2\xi+\tau^2x}{\sigma^2+\tau^2}\ \text{ y }\ \omega^2=\frac{\sigma^2\tau^2}{\sigma^2+\tau^2}
\]

La distribución a posteriori de \(\pi(\mu|x)\) es: \[
\begin{array}{rl}
\pi(\mu|x) & = l(\mu|x)\pi(\mu)\\
\pi(\mu|x) & = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}\frac{1}{\sqrt{2\pi\tau^2}}e^{-\frac{1}{2\tau^2}(\mu-\xi)^2}\\
\pi(\mu|x) & \propto e^{-\frac{1}{2\sigma^2}(x-\mu)^2}e^{-\frac{1}{2\tau^2}(\mu-\xi)^2}\\
\pi(\mu|x) & \propto e^{-\frac{1}{2\sigma^2}(x-\mu)^2-\frac{1}{2\tau^2}(\mu-\xi)^2}\\
\pi(\mu|x) & \propto e^{-\frac{1}{2}\left[\frac{(x-\mu)^2}{\sigma^2}+\frac{(\mu-\xi)^2}{\tau^2}\right]}\\
\end{array}
\]

Desarrollando \(\frac{(x-\mu)^2}{\sigma^2}+\frac{(\mu-\xi)^2}{\tau^2}\)
tenemos: \[
\begin{array}{rl}
\frac{(x-\mu)^2}{\sigma^2}+\frac{(\mu-\xi)^2}{\tau^2} & = \frac{x^2-2x\mu+\mu^2}{\sigma^2}+\frac{\mu^2-2\mu\xi+\xi^2}{\tau^2}\\
& = \frac{x^2}{\sigma^2}-\frac{2x\mu}{\sigma^2}+\frac{\mu^2}{\sigma^2}+\frac{\mu^2}{\tau^2}-\frac{2\mu\xi}{\tau^2}+\frac{\xi^2}{\tau^2}\\
& = \mu^2\left(\frac{1}{\sigma^2}+\frac{1}{\tau^2}\right) -2\mu\left(\frac{x}{\sigma^2}+\frac{\xi}{\tau^2}\right)+\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2}\\
\end{array}
\]

Sustituyendo estó: \[
\begin{array}{rl}
\pi(\mu|x) & \propto e^{-\frac{1}{2}\left[ \mu^2\left(\frac{1}{\sigma^2}+\frac{1}{\tau^2}\right) -2\mu\left(\frac{x}{\sigma^2}+\frac{\xi}{\tau^2}\right)+\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2}\right]}\\
\pi(\mu|x) & \propto e^{-\frac{1}{2}\left[ \mu^2\left(\frac{1}{\sigma^2}+\frac{1}{\tau^2}\right) -2\mu\left(\frac{x}{\sigma^2}+\frac{\xi}{\tau^2}\right)\right]} e^{\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2}}\\
\pi(\mu|x) & \propto e^{-\frac{1}{2}\left[ \mu^2\left(\frac{1}{\sigma^2}+\frac{1}{\tau^2}\right) -2\mu\left(\frac{x}{\sigma^2}+\frac{\xi}{\tau^2}\right)\right]}\\
\pi(\mu|x) & \propto e^{-\frac{1}{2}\left[ \mu^2\left(\frac{\tau^2+\sigma^2}{\tau^2 \sigma^2}\right) -2\mu\left(\frac{x\tau^2+\xi\sigma^2}{\tau^2\sigma^2}\right)\right]}\\
\pi(\mu|x) & \propto e^{-\frac{1}{2\tau^2\sigma^2}[ \mu^2(\tau^2+\sigma^2) -2\mu(x\tau^2+\xi\sigma^2)]}\\
\pi(\mu|x) & \propto e^{-\frac{\tau^2+\sigma^2}{2\tau^2\sigma^2}\left[ \mu^2 -2\mu\frac{(x\tau^2+\xi\sigma^2)}{(\tau^2+\sigma^2)}\right]}\\
\pi(\mu|x) & \propto e^{-\frac{1}{2\tau^2\sigma^2}[ \mu^2(\tau^2+\sigma^2) -2\mu(x\tau^2+\xi\sigma^2)]}\\
\pi(\mu|x) & \propto e^{-\frac{\tau^2+\sigma^2}{2\tau^2\sigma^2}\left[ \mu^2 -2\mu\frac{x\tau^2+\xi\sigma^2}{\tau^2+\sigma^2}+\left( \frac{(x\tau^2+\xi\sigma^2)}{(\tau^2+\sigma^2)} \right)^2-\left( \frac{x\tau^2+\xi\sigma^2}{\tau^2+\sigma^2} \right)^2\right]}\\
\pi(\mu|x) & \propto e^{-\frac{\tau^2+\sigma^2}{2\tau^2\sigma^2}\left[ \mu -\frac{x\tau^2+\xi\sigma^2}{\tau^2+\sigma^2}\right]^2}e^{\frac{(\tau^2+\sigma^2)}{2\tau^2\sigma^2}\left( \frac{x\tau^2+\xi\sigma^2}{\tau^2+\sigma^2} \right)^2}\\
\pi(\mu|x) & \propto e^{-\frac{\tau^2+\sigma^2}{2\tau^2\sigma^2}\left[ \mu -\frac{x\tau^2+\xi\sigma^2}{\tau^2+\sigma^2}\right]^2}\\
\pi(\mu|x) & \propto \frac{\sqrt{2\pi\frac{\tau^2\sigma^2}{\tau^2+\sigma^2}}}{\sqrt{2\pi\frac{\tau^2\sigma^2}{\tau^2+\sigma^2}}} e^{-\frac{1}{2\frac{\tau^2\sigma^2}{\tau^2+\sigma^2}}\left[ \mu -\frac{x\tau^2+\xi\sigma^2}{\tau^2+\sigma^2}\right]^2}\\
\pi(\mu|x) & \propto \frac{1}{\sqrt{2\pi\frac{\tau^2\sigma^2}{\tau^2+\sigma^2}}}e^{-\frac{1}{2\frac{\tau^2\sigma^2}{\tau^2+\sigma^2}}\left[ \mu -\frac{x\tau^2+\xi\sigma^2}{\tau^2+\sigma^2}\right]^2}\\
\end{array}
\]

Obteniendo la dristribución normal con media
\(\xi(x)=\frac{\sigma^2\xi+\tau^2x}{\sigma^2+\tau^2}\) y la varianza
\(\omega^2=\frac{\sigma^2\tau^2}{\sigma^2+\tau^2}\).

Entonces: \[
\begin{array}{rl}
\Phi\bigg(\frac{-\xi(x)}{\omega}\bigg)=&\Phi\Bigg(-\displaystyle \frac{\displaystyle \sigma^2\xi+\tau^2x}{\sigma^2+\tau^2}\Bigg/{\displaystyle\sqrt{\frac{\sigma^2\tau^2}{\sigma^2+\tau^2}}}\Bigg)\\
=&\Phi\Bigg(-\displaystyle \frac{\displaystyle \sigma^2\xi+\tau^2x}{\sqrt{\sigma^2+\tau^2} \sqrt{\sigma^2\tau^2}}\Bigg)\\
\end{array}
\] ahora vamos a ver que pasa cuando \(\xi\to 0\) \[
\lim_{\xi\to 0}-\displaystyle \frac{\displaystyle \sigma^2\xi+\tau^2x}{\sqrt{\sigma^2+\tau^2} \sqrt{\sigma^2\tau^2}}=-\displaystyle \frac{\displaystyle \tau^2x}{\sqrt{\sigma^2+\tau^2} \sqrt{\sigma^2\tau^2}}
\] ahora vamor a ver que pasa cuando \(\tau\to\infty\) \[
\begin{array}{rl}
\displaystyle \lim_{\tau\to\infty} -\displaystyle \frac{\displaystyle \tau^2x}{\sqrt{\sigma^2+\tau^2} \sqrt{\sigma^2\tau^2}}=& \displaystyle \lim_{\tau\to\infty}-\frac{\displaystyle \tau^2x}{\sqrt{\tau^2(\frac{\sigma^2}{\tau^2}+1}) \sigma\ \tau} \\
=&\displaystyle \lim_{\tau\to\infty}-\frac{\displaystyle \tau^2x}{\tau^2\ \sigma \sqrt{\frac{\sigma^2}{\tau^2}+1}}\\
=&\displaystyle \lim_{\tau\to\infty}-\frac{\displaystyle x}{\sigma \sqrt{\frac{\sigma^2}{\tau^2}+1}}=-\frac{x}{\sigma}
\end{array}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{\texorpdfstring{2.3.2 El \emph{Factor de
Bayes}}{2.3.2 El Factor de Bayes}}\label{el-factor-de-bayes}

Una noción central a las pruebas Bayesianas es el \emph{Factor de Bayes}
\[
B_{10}^\pi=\displaystyle \frac{\frac{P^\pi(\theta\in\Theta_1|x)}{P^\pi(\theta\in\Theta_0|x)}}{\frac{P^\pi(\theta\in\Theta_1)}{P^\pi(\theta\in\Theta_0)}}=\displaystyle \frac{\frac{P^\pi(\mathcal{M}_1|x)}{P^\pi(\mathcal{M}_0|x)}}{\frac{P^\pi(\mathcal{M}_1)}{P^\pi(\mathcal{M}_0)}}
\] que corresponde a las posibilidades clásica (classical odds) o razón
de verosimilitud, la diferencia está en que los parámetros se integran
en lugar de maximizarce bajo cada modelo \(\mathcal{M}_1\) versus
\(\mathcal{M}_0\).

Dado que es una simple transformada uno a uno de la probabilidad
posterior, se puede usar para pruebas Bayesianas sin utilizar una
función de pérdida específica, por ejemplo usando la escala de evidencia
de Jeffreys:

\begin{itemize}
\tightlist
\item
  Si \(0\le\log_{10}(B_{10}^\pi)<\frac{1}{2}\) la evidencia en contra
  \(H_0\) es débil
\item
  Si \(\frac{1}{2}\le\log_{10}(B_{10}^\pi)<1\) la evidencia es
  sustancial
\item
  Si \(1\le\log_{10}(B_{10}^\pi)<2\) la evidencia es fuerte y
\item
  Si \(\log_{10}(B_{10}^\pi)\ge2\) la evidencia es decisiva
\end{itemize}

A pesar de que esta escala está lejos de ser justificada bajo principios
estrictos, provee una referencia para evaluar las hipótesis sin
necesidad de definir las probabilidades \emph{a priori}
\(H_0:\theta\in\Theta_0\) versus \(H_1:\theta\in\Theta_1\), lo cual es
una de las ventajas de utilizar el \emph{Factor Bayesiano}.

En general, el \emph{Factor de Bayes} obviamente depende de la
información \emph{a priori}, pero todavía se considera una respuesta
Bayesiana objetiva en parte por que elimina la influencia del modelo
\emph{a priori} y enfatiza el rol de las observaciones.

Alternativamente, se puede notar (perceived) como una razón de
verosimilitud Bayesiana dado que si \(\pi_0\) y \(\pi_1\) son
distribuciones \emph{a priori} bajo \(H_0\) y \(H_1\), respectivamente y
si \(P^\pi(\theta\in\Theta_0)=P^\pi(\theta\in\Theta_1)=\frac{1}{2}\),
\(B_{10}^\pi\) se puede expresar como \[
B_{10}^\pi=\frac{\displaystyle\int_{\Theta_1}f_\theta(x)\pi_1(\theta)d\theta}{\displaystyle\int_{\Theta_0}f_\theta(x)\pi_0(\theta)d\theta}=\frac{m_1(x)}{m_0(x)}
\] en la cual reemplazamos las verosimilitudes con las marginales bajo
ambas hipótesis.

Cuando la hipótesis que se va a probar es una hipótesis nula puntual,
\(H_0:\theta=\theta_0\), hay dificultades en la construcción de un
procedimiento Bayesiano, dado que, para una distribución \emph{a priori}
contínua \(\pi\) \[
P^\pi(\theta=\theta_0)=0
\]

Obviamente, las hipótesis nulas puntuales se pueden criticar y
considerar como artificiales e imposibles de probar (\emph{¿Con cuanta
frecuencia se puede distinguir entre \(\theta=0\) de
\(\theta=0.0001\)?!}), sin embargo también se deben realizar como parte
de los requerimientos del análisis estadístico del día a día y también
se pueden ver como una representación conveniente de algunos problemas
de escogencia del modelo (model choice problems) que serán tratados
luego.

La prueba de hipótesis nulas puntuales ameritan una modificación de la
distribución \emph{a priori} tal que cuando se prueba
\(H_0:\theta\in\Theta_0\) versus \(H_1:\theta\in\Theta_1\), se cumple
que \[
\pi(\Theta_0)>0\ \ y \ \ \pi(\Theta_1)>0
\] independientemente de los cálculos (measures) de \(\Theta_0\) y
\(\Theta_1\) para la distribución \emph{a priori} original, lo cual
significa que la distribución \emph{a priori} se puede descomponer como
\[
\pi(\theta)=P^\pi(\theta\in\Theta_0)\times\pi_0(\theta)+P^\pi(\theta\in\Theta_1)\times\pi_1(\theta)
\] con pesos positivos en ambos conjuntos \(\Theta_0\) y \(\Theta_1\).

\begin{quote}
Note que esta modificación tiene sentido tanto desde el punto de vista
de la información como operativo. Si \(H_0:\theta=\theta_0\), el hecho
de que la hipótesis es puesta a prueba implica que \(\theta=\theta_0\)
es una posibilidad y trae información \emph{a priori} adicional sobre el
parámetro \(\theta\). Además, si se prueba \(H_0\) y es aceptada, esto
significa, en la mayoría de los casos, que el modelo (reducido) bajo
\(H_0\) se utilizará en lugar del modelo (completo) considerado
anteriormente. Entonces, una distribución \emph{a priori} bajo el modelo
reducido debe estar disponible para inferencias posteriores potenciales.
(Obviamente, el hecho de que esta inferencia posterior depende de la
selección de la hipótesis nula \(H_0\) también se debería tomar en
consideración.)
\end{quote}

En el caso especial \(\Theta_0=\{\theta_0\}\), \(\pi_0\) es la masa de
\emph{Dirac} en \(\theta_0\), que significa que
\(P^{\pi_0}(\theta=\theta_0)=1\), y se requiere introducir un peso
\emph{a priori} separado de \(H_0\), digamos, \[
\rho=P(\theta=\theta_0)\ \text{ y }\ \pi(\theta)=\rho\ 1_{\theta_0}(\theta)\ +\ (1-\rho)\pi_1(\theta) 
\] entonces, \[
\begin{array}{rl}
\displaystyle \pi(\Theta_0|x)&=\displaystyle \frac{f_{\theta_0}(x)\rho}{\displaystyle\int f_\theta(x)\pi(\theta)d\theta} \\
&=\displaystyle \frac{f_{\theta_0}(x)\rho}{f_{\theta_0}(x)\rho\ +\ (1-\rho)m_1(x)}
\end{array}
\]

Para \(x\sim\mathcal{N}(\mu,\sigma^2)\) y
\(\mu\sim\mathcal{N}(\xi,\tau^2)\), considere la prueba \(H_0:\mu=0\).
Podemos escoger \(\xi\) igual a \(0\) si no contamos con información
\emph{a priori} adicional. \[
\begin{array}{rl}
m_1(x)=&\displaystyle\int_{\Theta_1}f_\theta(x)\pi_1(\theta)d\theta\\
=&\displaystyle\int_{\Theta_1} \frac{1}{\sqrt{2\pi}\sigma}\exp\bigg\{-\frac{(x-\mu)^2}{2\sigma^2}\bigg\}
\frac{1}{\sqrt{2\pi}\tau}\exp\bigg\{-\frac{(\mu-\xi)^2}{2\tau^2}\bigg\}d\mu\\
=&\displaystyle\int_{\Theta_1} \frac{1}{\sqrt{2\pi}\sigma}\frac{1}{\sqrt{2\pi}\tau} \exp\bigg\{-\frac{(x-\mu)^2}{2\sigma^2}-\frac{(\mu-\xi)^2}{2\tau^2}\bigg\}d\mu\\
\end{array}
\] vamos a ver el exponente de la exponencial \[
\begin{array}{rl}
-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}-\frac{1}{2}\frac{(\mu-\xi)^2}{\tau^2}=&-\frac{1}{2}\frac{x^2-2x+\mu^2}{\sigma^2}-\frac{1}{2}\frac{\mu^2-2\mu\xi+\xi^2}{\tau^2}\\
=&-\frac{1}{2}\bigg[\frac{x^2}{\sigma^2}-\frac{2x\mu}{\sigma^2}+\frac{\mu^2}{\sigma^2}+\frac{\mu^2}{\tau^2}-\frac{2\mu\xi}{\tau^2}+\frac{\xi^2}{\tau^2}  \bigg]\\
=&-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)+\mu^2\bigg(\frac{1}{\sigma^2}+\frac{1}{\tau^2} \bigg) - 2\mu\bigg( \frac{x}{\sigma^2}+\frac{\xi}{\tau^2}  \bigg)  \bigg]\\
=&-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)+\bigg(\frac{1}{\sigma^2}+\frac{1}{\tau^2} \bigg)\bigg(\mu^2- 2\mu\bigg( \frac{x\tau^2+\xi\sigma^2}{\sigma^2\tau^2} \bigg)\bigg(\frac{\sigma^2\tau^2}{\sigma^2+\tau^2}\bigg)  \bigg]\\
=&-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)+\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg\{\mu^2- 2\mu\bigg( \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg) +\bigg( \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2-\bigg( \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2 \bigg\}  \bigg]\\
=&-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)+\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg\{\bigg(\mu -  \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2-\bigg( \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2 \bigg\}  \bigg]\\
=&-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)+\bigg\{\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg(\mu -  \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2-\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg( \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2 \bigg\}  \bigg]\\
=&-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)+\bigg\{\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg(\mu -  \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}  \bigg\}  \bigg]\\
\end{array}
\]

Ahora desarrollamos \(m_1(x)\) \[
\begin{array}{rl}
m_1(x)=& \displaystyle\int_{\Theta_1} \frac{1}{\sqrt{2\pi}\sigma}\frac{1}{\sqrt{2\pi}\tau} e^{-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)+\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg(\mu -  \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}  \bigg]}d\mu\\
m_1(x)=& \displaystyle\int_{\Theta_1} \frac{1}{\sqrt{2\pi}\sigma}\frac{1}{\sqrt{2\pi}\tau} e^{-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}+\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg(\mu -  \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2\bigg]}d\mu\\
m_1(x)=& \displaystyle\int_{\Theta_1} \frac{1}{\sqrt{2\pi}\sigma}\frac{1}{\sqrt{2\pi}\tau} e^{-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}\bigg]}e^{\frac{1}{2}\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg(\mu -  \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2}d\mu\\
m_1(x)=& \displaystyle \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}\bigg]} \int_{\Theta_1}\frac{1}{\sqrt{2\pi}\sigma\tau} e^{\frac{1}{2}\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg(\mu -\frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2}d\mu\\
m_1(x)=& \displaystyle \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}\bigg]} \int_{\Theta_1}\frac{\sqrt{\sigma^2+\tau^2}}{\sqrt{\sigma^2+\tau^2}}\frac{1}{\sqrt{2\pi}\sigma\tau} e^{\frac{1}{2}\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg(\mu -\frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2}d\mu\\
m_1(x)=& \displaystyle \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}\bigg]} \int_{\Theta_1}\frac{\sqrt{\sigma^2+\tau^2}}{\sqrt{2\pi\sigma^2\tau^2}} e^{\frac{1}{2}\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg(\mu -\frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2}d\mu\\
m_1(x)=& \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}\bigg]}\\
m_1(x)=& \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\bigg[\frac{x^2\tau^2+\xi^2\sigma^2}{\tau^2\sigma^2}-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}\bigg]}
\end{array}
\]

Si \(\xi=0\) tenemos: \[
\begin{array}{rl}
m_1(x)=& \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\left[\frac{x^2\tau^2}{\tau^2\sigma^2}-
\frac{(x\tau^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}\right]}\\
m_1(x)=& \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\left[\frac{x^2}{\sigma^2}-
\frac{x^2\tau^2}{\sigma^2(\sigma^2+\tau^2)}\right]}\\
m_1(x)=& \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\frac{x^2}{\sigma^2}\left[1-
\frac{\tau^2}{\sigma^2+\tau^2}\right]}\\
m_1(x)=& \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\frac{x^2}{\sigma^2}\left[\frac{\sigma^2+\tau^2-\tau^2}{\sigma^2+\tau^2}\right]}\\
m_1(x)=& \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\frac{x^2}{\sigma^2}\left[\frac{\sigma^2}{\sigma^2+\tau^2}\right]}\\
m_1(x)=& \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\frac{x^2}{\sigma^2+\tau^2}}\\
m_1(x)=&\frac{1}{\sqrt{2\pi}\sqrt{\sigma^2+\tau^2}}e^{ -\frac{1}{2}\frac{x^2}{\sigma^2+\tau^2}}
\end{array}
\]

Como \[
f_0(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\bigg\{-\frac{(x-0)^2}{2\sigma^2} \bigg\}=\frac{1}{\sqrt{2\pi}\sigma}\exp\bigg\{-\frac{x^2}{2\sigma^2} \bigg\}\sim\mathcal{N(\mu,\sigma^2)}\text{ con }\mu=0
\]

entonces \(B_{10}^\pi\) \[
\begin{array}{rl}
B_{10}^\pi & =\frac{m_1(x)}{f_0(x)}\\
B_{10}^\pi & =\frac{ \frac{1}{\sqrt{2\pi}\sqrt{\sigma^2+\tau^2}}e^{ -\frac{1}{2}\frac{x^2}{\sigma^2+\tau^2}}}{\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^2}{2\sigma^2}}}\\
B_{10}^\pi & =\frac{\sqrt{\sigma^2}}{\sqrt{\sigma^2+\tau^2}} e^{ -\frac{1}{2}\frac{x^2}{\sigma^2+\tau^2}+\frac{x^2}{2\sigma^2}}\\
B_{10}^\pi & =\frac{\sqrt{\sigma^2}}{\sqrt{\sigma^2+\tau^2}} e^{ -\frac{1}{2}\left( \frac{x^2}{\sigma^2+\tau^2}-\frac{x^2}{\sigma^2}\right)}\\
B_{10}^\pi & =\frac{\sqrt{\sigma^2}}{\sqrt{\sigma^2+\tau^2}} e^{ -\frac{1}{2}\left( \frac{x^2\sigma^2 -x^2(\sigma^2+\tau^2)}{\sigma^2(\sigma^2+\tau^2)}\right)}\\
B_{10}^\pi & =\frac{\sqrt{\sigma^2}}{\sqrt{\sigma^2+\tau^2}} e^{-\frac{1}{2}\left( \frac{-x^2\tau^2}{\sigma^2(\sigma^2+\tau^2)}\right)}\\
B_{10}^\pi & =\frac{\sqrt{\sigma^2}}{\sqrt{\sigma^2+\tau^2}} e^{ \frac{x^2\tau^2}{2\sigma^2(\sigma^2+\tau^2)}}
\end{array}
\]

Entonces el \emph{Factor de Bayes} es la razón de las marginales bajo
ambas hipótesis, \(\mu=0\) y \(\mu\ne 0\), \[
B_{10}^\pi=\frac{m_1(x)}{f_0(x)}=\sqrt{\frac{\sigma^2}{\sigma^2+\tau^2}}e^{\frac{\tau^2x^2}{2\sigma^2(\sigma^2+\tau^2)}}
\] y si partimos de que \(\rho=P(\theta=\theta_0)\) y
\(\pi(\theta)=\rho 1_{\Theta_0}(\theta)+(1-\rho)\pi_1(\theta)\) \[
\begin{array}{rl}
\pi(\Theta_0|x) & =\frac{f_{\Theta_0}(x)\rho}{\int f_{\Theta_0}(x)\pi(\theta)d\theta}\\
\pi(\Theta_0|x) & =\frac{f_{\Theta_0}(x)\rho}{f_{\Theta_0}(x)\rho+(1-\rho)m_1(x)}\\
\pi(\Theta_0|x) & =\frac{\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^2}{2\sigma^2} }\rho}{\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^2}{2\sigma^2} }\rho+(1-\rho)\frac{1}{\sqrt{2\pi}\sqrt{\sigma^2+\tau^2}}e^{ -\frac{1}{2}\frac{x^2}{\sigma^2+\tau^2}}}\\
\pi(\Theta_0|x) & =\frac{\frac{1}{\sigma}e^{-\frac{x^2}{2\sigma^2} }\rho}{\frac{1}{\sigma}e^{-\frac{x^2}{2\sigma^2} }\rho+(1-\rho)\frac{1}{\sqrt{\sigma^2+\tau^2}}e^{ -\frac{1}{2}\frac{x^2}{\sigma^2+\tau^2}}} \frac{\frac{1}{\frac{1}{\sigma}e^{-\frac{x^2}{2\sigma^2} }\rho}}{\frac{1}{\frac{1}{\sigma}e^{-\frac{x^2}{2\sigma^2} }\rho}}\\
\pi(\Theta_0|x) & =\frac{1}{1+(1-\rho)\frac{1}{\sqrt{\sigma^2+\tau^2}}\frac{\sigma}{\rho} e^{ -\frac{1}{2}\frac{x^2}{\sigma^2+\tau^2}}e^{\frac{1}{2}\frac{x^2}{\sigma^2}}}\\
\pi(\Theta_0|x) & =\frac{1}{1+(1-\rho)\frac{1}{\sqrt{\sigma^2+\tau^2}}\frac{\sigma}{\rho} e^{ -\frac{1}{2}\left(\frac{x^2}{\sigma^2+\tau^2}-\frac{x^2}{\sigma^2}\right)}}\\
\pi(\Theta_0|x) & =\frac{1}{1+(1-\rho)\frac{1}{\sqrt{\sigma^2+\tau^2}}\frac{\sigma}{\rho} e^{ -\frac{1}{2}\left(\frac{x^2\sigma^2-x^2(\sigma^2+\tau^2)}{\sigma^2(\sigma^2+\tau^2)}\right)}}\\
\pi(\Theta_0|x) & =\frac{1}{1+\frac{(1-\rho)}{\rho}\frac{\sqrt{\sigma^2}}{\sqrt{\sigma^2+\tau^2}} e^{ \frac{1}{2}\frac{x^2\tau^2}{\sigma^2(\sigma^2+\tau^2)}}}\\
\end{array}
\]

entonces \(\pi(\mu=0|x)\) \[
\pi(\mu=0|x)=\bigg[1\ +\ \frac{1-\rho}{\rho} \displaystyle \sqrt{\frac{\sigma^2}{\sigma^2+\tau^2}}\exp\bigg\{\frac{\tau^2x^2}{2\sigma^2(\sigma^2+\tau^2)} \bigg\}\bigg]^{-1}
\] es la probabilidad posterior de \(H_0\). La Tabla 2.1 ofrece una idea
de los valores de la probabilidad posterior cuando la cantidad
normalizada \(x/\sigma\) varía. Esta probabilidad posterior obviamente
depende de la selección de la variancia \emph{a priori} \(\tau^2\): La
dependencia es en realidad muy severa, como veremos más adelante con la
paradoja de \emph{Jeffreys Lindley}.

\textbf{Tabla 2.1.} Probabilidad posterior de \(\mu=0\) para valores
diferentes de \(z=x/\sigma\), \(\rho=1/2\) y para \(\tau=\sigma\) (fila
superior), \(\tau^2=10\sigma^2\) (fila inferior).

\[
\begin{array}{ccccc}
\hline
z              & 0 & 0.68  & 1.28  & 1.96  \\ \hline
\pi(\mu=0|z)   & 0.586                  & 0.557 & 0.484 & 0.351 \\ \hline
\pi(\mu=0|z)   & 0.768                  & 0.729 & 0.612 & 0.366 \\ \hline
\end{array}
\] Para \textbf{normaldata}, si escogemos \(\tau\) igual a \(0.1\), el
\emph{Factor de Bayes} contra \(\mu=0\) sólo depende de
\(\overline{x}_n\sim\mathcal{N}(\mu,\hat\sigma^2/90)\) y es igual a \[
B_{10}^\pi(\overline{x}_n)=\sqrt{\frac{\hat\sigma^2}{\hat\sigma^2+n\tau^2}}\exp\bigg\{\frac{n\tau^2\ \overline{x}^2_n}{2\hat\sigma^2(\hat\sigma^2+n\tau^2)} \bigg\}=0.1481
\] El conjunto de datos, por tanto, favorece la hipótesis nula
\(H_0:\mu=0\). Sin embargo, otras elecciones de \(\tau\) pueden resultar
en evaluaciones numéricas diferentes, como se muestra en la Figura 2.6,
como el \emph{Factor de Bayes} varía entre \(1\) y \(0\) así como
\(\tau\) crece de \(0\) a \(\infty\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{from<-}\DecValTok{10}\NormalTok{^-}\DecValTok{4}
\NormalTok{to<-}\DecValTok{10}
\NormalTok{step<-}\FloatTok{0.001}

\NormalTok{b10<-}\KeywordTok{c}\NormalTok{()}
\NormalTok{sdsqred<-}\KeywordTok{sd}\NormalTok{(larcenies)^}\DecValTok{2}
\NormalTok{n<-}\KeywordTok{length}\NormalTok{(larcenies)}
\NormalTok{xbarsqred<-}\KeywordTok{mean}\NormalTok{(larcenies)^}\DecValTok{2}

\CommentTok{#testing}
\NormalTok{tau<-}\FloatTok{0.1}
\NormalTok{value<-}\KeywordTok{sqrt}\NormalTok{(sdsqred/(sdsqred+n*tau^}\DecValTok{2}\NormalTok{))*}
\StringTok{  }\KeywordTok{exp}\NormalTok{(n*tau^}\DecValTok{2}\NormalTok{*xbarsqred/(}\DecValTok{2}\NormalTok{*sdsqred*(sdsqred+n*tau^}\DecValTok{2}\NormalTok{)))}
\NormalTok{value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1480872
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{value<-}\DecValTok{0}
\NormalTok{tau<-from}
\NormalTok{while (tau<=to) \{}
  \NormalTok{value<-}\KeywordTok{sqrt}\NormalTok{(sdsqred/(sdsqred+n*tau^}\DecValTok{2}\NormalTok{))*}
\StringTok{    }\KeywordTok{exp}\NormalTok{(n*tau^}\DecValTok{2}\NormalTok{*xbarsqred/(}\DecValTok{2}\NormalTok{*sdsqred*(sdsqred+n*tau^}\DecValTok{2}\NormalTok{)))}
  \NormalTok{b10<-}\KeywordTok{c}\NormalTok{(b10,value)}
  \NormalTok{tau<-tau+step}
\NormalTok{\}}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{seq}\NormalTok{(from,to,step),b10,}\DataTypeTok{type =} \StringTok{"l"}\NormalTok{,}\DataTypeTok{log =} \StringTok{"x"}\NormalTok{,}\DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(from, to))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Capitulo2-Teoria_files/figure-latex/unnamed-chunk-4-1.pdf}

\textbf{Figura 2.6.} Datos \textbf{normaldata}: Rango del \emph{Factor
de Bayes} \(B_{10}^\pi\) cuando \(\tau\) va de \(10^{-4}\) a 10. (El eje
\(x\) está en escala logarítmica)

\subsection{\texorpdfstring{2.3.3 El veto a las distribuciones \emph{a
priori}
impropias}{2.3.3 El veto a las distribuciones a priori impropias}}\label{el-veto-a-las-distribuciones-a-priori-impropias}

Desafortunadamente, esta descomposición de la distribución \emph{a
priori} en dos sub distribuciones \emph{a priori} genera una seria
dificultad relacionada con las distribuciones \emph{a priori} imprópias,
que implica en términos prácticos, el veto de su uso en situaciones de
prueba (testing situations). De hecho, cuando se utiliza la
representación \[
\pi(\theta)=P(\theta\in\Theta_0)\times\pi_0(\theta)+P(\theta\in\Theta_1)\times\pi_1(\theta)
\] los pesos \(P(\theta\in\Theta_0)\) y \(P(\theta\in\Theta_1)\) son
significativos sólo si \(\pi_0\) y \(\pi_1\) son densidades de
probabilidad normalizadas. De lo contrario, no se pueden interpretar
como \emph{pesos}.

Cuando \(x\sim\mathcal{N}(\mu,1)\) y \(H_0:\mu=0\), la distribución
\emph{a priori} imprópia (Jeffreys) es \(\pi_1(\mu)=1\); si escribimos
\[
\pi(\mu)=\frac{1}{2} 1_{0}(\mu)+\frac{1}{2} \cdotp 1
\]

entonces la probabilidad posterior es \[
\pi(\mu=0|x)=  \frac{\displaystyle e^{-\frac{x^2}{2}}}{e^{-\frac{x^2}{2}}\ +\ \displaystyle \int_{-\infty}^{\infty}e^{-\frac{(x-\theta)^2}{2}}d\theta}=\frac{1}{1+\sqrt{2\pi}\ e^{\frac{x^2}{2}}}
\]

Una primera consecuencia de esta elección la probabilidad posterior de
\(H_0\) es acotada superiormente por \[
\pi(\mu=0|x)\le\frac{1}{(1+\sqrt{2\pi})}=0.285
\] La Tabla 2.2 provee la evolución (el cambio) de esta probabilidad
cuando \(x\) se separa del \(0\). Un punto interesante es que los
valores numéricos, de alguna forma, coinciden con los \emph{p}-valores
utilizados en las pruebas de hipótesis clásicas (Casella and Berger,
2001).

\textbf{Tabla 2.2.} Probabilidad posterior de \(H_0:\mu=0\) para la
distribución \emph{a priori} de Jeffrey \(\pi_1(\mu)=1\) bajo \(H_1\).

\[
\begin{array}{cccccc}
\hline
x              & 0     & 1.0   & 1.65  & 1.96  & 2.58  \\ \hline
\pi(\mu=0|x)   & 0.285 & 0.195 & 0.089 & 0.055 & 0.014 \\ \hline
\end{array}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{quote}
Si estamos interesados en probar \(H_0: \theta\le 0\) versus
\(H_1:\theta>0\) entonces la probabilidad posterior es \[
\pi(\theta\le 0|x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^0 e^{-\frac{(x-\theta)^2}{2}}d\theta=\Phi(-x)
\] y la respuesta es ahora \emph{exáctamente} el \emph{p}-valor que
encontramos en la estadística clásica.
\end{quote}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Para \textbf{normaldata}, si consideramos el párametro
\(\theta=(\mu,\sigma^2)\) con una distribución que proviene de una
distribución \emph{a priori} no informativa \(\pi(\theta)=1/\sigma^2\),
la probabilidad posterior de que \(\mu\) sea positiva es una
probabilidad de que una distribución t-Student sea positiva con \(89\)
grados de libertad, media \(-0.0144\) y variancia \(0.000206\). Esto es
esencialmente una distribución normal \(\mathcal{N}(-0.0144,0.000206)\)
y la probabilidad correspondiente es \(0.0021\), la cual es muy pequeña
para la hipótesis \(H_0:\mu>0\) se sostenga bajo cualquier
verosimilitud.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

La dificultad en el uso de una distribución \emph{a priori} imprópia se
relaciona a la llamada paradoja de \emph{Jeffrey Lindley}, un fenómeno
que muestra que los argumentos limitantes (limiting arguments) no son
válidos en situaciones de pruebas de hipótesis. Por el contrario en
estimación, las distribuciones \emph{a priori} no informativas no
corresponden mas al límite de inferencias conjugadas. De hecho, para una
distribución \emph{a priori} conjugada, la probabilidad posterior \[
\pi(\mu=0|x)=\bigg[1\ +\ \frac{1-\rho_0}{\rho_0} \displaystyle \sqrt{\frac{\sigma^2}{\sigma^2+\tau^2}}\exp\bigg\{\frac{\tau^2x^2}{2\sigma^2(\sigma^2+\tau^2)} \bigg\}\bigg]^{-1}
\] converge a \(1\) cuando \(\tau\) va a \(+\infty\), para cada valor de
\(x\), como se ilustró en la Figura 2.6. Este procedimiento no
informativo, obviamente difiere de la respuesta no informativa previa
\([1+\sqrt{2\pi}\exp(x^2/2)]^{-1}\).

El principal obstáculo (bars us) que impide el uso de \emph{a priori}
imprópios en uno o dos conjuntos \(\Theta_0\) y \(\Theta_1\) es una
dificultad para normalizar (normalizing difficulty): Si \(g_0\) y
\(g_1\) son medidas (en lugar de probabilidades) sobre los subespacios
\(\Theta_0\) y \(\Theta_1\), la selección de las constantes
normalizadoras influencia en \emph{Factor de Bayes}. De hecho, cuando
\(g_i\) es reemplazada por \(c_i g_i (i=0,1)\), donde \(c_i\) es una
constante arbitraria, el \emph{Factor de Bayes} es multiplicado por
\(c_0/c_1\). Entonces, por ejemplo, si el \emph{a priori} de Jeffrey es
uniforme y \(g_0=c_0\),\(g_1=c_1\), la probabilidad posterior \[
\begin{array}{rr}
\displaystyle \pi(\theta\in\Theta_0|x)=&\frac{\displaystyle \rho_0c_0\int_{\Theta_0}f(x|\theta)d\theta}{\displaystyle \rho_0c_0\int_{\Theta_0}f(x|\theta)d\theta\ +\ (1-\rho_0)c_1\int_{\Theta_1}f(x|\theta)d\theta}\\
=&\frac{\displaystyle \rho_0\int_{\Theta_0}f(x|\theta)d\theta}{\displaystyle \rho_0\int_{\Theta_0}f(x|\theta)d\theta\ +\ (1-\rho_0)[c_1/c_0]\int_{\Theta_1}f(x|\theta)d\theta}
\end{array}
\] es completamente determinado por la selección de \([c_0/c_1]\). Esto
implica, por ejemplo, que la función \([1+\sqrt{2\pi}\exp(x^2/2)]^{-1}\)
obtenida previamente no tiene ninguna validez.

\begin{quote}
Dado que los \emph{a priori} imprópios son una parte esencial del
enfoque Bayesiano, han habido muchas propuestas para superar este
obstáculo. Muchos utilizan una herramienta (device) que transforma el
\emph{a priori} en una distribución de probabilidad utilizando una
porción de los datos \(\mathcal{D}\) y luego utilizan la otra parte de
los datos para realizar una prueba de hipótesis en una situación
estándar. La variedad de soluciones disponibles se debe a las muchas
posibilidades de eliminar la dependencia en la selección de una porción
de los datos utilizados en la primera parte. El procedimiento resultante
es llamado \emph{pseudo Factor de Bayes}, aunque algunos realmente sean
\emph{Factores de Bayes}. Ver Robert (2001, Capítulo 6) para más
detalles.
\end{quote}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Para \textbf{CMBdata}, considere dos muestras construidas tomando dos
segmentos sobre la imagen, como se ve en la Figura 2.7.. Podemos
considerar esas muestras como, \((x_1,\ldots,x_n)\) y
\((y_1,\ldots,y_n)\), ambas provenientes de una distribución normal,
\(\mathcal{N}(\mu_x, \sigma^2)\) y \(\mathcal{N}(\mu_y, \sigma^2)\). La
pregunta de interés es si ambas medias son iguales, \(H_0:\mu_x=\mu_y\).

\textbf{FALTA LA GRAFICA}

\textbf{Figura 2.7.} Datos \textbf{CMBdata}: Muestras del conjunto de
datos representadas por segmentos de recta.

Para tomar ventaja de la estructura de este modelo, podemos asumir que
\(\sigma^2\) es el mismo error de medición bajo ambos modelos y por
tanto comparten la misma distribución \emph{a priori}. Esto significa
que el \emph{Factor de Bayes} \[
B_{10}^\pi=\frac{\int l(\mu_x,\mu_y,\sigma|\mathcal{D})\pi(\mu_x,\mu_y)\pi_\sigma(\sigma^2)d\sigma^2d\mu_x d\mu_y}{\int l(\mu,\sigma|\mathcal{D})\pi_\mu(\mu)\pi_\sigma(\sigma^2)d\sigma^2d\mu}
\] no depende de la constante de normalización utilizada para
\(\pi_\sigma(\sigma^2)\) y por tanto todavía podemos utilizar \emph{a
priori} imprópios como \(\pi_\sigma(\sigma^2)=1/\sigma^2\) en este caso.
Adicionalmente, podemos reescribir \(\mu_x\) y \(\mu_y\) como
\(\mu_x=\mu-\xi\) y \(\mu_y=\mu+\xi\) respectivamente y utilizar un
\emph{a priori} de la forma \(\pi(\mu,\xi)=\pi_\mu(\mu)\pi_\xi(\xi)\) en
la nueva parametrización, así, de nuevo, el mismo \emph{a priori}
\(\pi_\mu\) se puede utilizar bajo ambas \(H_0\) y su alternativa. La
misma cancelación de la constante de normalización ocurre para
\(\pi_\mu\) y la selección del \emph{a priori} \(\pi_\mu(\mu)=1\) y
\(\xi\sim\mathcal{N}(0,1)\) nos da \[
\begin{array}{rl}
B_{10}^\pi=&\frac{\displaystyle \int\exp-\frac{n}{2\sigma^2}[(\mu-\xi-\bar{x})^2+(\mu+\xi-\bar{y})^2+S^2]\sigma^{-2n-2}e^{-\xi^2/2}/\sqrt{2\pi}\ d\sigma^2\ d\mu \ d\xi}
{\displaystyle \int\exp-\frac{n}{2\sigma^2}[(\mu-\bar{x})^2+(\mu-\bar{y})^2+S^2]\sigma^{-2n-2}\ d\sigma^2\ d\mu}\\
=&\frac{\displaystyle \int[(\mu-\xi-\bar{x})^2+(\mu+\xi-\bar{y})^2+S^2]^{-n}e^{-\xi^2/2}/\sqrt{2\pi}\ d\mu \ d\xi}
{\displaystyle \int[(\mu-\bar{x})^2+(\mu-\bar{y})^2+S^2]^{-n}\ d\mu}
\end{array}
\]

donde \(S\) denota la media \[
S=\frac{1}{n}\sum_{i=1}^{n}(x_i-\bar{x})^2+\frac{1}{n}\sum_{i=1}^{n}(y_i-\bar{y})^2
\]

Mientras que el denominador se puede integrar, el numerador no. Una
aproximación numérica de \(B_{10}^\pi\) es necesaria. Este asunto será
tratado en la Sección 2.4.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Ejercicio 2.21.}\label{ejercicio-2.21.}

Recordemos que la constante de normalización para la distribución
t-Student \(\tau(\nu,\mu,\sigma^2)\) es \[
\displaystyle \frac{\frac{\Gamma\bigg(\frac{\nu+1}{2}\bigg)}{\Gamma\big(\frac{\nu}{2}\big)}}{\sigma\sqrt{\nu\pi}}=\displaystyle \frac{\Gamma\bigg(\frac{\nu+1}{2}\bigg)}{\sigma\sqrt{\nu\pi}\ \Gamma\big(\frac{\nu}{2}\big)}
\]

Calcule el valor de la integral en el denominador de \(B_{10}^\pi\)
previo.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Primero vamos a desarrollar \[
\begin{array}{rl}
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&(\mu^2-2\mu\bar{x}+\bar{x}^2)+(\mu^2-2\mu\bar{y}+\bar{y}^2)\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2\mu^2-2\mu(\bar{x}+\bar{y})+\bar{x}^2+\bar{y}^2\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2(\mu^2-\mu(\bar{x}+\bar{y}))+\bar{x}^2+\bar{y}^2\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2(\mu^2-\mu(\bar{x}+\bar{y}) + \big(\frac{\bar{x}+\bar{y}}{2}\big)^2-\big(\frac{\bar{x}+\bar{y}}{2}\big)^2)+\bar{x}^2+\bar{y}^2\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2([\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2-\big(\frac{\bar{x}+\bar{y}}{2}\big)^2)+\bar{x}^2+\bar{y}^2\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2-2\big(\frac{\bar{x}+\bar{y}}{2}\big)^2+\bar{x}^2+\bar{y}^2\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2-\big(\frac{\bar{x}^2+2\bar{x}\bar{y}+\bar{y}^2}{2}\big)+2\frac{\bar{x}^2}{2}+2\frac{\bar{y}^2}{2}\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2-\frac{\bar{x}^2+2\bar{x}\bar{y}+\bar{y}^2-2\bar{x}^2-2\bar{y}^2}{2}\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2+\frac{\bar{x}^2-2\bar{x}\bar{y}+\bar{y}^2}{2}\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2+\frac{(\bar{x}-\bar{y})^2}{2}
\end{array}
\] sustituyendo esta expresión en el denominador de la integral
anterior, tenemos \[
\begin{array}{rl}
\displaystyle \int[(\mu-\bar{x})^2+(\mu-\bar{y})^2+S^2]^{-n}d\mu=&\displaystyle \int \Bigg(2[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2+\frac{(\bar{x}-\bar{y})^2}{2}+S^2\Bigg)^{-n}d\mu\\
=&\displaystyle \int\bigg(2\bigg[ [\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2+\frac{(\bar{x}-\bar{y})^2}{4}+\frac{S^2}{2} \bigg]\bigg)^{-n}d\mu\\
=&2^{-n}\displaystyle \int\bigg( [\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2+\frac{(\bar{x}-\bar{y})^2}{4}+\frac{S^2}{2}\bigg)^{-n}d\mu
\end{array}
\] si \(\nu=2n-1\) entonces \(n=(\nu+1)/2\), sustituyendo \[
\begin{array}{rl}
\displaystyle \int[(\mu-\bar{x})^2+(\mu-\bar{y})^2+S^2]^{-n}d\mu=&2^{-(\nu+1)/2}\displaystyle \int\bigg( [\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2+\frac{(\bar{x}-\bar{y})^2}{4}+\frac{S^2}{2}\bigg)^{-(\nu+1)/2}d\mu
\end{array}
\]

si \(\sigma^2=\frac{\frac{(\bar{x}-\bar{y})^2}{4}+\frac{S^2}{2}}{\nu}\)
queda \[
\begin{array}{rl}
& \displaystyle \int[(\mu-\bar{x})^2+(\mu-\bar{y})^2+S^2]^{-n}d\mu=2^{-(\nu+1)/2}\displaystyle \int\left( [\mu- \left(\frac{\bar{x}+\bar{y}}{2}\right)]^2+\sigma^2\nu\right)^{-(\nu+1)/2}d\mu\\
=&2^{-(\nu+1)/2}\displaystyle \int\bigg( \sigma^2\nu\bigg[ \frac{[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2}{\sigma^2\nu}+1\bigg]\bigg)^{-(\nu+1)/2}d\mu\\
=&(2\sigma^2)^{-(\nu+1)/2}\displaystyle \int\nu^{-(\nu+1)/2}\bigg( \frac{[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2}{\sigma^2\nu}+1\bigg)^{-(\nu+1)/2}d\mu\\
=&(2\sigma^2)^{-(\nu+1)/2}\frac{\sigma\sqrt{\nu\pi}\ \Gamma\big(\frac{\nu}{2}\big)}{\Gamma\bigg(\frac{\nu+1}{2}\bigg)} \displaystyle \int \frac{\Gamma\bigg(\frac{\nu+1}{2}\bigg)}{\sigma\sqrt{\nu\pi}\ \Gamma\big(\frac{\nu}{2}\big)}\nu^{-(\nu+1)/2}\bigg( \frac{[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2}{\sigma^2\nu}+1\bigg)^{-(\nu+1)/2}d\mu\\
=&(2\sigma^2)^{-(\nu+1)/2}\frac{\sigma\sqrt{\nu\pi}\ \Gamma\big(\frac{\nu}{2}\big)}{\Gamma\bigg(\frac{\nu+1}{2}\bigg)} \\
=&(2\sigma^2)^{-n}\frac{\sigma\sqrt{\nu\pi}\ \Gamma\big(\frac{\nu}{2}\big)}{\Gamma\bigg(\frac{\nu+1}{2}\bigg)} \\
=&2^{-n}\sigma^{-2n+1}\frac{\sqrt{\nu\pi}\ \Gamma\big(\frac{\nu}{2}\big)}{\Gamma\bigg(\frac{\nu+1}{2}\bigg)} \\
=&\displaystyle\frac{\sqrt{\nu\pi}\ \Gamma\big(\frac{\nu}{2}\big)}{2^{n}\sigma^{2n-1}\Gamma\bigg(\frac{\nu+1}{2}\bigg)} \\
=&\displaystyle\frac{\sqrt{\nu\pi}\ \Gamma\big(\frac{\nu}{2}\big)}{2^{n}\Bigg\{\sqrt{\frac{\frac{(\bar{x}-\bar{y})^2}{4}+\frac{S^2}{2}}{\nu}}   \Bigg\}^{2n-1}\Gamma\bigg(\frac{\nu+1}{2}\bigg)} \\
=&\displaystyle \frac{\nu^n \sqrt{\pi}\ \Gamma\big(\frac{\nu}{2}\big)}{2^{n}\Bigg\{\sqrt{\frac{(\bar{x}-\bar{y})^2}{4}+\frac{S^2}{2}}   \Bigg\}^{2n-1}\Gamma\bigg(\frac{\nu+1}{2}\bigg)} 
\end{array}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}


\end{document}
