---
title: "Bayesian Core - Capítulo 2 - Normal Models"
author: "Daysi Febles y Alexander Ramírez"
date: "30/1/2017"
output:
  pdf_document:
    fig_caption: yes
    includes:
      in_header: styles.sty
    keep_tex: yes
    number_sections: no
    toc: yes
    toc_depth: 4
  html_document:
    keep_md: yes
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: no
      smooth_scroll: no
  md_document:
    toc: yes
    toc_depth: 4
    variant: markdown_github
  github_document:
    toc: yes
    toc_depth: 4
  word_document:
    keep_md: yes
    toc: yes
    toc_depth: 4
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, echo = FALSE, warning=FALSE}
library(readr)
set.seed(77777)
normal_fit<-function(data,color) {
  xfit<-seq(min(data),max(data),length=100)
  yfit<-dnorm(xfit,mean=mean(data),sd=sd(data))
  lines(xfit, yfit, col=color, lwd=2)
}
gen_cords_pairs<-function() {
  coords<-c(trunc(runif(4)*800))
  if(coords[1]>coords[3]) {
    temp<-coords[1]
    coords[1]<-coords[3]
    coords[3]<-temp
  }
  if(coords[2]>coords[4]) {
    temp<-coords[2]
    coords[2]<-coords[4]
    coords[4]<-temp
  }
  coords
}
```

```{r, echo=FALSE,warning=FALSE, message=FALSE}
normaldata <- read_delim("normaldata", " ", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
larcenies<-c(normaldata$X1,normaldata$X2,normaldata$X3,normaldata$X4,normaldata$X5)
hist(larcenies,breaks = 21)
```

# 2.2 Hablando Bayesiano

## 2.2.1 Bases

-----

### Ejercicio 2.5

**Muestre que $\pi(\theta|\mathcal{D})=\frac{l(\theta|\mathcal{D})\pi(\theta)}{\int l(\theta|\mathcal{D})\pi(\theta) d\theta}$ puede ser calculada por el ajuste de $\theta$ como una variable aleatoria con función de densidad $\pi$ y entonces la distribución condicional $D$ en $\theta$ como una distribución de $l(\theta|\mathcal{D})$**

Por el teorema de Bayes tenemos 
$$\pi(\theta| \mathcal{D})=\frac{\pi(\theta, \mathcal{D})}{\pi({\mathcal{D}})}$$
Lo que implica 

$$\pi(\theta, \mathcal{D})=\pi(\theta| \mathcal{D})\pi({\mathcal{D}})=\pi(\mathcal{D}|\theta)\pi(\theta)$$

Si usamos $L(\theta|\mathcal{D})$ como estimador de $\pi(\theta|\mathcal{D})$ tenemos:
$$\pi(\theta| \mathcal{D})=\frac{\pi(\mathcal{D}|\theta)\pi(\theta)}{\pi({\mathcal{\mathcal{D}}})}\propto L(\mathcal{D}|\theta)\pi(\theta)$$

> 1.1 **Si suponemos que el cambio relativo en los reportes de hurtos tienen una media $\mu$ entre -0.5 y 0.5 y no provee una información a priori, cambiaremos la priori por una distribución Uniforme $\mathcal{U}(-0.5,0.5)$**
   
La distribución a posteriori viene dada por
$$\pi(\theta|\mathcal{D})=\frac{L(\theta|\mathcal{D})\pi(\theta)}{\int L(\theta|\mathcal{D})\pi(\theta)d\theta}$$

Donde asumiremos que los datos $x_1,x_2,...,x_n$ son normales $\mathcal{N}(\mu,\sigma^2)$ iid, entonces la verosimilitud viene dada por:
$$
\begin{array}{rl}
L(\theta|\mathcal{D}) & = f_\theta(x_1)f_\theta(x_2)...f_\theta(x_n)\\
L(\theta|\mathcal{D}) & = \prod_{i=1}^n f_\theta(x_i)\\
L(\theta|\mathcal{D}) & = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}e^{\frac{1}{2\sigma^2}(x_i-\mu)^2}\\
L(\theta|\mathcal{D}) & = \left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2}
\end{array}
$$

Desarrollando $\sum_{i=1}^n(x_i-\mu)^2$ tenemos 
$$
\begin{array}{rl}
\sum_{i=1}^n(x_i-\mu)^2 & = \sum_{i=1}^n (x_i^2-2\mu x_i +\mu^2)\\
\sum_{i=1}^n(x_i-\mu)^2 & = \sum_{i=1}^n x_i^2-2\mu \sum_{i=1}^n x_i +\sum_{i=1}^n\mu^2\\
\sum_{i=1}^n(x_i-\mu)^2 & = \sum_{i=1}^n x_i^2-2\mu n\frac{\sum_{i=1}^n x_i}{n} +n\mu^2\\
\sum_{i=1}^n(x_i-\mu)^2 & = \sum_{i=1}^n x_i^2-2\mu n\bar x +n\mu^2\\
\sum_{i=1}^n(x_i-\mu)^2 & = \sum_{i=1}^n x_i^2-2\mu n\bar x +n\mu^2 +n\bar x^2 - n\bar x^2\\
\sum_{i=1}^n(x_i-\mu)^2 & = \sum_{i=1}^n x_i^2+n(-2\mu \bar x +\mu^2 +\bar x^2) - n\bar x^2\\
\sum_{i=1}^n(x_i-\mu)^2 & = \sum_{i=1}^n x_i^2 - n\bar x^2 + n(\mu -\bar x)^2 \\
\end{array}
$$

Sustituyendo:
$$
\begin{array}{rl}
L(\theta|\mathcal{D}) & = \left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n e^{-\frac{1}{2\sigma^2}\left(\sum_{i=1}^n x_i^2 - n\bar x^2 + n(\mu -\bar x)^2\right)}\\
L(\theta|\mathcal{D}) & =\left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2 - n\bar x^2 }e^{\frac{1}{2\sigma^2} n(\mu -\bar x)^2}\\
\end{array}
$$

Sustituyendo esta verosimilitud en la distribución a posteriori:
$$
\begin{array}{rl}
\pi(\theta|\mathcal{D}) & = \displaystyle \frac{\left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2 - n\bar x^2 }e^{-\frac{1}{2\sigma^2} n(\mu -\bar x)^2} 1_{[-0.5,0.5]}}{\int \left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2 - n\bar x^2 }e^{-\frac{1}{2\sigma^2} n(\theta -\bar x)^2} 1_{[-0.5,0.5]}d\theta}\\
\pi(\theta|\mathcal{D}) &= \displaystyle \frac{\left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2 - n\bar x^2 }e^{-\frac{1}{2\sigma^2} n(\mu -\bar x)^2} 1_{[-0.5,0.5]}}{\left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2 - n\bar x^2 }\int_{-0.5}^{0.5} e^{-\frac{1}{2\sigma^2} n(\theta -\bar x)^2}d\theta}\\
\pi(\theta|\mathcal{D}) &=\displaystyle \frac{e^{-\frac{1}{2\sigma^2} n(\mu -\bar x)^2} 1_{[-0.5,0.5]}}{\int_{-0.5}^{0.5} e^{-\frac{1}{2\sigma^2} n(\theta -\bar x)^2}d\theta}\\
\end{array}
$$
La cual es una distribución normal truncada.

-----

### Ejercicio 2.6

**Muestre que la minimización de $\hat{\theta}$ en la esperanza $E[L(\theta,\hat{\theta})|D]$, es la esperanza de la función de perdida cuadrática sobre la distribución con función de densidad $\pi(\theta|D)$, produce el valor posterior esperado de la solución $\hat\theta$**

La función de perdida se define como $L(\theta, \hat{\theta})=\|\theta, \hat{\theta}\|^2$, entonces:
$$
\begin{array}{rl}
\mathbb{E}(L(\theta, \hat{\theta})|\mathcal{D}) & = \mathbb{E}(\|\theta- \hat{\theta}\|^2|\mathcal{D})\\
\mathbb{E}(L(\theta, \hat{\theta})|\mathcal{D}) & = \mathbb{E}(<\theta-\hat{\theta}><\theta-\hat{\theta}>|\mathcal{D})\\
\mathbb{E}(L(\theta, \hat{\theta})|\mathcal{D}) & = \mathbb{E}(\theta^t\theta-\theta^t\hat{\theta}-\hat{\theta}^t\theta+\hat{\theta}^t\hat{\theta}|\mathcal{D})\\
\mathbb{E}(L(\theta, \hat{\theta})|\mathcal{D}) & = \mathbb{E}(\|\theta\|^2-2\hat{\theta}^t\theta+\|\hat{\theta}\|^2|\mathcal{D})\\
\mathbb{E}(L(\theta, \hat{\theta})|\mathcal{D}) & = \mathbb{E}(\|\theta\|^2|\mathcal{D})-2\mathbb{E}(\hat{\theta}^t\theta|\mathcal{D})+\mathbb{E}(\|\hat{\theta}\|^2|\mathcal{D})\\
\mathbb{E}(L(\theta, \hat{\theta})|\mathcal{D}) & = \| \mathbb{E}(\theta|\mathcal{D})\|^2-2\hat{\theta}^t\mathbb{E}(\theta|\mathcal{D})+\|\hat{\theta}\|^2\\
\mathbb{E}(L(\theta, \hat{\theta})|\mathcal{D}) & = \| \mathbb{E}(\theta|\mathcal{D})-\hat{\theta}\|^2
\end{array}
$$

## 2.2.2  Distribuciones a Priori

-----

### Ejercicio 2.7

**Muestre que la distribución Normal es de la familia exponencial**

La función de densidad de una distribución normal es $f_\theta(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}$, para probar que esta función es de la familia exponencial tenemos que escribirla de la forma 
$$f_\theta(x)=h(x)e^{\theta . R(x)-\psi(\theta)}$$
Entonces:
$$
\begin{array}{rl}
f_\theta(x) & =\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}\\
f_\theta(x) & =\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^2}(x^2-2x\mu+\mu^2)}\\
f_\theta(x) & =\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2\sigma^2}(x^2-2x\mu)}e^{-\frac{1}{2\sigma^2} \mu^2}\\
f_\theta(x) & =\frac{1}{\sqrt{2\pi}}\frac{e^{-\frac{1}{2\sigma^2} \mu^2}}{\sigma} e^{-\frac{1}{2\sigma^2}(x^2-2x\mu)}\\
f_\theta(x) & =\frac{1}{\sqrt{2\pi}}\frac{e^{-\frac{1}{2\sigma^2} \mu^2}}{\sigma} e^{-\frac{1}{2\sigma^2}x^2+\frac{1}{2\sigma^2}2x\mu}\\
f_\theta(x) & =\frac{1}{\sqrt{2\pi}}\frac{e^{-\frac{1}{2\sigma^2} \mu^2}}{\sigma} e^{-\frac{1}{2\sigma^2}x^2+\frac{1}{\sigma^2}x\mu}\\
\end{array}
$$

Notemos que:
$$
\displaystyle -\frac{1}{2\sigma^2}x^2+\frac{1}{\sigma^2}x\mu = \begin{bmatrix}
-\frac{1}{2\sigma^2} & \frac{\mu}{\sigma^2}
\end{bmatrix} 
\begin{bmatrix}
x^2 \\ x
\end{bmatrix} = \theta. R(x)
$$

$$\displaystyle \frac{e^{-\frac{1}{2\sigma^2} \mu^2}}{\sigma}=\frac{e^{-\frac{1}{2\sigma^2} \mu^2}}{e^{\ln(\sigma)}}= e^{-\frac{1}{2\sigma^2} \mu^2-\ln(\sigma)}=e^{-\left(\frac{1}{2\sigma^2} \mu^2+\ln(\sigma)\right)}= e^{-\psi(\theta)}$$ 

$$\frac{1}{\sqrt{2\pi}}=h(x)$$
Asi tenemos la normal es de la familia exponencial.

-----

### Ejercicio 2.8

**Muestre que para una familia exponencial $\psi(\theta)$ es definida por la restricción de que $f_\theta$ es una función de densidad y que el valor esperado de esta distribución puede ser escrito como $\frac{\partial \psi(\theta)}{\partial \theta}$ el vector de las derivadasde $\psi (\theta)$ con respecto a la componente de $\theta$**

Por ser $f_\theta$ una función de densidad de la familia exponencial:

$\int f_\theta (y) dy = \int h(y)e^{\theta . R(y)-\psi(\theta)} dy =1$

Así:
$$
\begin{array}{rl}
\int h(y)e^{\theta R(y)-\psi(\theta)} dy =1\\
\int h(y)e^{\theta R(y)}e^{-\psi(\theta)} dy =1\\
e^{-\psi(\theta)} \int h(y)e^{\theta R(y)} dy =1\\
e^{\psi(\theta)} = \int h(y)e^{\theta R(y)} dy
\end{array}
$$

$R(y)$ es un estadístico suficiente, por lo tanto:
$$\mathbb{E}(R(y)) =\int R(y)h(y)e^{\theta . R(y)-\psi(\theta)} dy$$

Observemos que $\frac{\partial (\theta R(y))}{\partial \theta}=R(y)$, así:
$$
\begin{array}{rl}
\mathbb{E}(R(y)) & =\displaystyle \int \frac{\partial (\theta R(y))}{\partial \theta} h(y)e^{\theta  R(y)-\psi(\theta)} dy\\
\mathbb{E}(R(y)) & = \displaystyle \int \frac{\partial (\theta R(y)+\psi(\theta)-\psi(\theta))}{\partial \theta} h(y)e^{\theta  R(y)-\psi(\theta)} dy\\
\mathbb{E}(R(y)) & = \displaystyle \int \frac{\partial (\theta R(y)-\psi(\theta))}{\partial \theta} h(y)e^{\theta  R(y)-\psi(\theta)} dy\\
 & +\displaystyle \int \frac{\partial (\psi(\theta))}{\partial \theta} h(y)e^{\theta  R(y)-\psi(\theta)} dy\\
\mathbb{E}(R(y)) & = \displaystyle \int \frac{\partial (\theta R(y)-\psi(\theta))}{\partial \theta} h(y)e^{\theta  R(y)-\psi(\theta)} dy\\
& +\displaystyle \frac{\partial (\psi(\theta))}{\partial \theta} \int h(y)e^{\theta  R(y)-\psi(\theta)} dy\\
\end{array}
$$

Observemos que $\frac{\partial (e^{\theta R(y)-\psi(\theta))}}{\partial \theta}=e^{\theta R(y)-\psi(\theta)}\frac{\partial (\theta R(y)-\psi(\theta))}{\partial \theta}$ así:
$$
\begin{array}{rl}
\mathbb{E}(R(y)) & = \displaystyle \int  h(y) \frac{\partial (e^{\theta R(y)-\psi(\theta)})}{\partial \theta} dy +\displaystyle \frac{\partial (\psi(\theta))}{\partial \theta}  1\\
\mathbb{E}(R(y)) & = \displaystyle \frac{\partial}{\partial \theta} \int  h(y)  e^{\theta R(y)-\psi(\theta)} dy +\displaystyle \frac{\partial (\psi(\theta))}{\partial \theta}\\
\mathbb{E}(R(y))& = \displaystyle \frac{\partial}{\partial \theta} 1 +\displaystyle \frac{\partial (\psi(\theta))}{\partial \theta}\\
\mathbb{E}(R(y))& = \displaystyle \frac{\partial (\psi(\theta))}{\partial \theta}\\
\end{array}
$$

-----

### Ejercicio 2.9

**Muestre que los hiperparametros actualizados estan dado por $\xi'(y) = \xi+R(y)$ y $\lambda'(y)=\lambda +1$, encuentre la expresi?n correspondiente para $\pi (\theta | \xi, \lambda, y_1,...,y_n)$**

Cuando el modelo es de la familia de distribuciones exponencial, existe una clase genérica de prioris llamadas clase de prioris conjugadas $\pi (\theta| \xi, \lambda)\propto e^{\theta.\xi -\lambda\psi(\theta)}$. Esta distribución a priori parametrizada en $\theta$ es tal que la distribución a posteriori tiene la misma forma, veamos esto:
$$
\begin{array}{rl}
\pi(\theta|\xi, \lambda) & \propto f_\theta(y)\pi(\theta|\xi,\lambda)\\
& = h(y)e^{\theta R(y)-\psi(\theta)} e^{\theta\xi -\lambda\psi(\theta)}\\
& \propto e^{\theta R(y)-\psi(\theta)} e^{\theta\xi -\lambda\psi(\theta)}\\
& = e^{\theta R(y)-\psi(\theta)+\theta\xi -\lambda\psi(\theta)}\\
& = e^{\theta(R(y)-\xi) -\psi(\theta)(1+\lambda)}\\
& = e^{\theta\xi'(y) -\psi(\theta)\lambda'(y)}
\end{array}
$$

La distribución a posteriori será:
$$
\begin{array}{rl}
\pi (\theta | \xi, \lambda, y_1,...,y_n) & \propto f_\theta(y)\pi(\theta|\xi, \lambda)\\
\pi (\theta | \xi, \lambda, y_1,...,y_n)& = \prod_{i=1}^n h(y_i)e^{\theta R(y_i)-\psi(\theta)}\pi(\theta|\xi, \lambda)\\
\pi (\theta | \xi, \lambda, y_1,...,y_n)& \propto \prod_{i=1}^n e^{\theta R(y_i)-\psi(\theta)}\pi(\theta|\xi, \lambda)\\
\pi (\theta | \xi, \lambda, y_1,...,y_n)& = e^{\sum_{i=1}^n (\theta R(y_i)-\psi(\theta))}\pi(\theta|\xi, \lambda)\\
\pi (\theta | \xi, \lambda, y_1,...,y_n)& = e^{\sum_{i=1}^n (\theta R(y_i)-\psi(\theta))}e^{\theta\xi -\lambda\psi(\theta)}\\
\pi (\theta | \xi, \lambda, y_1,...,y_n)& = e^{\sum_{i=1}^n (\theta R(y_i)-\psi(\theta))+\theta\xi -\lambda\psi(\theta)}\\
\pi (\theta | \xi, \lambda, y_1,...,y_n)& = e^{ \theta\sum_{i=1}^n R(y_i)-n\psi(\theta)+\theta\xi -\lambda\psi(\theta)}\\
\pi (\theta | \xi, \lambda, y_1,...,y_n) & = e^{ \theta(\sum_{i=1}^n R(y_i)+\xi)-\psi(\theta)(n+\lambda)}\\
\end{array}
$$

Así $\xi(y_1,...,y_n)=\sum_{i=1}^n R(y_i)+\xi$ y $\lambda(y_1,...,y_n)=n+\lambda$
$$\pi (\theta | \xi, \lambda, y_1,...,y_n)\propto e^{ \theta\xi(y_1,...,y_n)-\psi(\theta)\lambda(y_1,...,y_n)}$$

Si tenemos una normal $\mathcal{N}(\mu,1)$ este es un caso de la familia exponencial, es decir:
$$
\begin{array}{rl}
f_\theta(x)&=\frac{1}{\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2}}\\
f_\theta(x)&=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2-2x\mu+ \mu^2}{2}}\\
f_\theta(x)&=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}+\frac{2x\mu- \mu^2}{2}}\\
f_\theta(x)&=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} e^{x\mu-\frac{\mu^2}{2}}\\
\end{array}
$$
donde $h(x)=\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}$, $\psi(\theta)=\frac{\mu^2}{2}$, $R(x)=x$ y $\theta=\mu$. La priori puede escribirse como:
$$
\begin{array}{rl}
\pi(\mu|\xi,\lambda) & \propto e^{\theta\xi-\lambda\psi(\theta)}\\
\pi(\mu|\xi,\lambda) & = e^{\mu\xi-\lambda\frac{\mu^2}{2}}
\end{array}
$$

Veamos que ésto implica que la priori conjugada de $\mu$ es también una normal $\mathcal{N}(\lambda^{-1}\xi,\lambda^{-1})$
$$
\begin{array}{rl}
\pi(\mu|\xi,\lambda) & \propto  e^{\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda) & =  e^{-\frac{\lambda}{2}[\mu^2-2\mu\frac{\xi}{\lambda} +(\frac{\xi}{\lambda})^2- (\frac{\xi}{\lambda})^2]}\\
\pi(\mu|\xi,\lambda) & =  e^{-\frac{\lambda}{2}[\mu^2-2\mu\frac{\xi}{\lambda} +(\frac{\xi}{\lambda})^2] +\frac{\lambda}{2}(\frac{\xi}{\lambda})^2}\\
\pi(\mu|\xi,\lambda) & =  e^{-\frac{\lambda}{2}(\mu-\frac{\xi}{\lambda})^2} e^{\frac{\lambda}{2}\frac{\xi^2}{\lambda^2}}\\
\pi(\mu|\xi,\lambda) & =  e^{-\frac{1}{2(\frac{1}{\lambda})}(\mu-\frac{\xi}{\lambda})^2} e^{\frac{\xi^2}{2\lambda}}\\
\pi(\mu|\xi,\lambda) & \propto  e^{-\frac{1}{2(\frac{1}{\lambda})}(\mu-\frac{\xi}{\lambda})^2} \\
\end{array}
$$
Es una normal con media $\frac{\xi}{\lambda}$ y varianza $\frac{1}{\lambda}$.

Para calcular la distribución a priori de $\mu$ usaremos el teorema de Bayes, recordemos que estamos suponiendo que $X\sim \mathcal{N}(\mu,1)$ por lo tanto 
$$
\begin{array}{rl}
f_\theta(x) & =\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x-\mu)^2}\\
f_\theta(x) & =\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x^2-2x\mu+\mu^2)}\\
f_\theta(x) & =\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2} e^{\frac{1}{2}(2x\mu-\mu^2)}\\
f_\theta(x) & =\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2} e^{x\mu-\frac{\mu^2}{2}}\\
\end{array}
$$

La distribución a priori de $\mu$ es:
$$
\begin{array}{rl}
\pi(\mu|x,\xi,\lambda) & \propto \pi(x|\mu)\pi(\mu|\xi,\lambda)\\
\pi(\mu|x,\xi,\lambda) & = e^{x\mu-\frac{\mu^2}{2}}e^{\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|x,\xi,\lambda) & = e^{x\mu-\frac{\mu^2}{2}+\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|x,\xi,\lambda) & = e^{-\frac{\mu^2}{2}(1+\lambda)+ x\mu +\mu\xi}\\
\pi(\mu|x,\xi,\lambda) & = e^{-\frac{(1+\lambda)}{2}[\mu^2- x\mu\frac{2}{1+\lambda} -\mu\xi\frac{2}{1+\lambda}]}\\
\pi(\mu|x,\xi,\lambda) & = e^{-\frac{(1+\lambda)}{2}[\mu^2- 2\mu(\frac{x}{1+\lambda} + \frac{\xi}{1+\lambda})]}\\
\pi(\mu|x,\xi,\lambda) & = e^{-\frac{(1+\lambda)}{2}[\mu^2- 2\mu(\frac{x+\xi}{1+\lambda})+ (\frac{x+\xi}{1+\lambda})^2- (\frac{x+\xi}{1+\lambda})^2]}\\
\pi(\mu|x,\xi,\lambda) & = e^{-\frac{(1+\lambda)}{2}[(\mu- \frac{x+\xi}{1+\lambda})^2- (\frac{x+\xi}{1+\lambda})^2]}\\
\pi(\mu|x,\xi,\lambda) & = e^{-\frac{(1+\lambda)}{2}(\mu- \frac{x+\xi}{1+\lambda})^2}e^{- (\frac{x+\xi}{1+\lambda})^2}\\
\pi(\mu|x,\xi,\lambda) & \propto e^{-\frac{(1+\lambda)}{2}(\mu- \frac{x+\xi}{1+\lambda})^2}
\end{array}
$$

Con lo cual obtenemos una distribución normal con media $\frac{x+\xi}{1+\lambda}$ y varianza $\frac{1}{(1+\lambda)}$

-----

### Ejecicio 2.10

**Calcular la distribución posterior para una muestra simple iid $\mathcal{D}=(x_1,x_2,...,x_n)$ de $\mathcal{N}(\mu,1)$ y muestre que esto solo depende del estadístico suficiente $\bar{x}=\frac{\sum_{i=1}^n x_{i}}{n}$**

De lo anterior sabemos que la priori conjugada es $\pi(\mu|\xi,\lambda) = e^{\mu\xi-\lambda\frac{\mu^2}{2}}$ y la verosimilitud de los datos es $L(\mu|\mathcal{D})=\prod_{i=1}^n \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x_i-\mu)^2}$, sustituyendo:
$$
\begin{array}{rl}
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto L(\mu|\mathcal{D})\pi(\mu|\xi,\lambda)\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x_i-\mu)^2}e^{\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto  \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} \sum_{i=1}^{n}(x_i-\mu)^2}e^{\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} \sum_{i=1}^{n}(x_i-\mu)^2+\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} \sum_{i=1}^{n}(x_i^2-2\mu x_i +\mu^2)+\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} (\sum_{i=1}^{n}x_i^2-2\mu n \bar{x} +n\mu^2)+\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & = \frac{1}{\sqrt{2\pi}}e^{-\frac{\sum_{i=1}^{n}x_i^2}{2} +n \mu \bar{x} -\frac{n\mu^2}{2}+\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & = \frac{1}{\sqrt{2\pi}}e^{-\frac{\sum_{i=1}^{n}x_i^2}{2} +n \mu \bar{x} -\frac{n\mu^2}{2}+\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & = \frac{1}{\sqrt{2\pi}}e^{-\frac{\sum_{i=1}^{n}x_i^2}{2} +n \mu \bar{x} -\frac{n\mu^2}{2}+\mu\xi-\lambda\frac{\mu^2}{2}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ n \mu \bar{x} -\frac{n\mu^2}{2}+\mu\xi-\lambda\frac{\mu^2}{2}}\\
\end{array}
$$

Completando cuadrados tenemos:
$$
\begin{array}{rl}
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ \mu^2(-\frac{n}{2}-\frac{\lambda}{2})+\mu(n\bar{x} +\xi)}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ -\frac{1}{2}(n+\lambda)\mu^2+\mu(n\bar{x} +\xi)}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ -\frac{1}{2}(n+\lambda)[\mu^2-\mu(n\bar{x} +\xi)\frac{2}{(n+\lambda)}]}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ -\frac{1}{2}(n+\lambda)[\mu^2-2\mu\frac{n\bar{x}+ \xi}{n+\lambda}]}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ -\frac{n+\lambda}{2}[\mu^2-2\mu\frac{n\bar{x} +\xi}{n+\lambda}+(\frac{n\bar{x}+ \xi}{n+\lambda})^2-(\frac{n\bar{x} +\xi}{n+\lambda})^2]}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ -\frac{n+\lambda}{2}[\mu-\frac{n\bar{x}+ \xi}{n+\lambda}]^2+\frac{n+\lambda}{2}(\frac{n\bar{x} +\xi}{n+\lambda})^2}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ -\frac{n+\lambda}{2}[\mu-\frac{n\bar{x} +\xi}{n+\lambda}]^2+\frac{1}{2}\frac{(n\bar{x} +\xi)^2}{n+\lambda}}\\
\pi(\mu|\xi,\lambda,\mathcal{D}) & \propto e^{ -\frac{n+\lambda}{2}[\mu-\frac{n\bar{x} +\xi}{n+\lambda}]^2}\\
\end{array}
$$

La cual es una normal con media $\frac{n\bar{x} +\xi}{n+\lambda}$ y varianza $\frac{1}{n+\lambda}$, notemos que la media solo depende del estadístico de prueba $\bar x$.

-----

### Ejercicio 2.11

**De el rango de valores de la media posteriori, donde el par $(\lambda,\lambda^{-1}\xi)$ varia sobre $\mathbb{R}^+\times\mathbb{R}$**

La media de la distribución a priori de $\mu$ es:
$$\frac{x+\xi}{1+\lambda}=\frac{1}{1+\lambda}x+\frac{1}{1+\lambda}\xi$$

Lo que es igual
$$\frac{\lambda^{-1}}{1+\lambda^{-1}}x+\frac{\xi\lambda^{-1}}{1+\lambda^{-1}}$$

Estamos suponiendo que $\lambda>0$

- Si $\lambda\to \infty$ entonces $\frac{\lambda^{-1}}{1+\lambda^{-1}}\to 0$.
- Si $\lambda\to 0$ entonces $\frac{\lambda^{-1}}{1+\lambda^{-1}}\to 1$, ya que el valor de arriba será igual que el denominador.

Por lo tanto 
$$\frac{\lambda^{-1}}{1+\lambda^{-1}}\le 1$$

**Si tengo una muestra simple idd $\mathcal{D}=(x_1,...,x_n)$ de una distribución normal $\mathcal{N}(\mu,\sigma^2)$ y $\theta=(\mu, \sigma^2)$, la priori de $\theta$ es el producto de una inversa gamma en $\sigma^2$, $\mathcal{IG}(\lambda_\sigma-1,\frac{\alpha}{2})$ y $\mu$ condicionada a $\sigma^2$ como $\mathcal{N}(\xi,\frac{\sigma^2}{\lambda_\mu})$**
$$
\begin{array}{rl}
\pi(\theta|\mathcal{D}) & \propto L(\theta|\mathcal{D})\pi(\theta)\\
\pi(\theta|\mathcal{D}) & = L(\theta|\mathcal{D})\pi(\mu, \sigma^2)\\
\pi(\theta|\mathcal{D}) & = L(\theta|\mathcal{D})\pi(\mu| \sigma^2)\pi(\sigma^2)\\
\end{array}
$$

Donde
$$
\begin{array}{rl}
L(\theta|\mathcal{D})&=\prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^2}(x_i-\mu)^2}\\
L(\theta|\mathcal{D})&= \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2}\\
L(\theta|\mathcal{D})&= \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i^2-2x_i\mu+\mu^2)}\\
L(\theta|\mathcal{D})&= \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}(\sum_{i=1}^n x_i^2-2n\mu\frac{\sum_{i=1}^n
x_i}{n}+n\mu^2)}\\
L(\theta|\mathcal{D})&= \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}(\sum_{i=1}^n x_i^2-2n\mu\bar x+n\mu^2)}\\
L(\theta|\mathcal{D})&= \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}(\sum_{i=1}^n x_i^2+n(\mu^2-2\mu\bar x+\bar x^2-\bar x^2))}\\
L(\theta|\mathcal{D})&= \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}(\sum_{i=1}^n x_i^2+n(\mu-\bar x)^2-n\bar x^2)}\\
L(\theta|\mathcal{D})&= \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}(n(\mu-\bar x)^2 + \sum_{i=1}^n x_i^2-n\bar x^2)}\\
L(\theta|\mathcal{D})&= \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}(n(\mu-\bar x)^2 + S_x)}
\end{array}
$$

Las prioris son:
$$
\begin{array}{rl}
\pi(\mu| \sigma^2) & = \frac{1}{\sqrt{2\pi\frac{\sigma^2}{\lambda_\mu}}}e^{-\frac{1}{2\frac{\sigma^2}{\lambda_\mu}}(\mu-\xi)^2}\\
& =\frac{\sqrt{\lambda_\mu}}{\sqrt{2\pi \sigma^2}}e^{-\frac{\lambda_\mu}{2\sigma^2}(\mu-\xi)^2}
\end{array}
$$

$$\pi(\sigma^2)=\frac{\left(\frac{\alpha}{2}\right)^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)}\frac{e^{-\frac{\alpha}{2\sigma^2}}}{{\sigma^2}^{\lambda_\sigma+1}}$$

Así tenemos:
$$
\begin{array}{rl}
\pi(\theta|\mathcal{D}) & =  \left( \frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{1}{2\sigma^2}[n(\mu-\bar x)^2 + S_x]}\frac{\sqrt{\lambda_\mu}}{\sqrt{2\pi \sigma^2}}e^{-\frac{\lambda_\mu}{2\sigma^2}(\mu-\xi)^2}\frac{\left(\frac{\alpha}{2}\right)^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)}\frac{e^{-\frac{\alpha}{2\sigma^2}}}{{\sigma^2}^{\lambda_\sigma+1}}\\
\pi(\theta|\mathcal{D}) & \propto \left( \frac{1}{\sigma}\right)^ne^{-\frac{1}{2\sigma^2}[n(\mu-\bar x)^2 + S_x]}\frac{1}{\sigma}e^{-\frac{\lambda_\mu}{2\sigma^2}(\mu-\xi)^2}\frac{e^{-\frac{\alpha}{2\sigma^2}}}{{\sigma^2}^{\lambda_\sigma+1}}\\
\pi(\theta|\mathcal{D}) & = \frac{1}{\sigma^n} e^{-\frac{1}{2\sigma^2}[n(\mu-\bar x)^2 + S_x]}\frac{1}{\sigma}e^{-\frac{\lambda_\mu}{2\sigma^2}(\mu-\xi)^2}e^{-\frac{\alpha}{2\sigma^2}}\frac{1}{{\sigma}^{2\lambda_\sigma+2}}\\
\pi(\theta|\mathcal{D}) & = \frac{1}{\sigma^{n+1+2\lambda_\sigma+2}} e^{-\frac{1}{2\sigma^2}[n(\mu-\bar x)^2 + S_x]}e^{-\frac{\lambda_\mu}{2\sigma^2}(\mu-\xi)^2}e^{-\frac{\alpha}{2\sigma^2}}\\
\pi(\theta|\mathcal{D}) & = \frac{1}{\sigma^{n+2\lambda_\sigma+3}} e^{-\frac{1}{2\sigma^2}[n(\mu-\bar x)^2 + S_x]-\frac{\lambda_\mu}{2\sigma^2}(\mu-\xi)^2-\frac{\alpha}{2\sigma^2}}\\
\pi(\theta|\mathcal{D}) & = \frac{1}{\sigma^{n+2\lambda_\sigma+3}} e^{-\frac{1}{2\sigma^2}[n(\mu-\bar x)^2 + S_x+\lambda_\mu(\mu-\xi)^2+\alpha]}\\
\pi(\theta|\mathcal{D}) & = \frac{1}{(\sigma^2)^{\frac{n}{2}+\lambda_\sigma+\frac{3}{2}}} e^{-\frac{1}{2\sigma^2}[n(\mu-\bar x)^2 + S_x+\lambda_\mu(\mu-\xi)^2+\alpha]}\\
\end{array}
$$

Si hacemos $\lambda_\sigma(\mathcal{D})=\frac{n}{2}+\lambda_\sigma+\frac{3}{2}$, $\lambda_\mu(\mathcal{D})=\lambda_\mu$ y $\alpha(\mathcal{D})=n(\mu-\bar x)^2 + S_x+\alpha$

$$\pi(\theta|\mathcal{D}) = \frac{1}{(\sigma^2)^{\lambda_\sigma(\mathcal{D})}} e^{-\frac{1}{2\sigma^2}[\lambda_\mu(\mathcal{D})(\mu-\xi)^2+\alpha(\mathcal{D})]}$$

-----

### Ejercicio 2.12

**Una distribución Weibull $\mathcal{W}(\alpha, \beta,\gamma)$ es definida como una potencia de una gamma $\mathcal{G}(\alpha, \beta)$: Si $X\sim\mathcal{W}(\alpha, \beta,\gamma)$ entonces $X^\gamma\sim\mathcal{G}(\alpha, \beta)$. Muestre que cuando $\gamma$ es conocida $\mathcal{W}(\alpha, \beta,\gamma)$ es una familia exponencial pero no lo es cuando $\gamma$ es desconocida.**

La distribución Weibull es:

$$f_\theta(x)=\frac{\gamma \alpha^\beta}{\Gamma(\beta)}x^{(\beta+1)\gamma -1}e^{-x^\gamma\alpha}$$
Supongamos que $\gamma$ es conocido y probemos que la Weibull es de la familia exponencial:
$$
\begin{array}{rl}
f_\theta(x) & = \frac{\gamma \alpha^\beta}{\Gamma(\beta)}x^{(\beta+1)\gamma -1}e^{-x^\gamma\alpha}\\
f_\theta(x) & = e^{\ln(\frac{\gamma \alpha^\beta}{\Gamma(\beta)}x^{(\beta+1)\gamma -1})}e^{-x^\gamma\alpha}\\
f_\theta(x) & = e^{\ln(\gamma \alpha^\beta) -\ln(\Gamma(\beta))+((\beta+1)\gamma -1)\ln(x)}e^{-x^\gamma\alpha}\\
f_\theta(x) & = e^{\ln(\gamma \alpha^\beta) -\ln(\Gamma(\beta))+((\beta+1)\gamma -1)\ln(x)-x^\gamma\alpha}\\
f_\theta(x) & = e^{\ln(\gamma \alpha^\beta) -\ln(\Gamma(\beta))+(\beta+1)\gamma\ln(x)-\ln(x)-x^\gamma\alpha}\\
f_\theta(x) & = e^{\ln(\gamma \alpha^\beta) -\ln(\Gamma(\beta))+\beta\gamma\ln(x)+\gamma\ln(x)+\gamma\ln(x)-\ln(x)-x^\gamma\alpha}\\
f_\theta(x) & = e^{\ln(\gamma \alpha^\beta) -\ln(\Gamma(\beta))+\beta\gamma\ln(x)+\gamma\ln(x)+\gamma\ln(x)-\ln(x)-x^\gamma\alpha}\\
\end{array}
$$

Si hacemos $\psi(\theta)= -\ln(\gamma \alpha^\beta) +\ln(\Gamma(\beta))$
$$
\begin{array}{rl}
f_\theta(x) & = e^{-\psi(\theta)+\beta\gamma\ln(x)-x^\gamma\alpha+\gamma\ln(x)-\ln(x)} \\
f_\theta(x) & = e^{-\psi(\theta)+\beta\gamma\ln(x)-x^\gamma\alpha+(\gamma-1)\ln(x)} \\
\end{array}
$$

Notemos que:
$$\beta\gamma\ln(x)-x^\gamma\alpha=[\beta, \alpha]\begin{bmatrix} \gamma\ln(x) \\ -x^\gamma \end{bmatrix}$$

Entonces $\theta=[\beta, \alpha]$, $R(x)=[\gamma\ln(x), -x^\gamma]^t$
$$
\begin{array}{rl}
f_\theta(x) & = e^{\theta R(x)-\psi(\theta)+(\gamma-1)\ln(x)} \\
f_\theta(x) & = e^{\theta R(x)-\psi(\theta)}e^{(\gamma-1)\ln(x)} \\
f_\theta(x) & = e^{\theta R(x)-\psi(\theta)}e^{\ln(x^{(\gamma-1)})} \\
f_\theta(x) & = x^{(\gamma-1)} e^{\theta R(x)-\psi(\theta)}\\
\end{array}
$$

Finalmente $h(x)=x^{(\gamma-1)}$, por lo tanto
$$f_\theta(x) = h(x) e^{\theta R(x)-\psi(\theta)}$$
Así la función Weibull es de la familia exponencial cuando $\gamma$ es conocido. Cuando $\gamma$ es desconocido es imposible conseguir la forma de la familia exponencial.

Además notemos que si $y=x^\gamma$ entonces $y'=\gamma x^{\gamma-1}$ y $x=y^{\frac{1}{\gamma}}$
$$
\begin{array}{rl}
f_\theta(x) & = \frac{\gamma}{\gamma}x^{(\gamma-1)} e^{[\beta, \gamma]\begin{bmatrix}\ln(x) \\ -x^\gamma \end{bmatrix} +\ln(\gamma \alpha^\beta) -\ln(\Gamma(\beta))+(\gamma-1)\ln(x)}\\
f_\theta(x) & = \frac{y'}{\gamma} e^{[\beta, \alpha]\begin{bmatrix} \ln(y^{\frac{1}{\gamma}}) \\ -y \end{bmatrix} +\ln(\gamma \alpha^\beta) -\ln(\Gamma(\beta))+(\gamma-1)\ln(y^{\frac{1}{\gamma}})}\\
f_\theta(x) & = \frac{y'}{\gamma} e^{[\beta, \alpha]\begin{bmatrix} \frac{1}{\gamma}\ln(y) \\ -y \end{bmatrix} +\ln(\gamma \alpha^\beta) -\ln(\Gamma(\beta))+\frac{\gamma-1}{\gamma}\ln(y)}\\
f_\theta(x) & = \frac{y'}{\gamma} e^{\frac{\beta}{\gamma}\ln(y) -y\alpha +\ln(\frac{\gamma \alpha^\beta}{\Gamma(\beta)}) +\frac{\gamma-1}{\gamma}\ln(y)}\\
f_\theta(x) & = \frac{y'}{\gamma} e^{\frac{\beta}{\gamma}\ln(y)}e^{-y\alpha}e^{\ln(\frac{\gamma \alpha^\beta}{\Gamma(\beta)})} e^{\frac{\gamma-1}{\gamma}\ln(y)}\\
f_\theta(x) & = \frac{y'}{\gamma} e^{\ln(y^{\frac{\beta}{\gamma}})}e^{-y\alpha}e^{\ln(\frac{\gamma \alpha^\beta}{\Gamma(\beta)})} e^{\ln(y^\frac{\gamma-1}{\gamma})}\\
f_\theta(x) & = \frac{y'}{\gamma} y^{\frac{\beta}{\gamma}}e^{-y\alpha} \frac{\gamma \alpha^\beta}{\Gamma(\beta)} y^\frac{\gamma-1}{\gamma}\\
f_\theta(x) & = \frac{y'}{\gamma} y^{\frac{\beta}{\gamma}+\frac{\gamma-1}{\gamma}}e^{-y\alpha} \frac{\gamma \alpha^\beta}{\Gamma(\beta)} \\
f_\theta(x) & = \frac{y'}{\gamma} y^{\frac{\beta-1+2\gamma}{\gamma}-1}e^{-y\alpha} \frac{\gamma \alpha^\beta}{\Gamma(\beta)} \\
\end{array}
$$

-----

### Ejercicio 2.13

**Muestre que cuando la priori de $\theta=(\mu, \sigma^2)$ es $\mathcal{N}(\xi,\frac{\sigma^2}{\lambda_\mu})\times \mathcal{IG}(\lambda_s,\alpha)$, la marginal de $\mu$ es una distribución *t-Student* $\mathcal{T}(1\lambda_s,\xi,2\alpha)$. Dar la correspondiente marginal a priori de $\sigma^2$. Para una muestra simple iid $\mathcal{D}=(x_1,...x_n)$ de $\mathcal{N}(\mu,\sigma^2)$**

La priori de $\theta$ es:
$$
\begin{array}{rl}
\pi(\theta) & =\frac{\sqrt{\lambda_\mu}}{\sqrt{2\pi \sigma^2}}e^{-\frac{\lambda_\mu}{2\sigma^2}(\mu-\xi)^2}\frac{\alpha^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)}\frac{e^{-\frac{\alpha}{\sigma^2}}}{{\sigma^2}^{\lambda_\sigma+1}}\\
\pi(\theta) & =\frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\sqrt{2\pi }\Gamma(\lambda_\sigma)}\frac{1}{{\sigma^2}^{(\frac{1}{2}+\lambda_\sigma+1)}}e^{-\frac{\lambda_\mu}{2\sigma^2}(\mu-\xi)^2-\frac{\alpha}{\sigma^2}}\\
\pi(\theta) & =\frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\sqrt{2\pi }\Gamma(\lambda_\sigma)}\frac{1}{{\sigma^2}^{(\frac{3}{2}+\lambda_\sigma)}}e^{-\frac{1}{2\sigma^2}[\lambda_\mu(\mu-\xi)^2-2\alpha]}\\
\end{array}
$$

La priori de $\mu$ es:
$$
\begin{array}{rl}
\pi(\mu) & =\displaystyle \int \pi(\theta)d\sigma^2\\
\pi(\mu) & =\displaystyle \int \frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\sqrt{2\pi }\Gamma(\lambda_\sigma)}\frac{1}{{\sigma^2}^{(\frac{3}{2}+\lambda_\sigma)}}e^{-\frac{1}{2\sigma^2}[\lambda_\mu(\mu-\xi)^2-2\alpha]}d\sigma^2\\
\pi(\mu) & =\displaystyle \frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\sqrt{2\pi }\Gamma(\lambda_\sigma)} \int \frac{1}{{\sigma^2}^{(\frac{3}{2}+\lambda_\sigma)}}e^{-\frac{1}{2\sigma^2}[\lambda_\mu(\mu-\xi)^2-2\alpha]}d\sigma^2
\end{array}
$$

Si llamamos $A=\frac{3}{2}+\lambda_\sigma$, $C=\frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\sqrt{2\pi }\Gamma(\lambda_\sigma)}$ y $B=\frac{1}{2}[\lambda_\mu(\mu-\xi)^2-2\alpha]$ tenemos
$$
\begin{array}{rl}
\pi(\mu) & =\displaystyle C \int \frac{1}{{\sigma^2}^A}e^{-\frac{1}{\sigma^2}B}d\sigma^2\\
\pi(\mu) & =\displaystyle C \int \left(\frac{1}{{\sigma^2}}\right)^Ae^{-(\frac{1}{\sigma^2})B}d\sigma^2\\
\pi(\mu) & =\displaystyle C \int \left(\frac{1}{{\sigma^2}}\right)^{A+2-2}e^{-(\frac{1}{\sigma^2})B}d\sigma^2\\
\pi(\mu) & =\displaystyle C \int \left(\frac{1}{{\sigma^2}}\right)^{A-2}e^{-(\frac{1}{\sigma^2})B}\left(\frac{1}{{\sigma^2}}\right)^2 d\sigma^2\\
\end{array}
$$

Haciendo el cambio de variable $u=\frac{1}{\sigma^2}$ $du=-\left( \frac{1}{\sigma^2} \right)^2d\sigma^2$
$$
\begin{array}{rl}
\pi(\mu) & =\displaystyle -C \int u^{A-2}e^{-uB} du\\
\pi(\mu) & =\displaystyle -C \int u^{(A-1)-1}e^{-uB} \frac{B^{A-1}}{\Gamma(A-1)}\frac{\Gamma(A-1)}{B^{A-1}}du\\
\pi(\mu) & =\displaystyle -C\frac{\Gamma(A-1)}{B^{A-1}}
\int u^{(A-1)-1}e^{-uB} \frac{B^{A-1}}{\Gamma(A-1)}du\\
\pi(\mu) & =\displaystyle -C\frac{\Gamma(A-1)}{B^{A-1}}
\end{array}
$$

Ya que $u^{(A-1)-1}e^{-uB} \frac{A^{B-1}}{\Gamma(B-1)}$ es la función de densidad de una distribución Gamma con parámetros $A-1$ y $B$
$$
\begin{array}{rl}
\pi(\mu) & \propto B^{-(A-1)} \\
\pi(\mu) & \propto B^{1-A} \\
\pi(\mu) & = (\frac{1}{2}[\lambda_\mu(\mu-\xi)^2-2\alpha])^{1-[\frac{3}{2}+\lambda_\sigma]}\\
\pi(\mu) & \propto [\lambda_\mu(\mu-\xi)^2-2\alpha]^{-\frac{1}{2}-\lambda_\sigma}\\
\pi(\mu) & \propto [-2\alpha(1-\frac{\lambda_\mu(\mu-\xi)^2}{2\alpha})]^{-\frac{1}{2}-\lambda_\sigma}\\
\pi(\mu) & \propto (1-\frac{\lambda_\mu\lambda_\sigma(\mu-\xi)^2}{2\alpha\lambda_\sigma})^{-\frac{1+2\lambda_\sigma}{2}}
\end{array}
$$

La priori marginal correspondiente para $\sigma^2$ será:
$$
\begin{array}{rl}
\pi(\mu) & =\displaystyle \int \pi(\theta)d\mu\\
\pi(\mu) & =\displaystyle \int \frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\sqrt{2\pi }\Gamma(\lambda_\sigma)}\frac{1}{{\sigma^2}^{(\frac{3}{2}+\lambda_\sigma)}}e^{-\frac{1}{2\sigma^2}[\lambda_\mu(\mu-\xi)^2-2\alpha]}d\mu\\
\pi(\mu) & =\displaystyle \frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\sqrt{2\pi }\Gamma(\lambda_\sigma)} \frac{1}{{\sigma^2}^{(\frac{3}{2}+\lambda_\sigma)}}e^{\frac{\alpha}{\sigma^2}}\int e^{-\frac{1}{2\sigma^2}\lambda_\mu(\mu-\xi)^2}d\mu\\
\pi(\mu) & =\displaystyle \frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)} \frac{1}{{\sigma^2}^{(\frac{3}{2}+\lambda_\sigma)}}e^{\frac{\alpha}{\sigma^2}}\int \frac{1}{\sqrt{2\pi }}e^{-\frac{1}{2\sigma^2}\lambda_\mu(\mu-\xi)^2}d\mu
\end{array}
$$

Ahora llamemos $D=\frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)} \frac{1}{{\sigma^2}^{(\frac{3}{2}+\lambda_\sigma)}}e^{\frac{\alpha}{\sigma^2}}$
$$
\begin{array}{rl}
\pi(\mu) & =\displaystyle D\int \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2\sigma^2}\lambda_\mu(\mu-\xi)^2}d\mu\\
\pi(\mu) & =\displaystyle D\int \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2\frac{\sigma^2}{\lambda_\mu}} (\mu-\xi)^2}d\mu\\
\pi(\mu) & =\displaystyle D\int \frac{\sqrt{\frac{\sigma^2}{\lambda_\mu}}}{\sqrt{2\pi\frac{\sigma^2}{\lambda_\mu}}}e^{-\frac{1}{2\frac{\sigma^2}{\lambda_\mu}} (\mu-\xi)^2}d\mu\\
\pi(\mu) & = D\sqrt{\frac{\sigma^2}{\lambda_\mu}} \displaystyle \int \frac{1}{\sqrt{2\pi\frac{\sigma^2}{\lambda_\mu}}}e^{-\frac{1}{2\frac{\sigma^2}{\lambda_\mu}} (\mu-\xi)^2}d\mu\\
\pi(\mu) & = D\sqrt{\frac{\sigma^2}{\lambda_\mu}}
\end{array}
$$

Ya que $\frac{1}{\sqrt{2\pi\frac{\sigma^2}{\lambda_\mu}}}e^{-\frac{1}{2\frac{\sigma^2}{\lambda_\mu}} (\mu-\xi)^2}$ es una función de densidad de una normal con media $\xi$ y varianza $\frac{\sigma^2}{\lambda_\mu}$

Por lo tanto:
$$
\begin{array}{rl}
\pi(\mu) & = \frac{\sqrt{\lambda_\mu}\alpha^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)} \frac{1}{{\sigma^2}^{(\frac{3}{2}+\lambda_\sigma)}}e^{\frac{\alpha}{\sigma^2}} \sqrt{\frac{\sigma^2}{\lambda_\mu}}\\
\pi(\mu) & = \frac{\alpha^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)}e^{\frac{\alpha}{\sigma^2}} (\sigma^2)^{\frac{1}{2}-\frac{3}{2}-\lambda_\sigma}\\
\pi(\mu) & = \frac{\alpha^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)}e^{\frac{\alpha}{\sigma^2}} (\sigma^2)^{-1-\lambda_\sigma}\\
\pi(\mu) & = \frac{\alpha^{\lambda_\sigma}}{\Gamma(\lambda_\sigma)}e^{\frac{\alpha}{\sigma^2}}\frac{1}{(\sigma^2)^{\lambda_\sigma+1}}
\end{array}
$$

La cual es la función de densidad de una Inversa Gamma con parámetros $\lambda_\sigma$ y $\alpha$.

La distribución a posteriori para la muestra normal será entonces:

## 2.2.3 Intervalos de Confianza

Un punto que debe quedar claro desde el principio es que el enfoque Bayesiano es un enfoque/acercamiento inferencial completo. Entonces, cubre la evaluación de confianza, pruebas, predicción, chequeo del modelo y estimación puntual. Vamos a cubrir progresivamente las diferentes facetas del Análisis Bayesiano en otros capítulos de este libro, pero vamos a refefirnos aquí sobre los intervalos de confianza.

Como en todo lo demás, la obtención/derivación de los intervalos de confianza (o regiones de confianza en una configuración más general) está basada en la distribución posterior $\pi(\theta,\mathcal{D})$. Dado que el enfoque Bayesiano trata/procesa al parámetro $\theta$ como una variable aleatoria (se le asigna una distribución), una definición "natural" de la región de confianza sobre $\theta$ es determinar $C(\mathcal{D})$ tal que 
$$
\tag{2.9}
\pi(\theta \in C(\mathcal{D})\ |\ \mathcal{D}) = 1 - \alpha
$$
donde $\alpha$ es un nivel predeterminado así como $0.05$.

**La grán diferencia, desde una perspectiva tradicional, es que la integración se hace sobre el espacio de parámetros, en lugar del espacio muestral. La cantidad $1-\alpha$ entonces corresponde a la probabilidad de que un $\theta$ aleatorio pertenece al conjunto $C(\mathcal{D})$, en lugar de la probabilidad de que un conjunto (intervalo) aleatorio contiene el "verdadero" valor del parámetro $\theta$.**

Dado este cambio en la interpretación de un conjunto de confianza (también llamado conjunto con credibilidad/creible por los Bayesianos), la determinación del mejor intervalo resulta más fácil que en el sentido clásico: simplemente corresponde al valor de $\theta$ con el mayor valor para la distribución posterior.
$$
C(\mathcal{D})=\{\theta; \pi(\theta\ |\ \mathcal{D})\ge k_{\alpha}\},
$$
donde $k_\alpha$ se determina por la restricción de cobertura (2.9). Esta región es llamada *highest posterior density (HPD) region* (región de mayor densidad posterior).

-----

Para **normaldata**, utilizando el parámetro $\theta=(\mu,\sigma^2)$, la distribución posterior marginal sobre $\mu$ utilizando el *a priori* $\pi(\theta)=1/\sigma^2$ es una distribución *t-Student*
$$
\pi(\mu|\mathcal{D})\propto [n(\mu-\bar{x})^2+s_x^2]^{-n/2}
$$
Veamos como llegan a esto:

La distribución a posteriori viene dada por $\pi(\theta|\mathcal{D})\propto l(\theta|\mathcal{D})\pi(\theta)$

Como los datos se distribuyen normal con media $\mu$ y varianza $\sigma^2$ tenemos que la verosimilitud de los datos viene dada por:
$$
\begin{array}{rl}
l(\theta|\mathcal{D}) & = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{1}{2\sigma^2}(x_i-\mu)^2}\\
& =  \left( \frac{1}{\sqrt{2\pi\sigma^2}}\right)^n e^{\frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2}\\
\end{array}
$$

Donde
$$
\begin{array}{rl}
\sum_{i=1}^n (x_i-\mu)^2 & = \sum_{i=1}^n x_i^2 -2x_i\mu +\mu^2\\
& = \sum_{i=1}^n x_i^2 -2\mu\sum_{i=1}^n x_i +\sum_{i=1}^n \mu^2\\
& = \sum_{i=1}^n x_i^2 -2\mu\frac{n\sum_{i=1}^nx_i}{n}  +n \mu^2\\
& = \sum_{i=1}^n x_i^2 -2n\mu\bar x  +n \mu^2\\
& = \sum_{i=1}^n x_i^2 + n(\mu^2-2\mu\bar x )\\
& = \sum_{i=1}^n x_i^2 + n(\mu^2-2\mu\bar x +\bar x^2-\bar x^2)\\
& = \sum_{i=1}^n x_i^2 + n((\mu-\bar x)^2 -\bar x^2)\\
& = \sum_{i=1}^n x_i^2 - n\bar x^2 + n(\mu-\bar x)^2\\
& = S_x + n(\mu-\bar x)^2
\end{array}
$$

Con $S_x=\sum_{i=1}^n x_i^2 - n\bar x^2$. Sustituyendo este nos queda:
$$l(\theta|\mathcal{D})= \left( \frac{1}{\sqrt{2\pi\sigma^2}}\right)^n e^{\frac{1}{2\sigma^2}\left[ S_x + n(\mu-\bar x)^2\right]}$$

Por lo tanto
$$
\begin{array}{rl}
\pi(\theta|\mathcal{D}) & \propto \left( \frac{1}{\sqrt{2\pi\sigma^2}}\right)^n e^{-\frac{1}{2\sigma^2}\left[ S_x + n(\mu-\bar x)^2\right]}\frac{1}{\sigma^2}\\
\pi(\theta|\mathcal{D}) & \propto \left( \frac{1}{\sqrt{\sigma^2}}\right)^n \frac{1}{\sigma^2} e^{-\frac{1}{2\sigma^2}\left[ S_x + n(\mu-\bar x)^2\right]}\\
\pi(\theta|\mathcal{D}) & \propto \left( \frac{1}{\sigma^2}\right)^{\frac{n}{2}+1}  e^{-\frac{1}{2\sigma^2}\left[ S_x + n(\mu-\bar x)^2\right]}
\end{array}
$$

Para conseguir la distribución a posteriori del parámetro $\mu$ tenemos que integrar esta última función con respecto a $\sigma^2$
$$
\begin{array}{rl}
\pi(\mu|\mathcal{D}) & = \displaystyle \int \pi(\theta|\mathcal{D}) d\sigma^2\\
\pi(\mu|\mathcal{D}) & = \displaystyle \int \left( \frac{1}{\sigma^2}\right)^{\frac{n}{2}+1}  e^{-\frac{1}{2\sigma^2}\left[ S_x + n(\mu-\bar x)^2\right]}d\sigma^2\\
\pi(\mu|\mathcal{D}) & = \displaystyle \int \left( \frac{1}{\sigma^2}\right)^{\frac{n}{2}+1}e^{-\frac{1}{2\sigma^2}A}d\sigma^2\\
\pi(\mu|\mathcal{D}) & = \displaystyle \int \left( \frac{1}{\sigma^2}\right)^{\frac{n}{2}-1}\left( \frac{1}{\sigma^2}\right)^{2}e^{-\frac{1}{2\sigma^2}A}d\sigma^2
\end{array}
$$
Haciendo el cambio de variable $u=\frac{1}{\sigma^2}$ entonces $du=-\left(\frac{1}{\sigma^2}\right)^2 d\sigma^2$
$$
\begin{array}{rl}
\pi(\mu|\mathcal{D}) & = \displaystyle \int (u)^{\frac{n}{2}-1}e^{-\frac{1}{2}uA}du\\
\pi(\mu|\mathcal{D}) & = \displaystyle \int (u)^{\frac{n}{2}-1}e^{-\frac{1}{2}Au}\frac{(\frac{A}{2})^{\frac{n}{2}}}{\Gamma{(\frac{n}{2}})}\frac{\Gamma{(\frac{n}{2}})}{(\frac{A}{2})^{\frac{n}{2}}}du\\
\pi(\mu|\mathcal{D}) & = \displaystyle \frac{\Gamma{(\frac{n}{2}})}{(\frac{A}{2})^{\frac{n}{2}}} \int \frac{(\frac{A}{2})^{\frac{n}{2}}}{\Gamma{(\frac{n}{2}})} (u)^{\frac{n}{2}-1}e^{-\frac{1}{2}Au}du\\
\end{array}
$$

Donde $\frac{(\frac{A}{2})^{\frac{n}{2}}}{\Gamma{(\frac{n}{2}})}(u)^{\frac{n}{2}-1}e^{-\frac{1}{2}Au}$ es la función de densidad de una distribución Gamma con parámetros $\frac{n}{2}$ y $\frac{A}{2}$, así esta intendral da 1, por lo tanto:
$$
\begin{array}{rl}
\pi(\mu|\mathcal{D}) & = \displaystyle \frac{\Gamma{(\frac{n}{2}})}{(\frac{A}{2})^{\frac{n}{2}}}\\
\pi(\mu|\mathcal{D}) & \propto \left( \frac{A}{2}\right)^{-\frac{n}{2}}\\
\pi(\mu|\mathcal{D}) & \propto  \frac{\left(S_x + n(\mu-\bar x)^2\right)^{-\frac{n}{2}}}{\left(2\right)^{-\frac{n}{2}}}\\
\pi(\mu|\mathcal{D}) & \propto  \left(S_x + n(\mu-\bar x)^2\right)^{-\frac{n}{2}}\\
\end{array}
$$

con $n-1=89$ grados de libertad, como se puede ver en (2.7), la región de confianza correspondiente al $95\%$ para $\mu$ es el intervalo $[-0.070,-0.013]$. Note que como el $0$ no pertenece al intervalo, uno puede sentir justificado el reporte de un decrecimiento en el número robos entre 1991 y 1995.

Intervalo sobre $\mu$
$$
[\bar{x}\ -\ t_{\alpha/2,n-1}s_x/\sqrt{n(n-1)},\ \bar{x}\ +\ t_{\alpha/2,n-1}s_x/\sqrt{n(n-1)}]
$$

- La región de confianza correspondiente al $95\%$ para $\mu$ es el intervalo $[-0.070,-0.013]$. Note que como el $0$ no pertenece al intervalo, uno puede sentir justificado el reporte de un decrecimiento en el número robos entre 1991 y 1995.

```{r, echo=TRUE}
alpha<-.05
n<-length(larcenies)
xbar<- mean(larcenies)
sx<- sd(larcenies)
prob<-qt(1-alpha/2,df=n-1)
c(xbar-prob*sx/sqrt(n),xbar+prob*sx/sqrt(n))
```

-----

Mientras que la forma de un intervalo de confianza Bayesiano óptimo es fácilmente derivable, el cálculo de la cota $k_\alpha$ o el conjunto $C(\mathcal{D})$ puede ser muy retador "to allow an analytic construction outside conjugate setups"

-----

### Ejemplo 2.1

Cuando la distribución *a priori* no es conjugada, la distribución posterior no se comporta bien necesariamente. Por ejemplo, si la distribución normal $\mathcal{N}(\mu,1)$ es reemplazada por la distribución de Cauchy, $\mathcal{C}(\mu,1)$, en la verosimilitud
$$
l(\mu|\mathcal{D})=\pi(\mathcal{D}\ |\ \theta)=\prod_{i=1}^{n}f_\mu(x_i)=\frac{1}{\pi^n\prod_{i=1}^n(1+(x_i-\mu)^2)},
$$
no hay un *a priori* conjugado disponible y podemos, por ejemplo, considerar un *a priori* normal para $\mu$, digamos $\mathcal{N}(0,10)$. La distribución posterior, entonces, es proporcional a
$$
\tilde\pi(\mu|\mathcal{D})=\frac{\exp(-\mu^2/20)}{\prod_{i=1}^{n}(1+(x_i-\mu)^2)}.
$$
Una solución analítica de $\tilde\pi(\mu|\mathcal{D})$ no es posible, sólo numérica, y el cálculo de una cota apropiada $k_\alpha$ requiere un nivel extra de cómputos numéricos para obtener la cobertura correcta. La Figura 2.5 nos dá la distribución posterior de $\mu$ para los datos (observaciones) $x_1=-4.3$ y $x_2=3.2$, normalizado por una simple integración trapezoidal, que es, calculando $\tilde\pi(\mu|\mathcal{D})$ en una grilla de ancho $\triangle$ y "and summing up". Para un valor $k$ dado, la misma aproximación trapezoidal se puede utilizar para calcular la cobertura aproximada de la región HDP. Para $\alpha=0.95$, una prueba de ensayo y error (tanteo) del rango de $k$ nos proporciona una aproximación de $k_\alpha=0.0415$ y la región HPD, se representa como en la figura. $\lhd$

**Introducir la figura**

**Fig. 2.5.** Distribución posterior de $\mu$ para una distribución *a priori* $\mathcal{N}(0,10)$ y una región HPD de $95\%$.

 > Dado que la distribución posterior no es necesariamente unimodal, la región HPD puede incluir varios conjuntos no relacionados (disconnected), como se ilustra en el Ejemplo 2.1. Esto puede parecer contraintuitivo desde el punto de vista clásico, pero se debe interpretar como un indicador de indeterminación, ya sea en los datos o en la distribución *a priori*, acerca de los posibles valores del parámetro $\theta$. Note también que las regiones HPD no son independientes de la selección de la medida de referencia que define el volumen (o superficie).

# 2.3 Prueba de Hipótesis

Decidir la validez de algunas premisas o restricciones sobre el parámetro $\theta$ es una gran parte del trabajo del estadístico. (Vamos a lidiar con la validez del modelo completo, por ejemplo, si es o no es apropiada una distribución normal para los datos provistos en la sección 2.5.2 y más generalmente en la sección 6.7.3.). Como el resultado del proceso de decisión es claro (*clearcut*), aceptar (representado por 1) o rechazar (representado por 0), la construcción y la evaluación de los procedimientos es crucial. Mientras que la solución Bayesiana es formalmente muy cercana a un estadístico de la razón de la verosimilitud, sus valores numéricos frecuentemente difieren fuertemente de las soluciones clásicas.

## 2.3.1 Decisiones Cero-Uno

Vamos a formalizar la premisa como espacios de parámetros restringidos, denotado como $\theta\in\Theta_0$.
Por ejemplo, $\theta>0$ corresponde a $\Theta_0=\mathbb{R}^+$.

El enfoque estándar de Neyman Pearson para pruebas de hipótesis se basa en la función de pérdida $0-1$ que penaliza todos los errores igualmente: Si consideramos la prueba de la Hipótesis Nula $H_0: \theta\in\Theta_0$ versus $H_1: \theta\notin\Theta_0$ y denotamos por $d\in\{0,1\}$ la decisión que toma el estadístico y por $\delta$ el procedimiento de decisión correspondiente, la pérdida
$$
L(\theta,d)=\Bigg\{
\begin{array}{ll}
1-d &\text{si } \theta\in\Theta_0\\
d &\text{si no}
\end{array}
$$
y el riesgo asociado
$$
\begin{array}{rl}
R(\theta,\delta) = & \mathbb{E}_\theta[L(\theta,\delta(x))]\\
R(\theta,\delta) = & \sum L(\theta,\delta(x))P(\delta(x))\\
R(\theta,\delta) = & L(\theta,\delta(x)=0)P(\delta(x)=0)+L(\theta,\delta(x)=1)P(\delta(x)=1)
\end{array}
$$

Supongamos que $\theta \in \Theta_0$ entonces tenemos por definición de $L(\theta,\delta(x))$ que:
$$
\begin{array}{rl}
L(\theta,\delta(x)=0)=1\ si \ d=0\\
L(\theta,\delta(x)=1)=0\ si \ d=1\\
\end{array}
$$

Sustituyendo esto nos queda:
$$
\begin{array}{rl}
R(\theta,\delta) = & 1P(\delta(x)=0)+0P(\delta(x)=1)\\
R(\theta,\delta) = & P(\delta(x)=0) \ si \ \theta\in \Theta_0 
\end{array}
$$

Ahora supongamos que $\theta \notin \Theta_0$ entonces:
$$
\begin{array}{rl}
L(\theta,\delta(x)=0)=0\ si \ d=0\\
L(\theta,\delta(x)=1)=1\ si \ d=1\\
\end{array}
$$

Así:
$$
\begin{array}{rl}
R(\theta,\delta) = & 0P(\delta(x)=0)+1P(\delta(x)=1)\\
R(\theta,\delta) = & P(\delta(x)=1) \ si \ \theta\notin \Theta_0 
\end{array}
$$

Por lo tanto:
$$
\begin{array}{rl}
R(\theta,\delta) = & \displaystyle\int_\chi L(\theta,\delta(x))f_\theta(x)dx\\
R(\theta,\delta) = & \bigg\{
\displaystyle\begin{array}{lr}
P_\theta(\delta(x)=0), \text{ si }\theta\in\Theta_0\\
P_\theta(\delta(x)=1), \text{ si no}
\end{array}
\end{array}
$$

donde $x$ denota los datos $\mathcal{D}$ disponibles. Bajo la función de pérdida $0-1$, el criterio de decisión de Bayes (estimador) asociado con la distribución *a priori* $\pi$ es
$$
\delta^\pi(x)=\Bigg\{
\begin{array}{lr}
1, \text{ si }P^\pi(\theta\in\Theta_0|x)>P^\pi(\theta\notin\Theta_0|x)\\
0, \text{ si no}
\end{array}
$$
Este estimador se justifica fácilmente sobre una base intuitiva dado que se escoge la hipótesis con la probabilidad posterior mayor. 

Una generalización de la pérdida mencionada anteriormente es la penalización de los errores de una manera diferente cuando la hipótesis nula es verdadera en lugar de cuando es falsa a través de la función de pérdida $0-1$ ponderada ($a_0,a_1>0$),
$$
L_{a_0,a_1}(\theta,d)= \Bigg \{
\begin{array}{ll}
a_0, &\text{ si }\ \theta\in\Theta_0\ \text{ y }\ d=0,\\
a_1, &\text{ si }\ \theta\in\Theta_1\ \text{ y }\ d=1,\\
0,&\text{ si no }
\end{array}
$$

-----

### Ejercicio 2.18.

Muestre que, bajo la función de pérdida $L_{a_0,a_1}$, el estimador de Bayes asociado con la distribución *a priori* $\pi$ es dada por
$$
\delta^\pi(x)=\Bigg\{
\begin{array}{ll}
1, &\text{ si }\ P^\pi(\theta\in\Theta_0|x)>\displaystyle \frac{a_1}{a_0+a_1},\\
0,&\text{ si no }
\end{array}
$$

-----

Sea 
$$
L_{a_0,a_1}(\theta,d)= \Bigg \{
\begin{array}{ll}
a_0, &\text{ si }\ \theta\in\Theta_0\ \text{ y }\ d=0,\\
a_1, &\text{ si }\ \theta\in\Theta_1\ \text{ y }\ d=1,\\
0,&\text{ si no }
\end{array}
$$
siendo $d$ la decisión que toma el estadístico (analista) y $\delta$ es el proceso de decisión. La pérdida esperada de la densidad posterior es
$$
\mathbb{E}[L_{a_0,a_1}(\theta,d)|x] = \bigg\{
\begin{array}{rl}
a_0\ P^\pi(\theta\in\Theta_0|x)\text{, si } d=0\\
a_1\ P^\pi(\theta\in\Theta_1|x)\text{, si } d=1\\
\end{array}
$$
la decisión que minimiza la pérdida posterior es $d=1$ cuando
$$
\begin{array}{rl}
a_0\ P^\pi(\theta\in\Theta_0|x) > & a_1\ P^\pi(\theta\in\Theta_1|x) \\
a_0\ P^\pi(\theta\in\Theta_0|x) > & a_1\ [1-P^\pi(\theta\in\Theta_0|x)]\\
a_0\ P^\pi(\theta\in\Theta_0|x) > & a_1\ -\ a_1P^\pi(\theta\in\Theta_0|x)\\
a_0\ P^\pi(\theta\in\Theta_0|x) + a_1P^\pi(\theta\in\Theta_0|x) > & a_1\ \\
(a_0+a_1)\ P^\pi(\theta\in\Theta_0|x) > & a_1\ \\
\ P^\pi(\theta\in\Theta_0|x) > & \displaystyle\frac{a_1}{a_0+a_1}\ \\
\end{array}
$$
y $0$ en otro caso.

Por lo tanto nos queda
$$
\delta^\pi(x)=\Bigg\{
\begin{array}{ll}
1, &\text{ si }\ P^\pi(\theta\in\Theta_0|x)>\displaystyle \frac{a_1}{a_0+a_1},\\
0,&\text{ si no }
\end{array}
$$

-----

Para esta clase de funciones de pérdida, la hipótesis nula $H_0$ se rechaza cuando la probabilidad posterior de $H_0$ es muy pequeña, el nivel de aceptación $a_1/(a_0+a_1)$ está determinando por la escogencia de ($a_0,a_1$).

-----

### Ejercicio 2.19.

Cuando $\theta\in\{\theta_0,\theta_1\}$, muestre que el procedimiento Bayesiano sólo depende de la razón/ratio $\frac{(1-\varrho_0) f_{\theta_1}(x)}{\varrho_0 f_{\theta_0}(x)}$, donde $\varrho_0$ es el peso *a priori* sobre $\theta_0$.

-----

En este caso $\pi$ le asigna masa $\varrho_0$ a un punto $\theta_0$ y le asigna por otra parte la masa $(1-\varrho_0)$ a $\theta_1$, entonces
$$
\begin{array}{rl}
P^\pi(\theta=\theta_0)=&\displaystyle \frac{\varrho_0 f_{\theta_0}(x)}{\varrho_0 f_{\theta_0}(x)\ +\ \varrho_1 f_{\theta_1}(x)}\\
=&\displaystyle \frac{\varrho_0 f_{\theta_0}(x)}{\varrho_0 f_{\theta_0}(x)\ +\ (1-\varrho_0) f_{\theta_1}(x)}\\
=&\displaystyle \frac{1}{1\ +\ \displaystyle \frac{(1-\varrho_0) f_{\theta_1}(x)}{\varrho_0 f_{\theta_0}(x)}}
\end{array}
$$

-----

Para $x\sim\mathcal{N}(\mu,\sigma^2)$ y $\mu\sim\mathcal{N}(\xi,\tau^2)$, recordemos que $\pi(\mu|x)$ es la distribución normal con $\mathcal{N}(\xi(x),w^2)$ con
$$
\xi(x)=\frac{\sigma^2\xi+\tau^2x}{\sigma^2+\tau^2}\ \text{ y }\ \omega^2=\frac{\sigma^2\tau^2}{\sigma^2+\tau^2}
$$

Para probar $H_0:\mu<0$ calculamos
$$
\begin{array}{rl}
P^\pi(\mu<0|x) = &  \displaystyle P^\pi\Bigg(\frac{\mu-\xi(x)}{\omega}<\frac{-\xi(x)}{\omega} \Bigg) \sim \mathcal{N}(0,1) \\
= & \displaystyle   \Phi\bigg(\frac{-\xi(x)}{\omega}\bigg)
\end{array}
$$

Si $z_{a_0,a_1}$ es el cuantil $\frac{a_1}{a_0+a_1}$ de una distribución normal $\mathcal{N}(0,1)$ (por ejemplo, $\Phi(z_{a_0,a_1})=\frac{a_1}{a_0+a_1}$), aceptamos la hipótesis nula $H_0$ cuando
$$
\begin{array}{rl}
-\xi(x) > & z_{a_0,a_1} \omega \\
-\displaystyle \frac{\sigma^2\xi+\tau^2 x}{\sigma^2+\tau^2}>& z_{a_0,a_1}\omega \\
-\displaystyle \sigma^2\xi-\tau^2x>& z_{a_0,a_1}\omega(\sigma^2+\tau^2) \\
\displaystyle -\tau^2x>& - z_{a_0,a_1}\omega(\sigma^2+\tau^2) - \sigma^2\xi \\
x<& \displaystyle \frac{-z_{ a_0,a_1}\omega(\sigma^2+\tau^2) - \sigma^2\xi}{\tau^2} \\
x<& \displaystyle -\frac{z_{ a_0,a_1}\omega(\sigma^2+\tau^2)}{\tau^2} - \frac{\sigma^2\xi}{\tau^2}
\end{array}
$$

La cota de aceptación superior es
$$
x<-\frac{\sigma^2}{\tau^2}\xi - \bigg(1+ \frac{\sigma^2}{\tau^2} \bigg)z_{a_0,a_1}\omega
$$

Esto ilustra una vez más la falta de robustez de la distribución *a priori* conjugada: Esta cota puede ser cualquier real cuando $\xi$ varía en $\mathbb{R}$.

-----

### Ejercicio 2.20.

Muestre que el límite de la probabilidad posterior $P^\pi(\mu<0|x)$ cuando $\xi$ tiende a $0$ y $\tau$ tiende a $\infty$ es $\Phi(-x/\sigma)$.

-----

Para $x\sim\mathcal{N}(\mu,\sigma^2)$ y $\mu\sim\mathcal{N}(\xi,\tau^2)$, probemos que $\pi(\mu|x)$ es la distribución normal con $\mathcal{N}(\xi(x),w^2)$ con $P^\pi(\mu<0|x)=\Phi\bigg(\frac{-\xi(x)}{\omega}\bigg)$
$$
\xi(x)=\frac{\sigma^2\xi+\tau^2x}{\sigma^2+\tau^2}\ \text{ y }\ \omega^2=\frac{\sigma^2\tau^2}{\sigma^2+\tau^2}
$$

La distribución a posteriori de $\pi(\mu|x)$ es: 
$$
\begin{array}{rl}
\pi(\mu|x) & = l(\mu|x)\pi(\mu)\\
\pi(\mu|x) & = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}\frac{1}{\sqrt{2\pi\tau^2}}e^{-\frac{1}{2\tau^2}(\mu-\xi)^2}\\
\pi(\mu|x) & \propto e^{-\frac{1}{2\sigma^2}(x-\mu)^2}e^{-\frac{1}{2\tau^2}(\mu-\xi)^2}\\
\pi(\mu|x) & \propto e^{-\frac{1}{2\sigma^2}(x-\mu)^2-\frac{1}{2\tau^2}(\mu-\xi)^2}\\
\pi(\mu|x) & \propto e^{-\frac{1}{2}\left[\frac{(x-\mu)^2}{\sigma^2}+\frac{(\mu-\xi)^2}{\tau^2}\right]}\\
\end{array}
$$

Desarrollando $\frac{(x-\mu)^2}{\sigma^2}+\frac{(\mu-\xi)^2}{\tau^2}$ tenemos:
$$
\begin{array}{rl}
\frac{(x-\mu)^2}{\sigma^2}+\frac{(\mu-\xi)^2}{\tau^2} & = \frac{x^2-2x\mu+\mu^2}{\sigma^2}+\frac{\mu^2-2\mu\xi+\xi^2}{\tau^2}\\
& = \frac{x^2}{\sigma^2}-\frac{2x\mu}{\sigma^2}+\frac{\mu^2}{\sigma^2}+\frac{\mu^2}{\tau^2}-\frac{2\mu\xi}{\tau^2}+\frac{\xi^2}{\tau^2}\\
& = \mu^2\left(\frac{1}{\sigma^2}+\frac{1}{\tau^2}\right) -2\mu\left(\frac{x}{\sigma^2}+\frac{\xi}{\tau^2}\right)+\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2}\\
\end{array}
$$

Sustituyendo estó:
$$
\begin{array}{rl}
\pi(\mu|x) & \propto e^{-\frac{1}{2}\left[ \mu^2\left(\frac{1}{\sigma^2}+\frac{1}{\tau^2}\right) -2\mu\left(\frac{x}{\sigma^2}+\frac{\xi}{\tau^2}\right)+\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2}\right]}\\
\pi(\mu|x) & \propto e^{-\frac{1}{2}\left[ \mu^2\left(\frac{1}{\sigma^2}+\frac{1}{\tau^2}\right) -2\mu\left(\frac{x}{\sigma^2}+\frac{\xi}{\tau^2}\right)\right]} e^{\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2}}\\
\pi(\mu|x) & \propto e^{-\frac{1}{2}\left[ \mu^2\left(\frac{1}{\sigma^2}+\frac{1}{\tau^2}\right) -2\mu\left(\frac{x}{\sigma^2}+\frac{\xi}{\tau^2}\right)\right]}\\
\pi(\mu|x) & \propto e^{-\frac{1}{2}\left[ \mu^2\left(\frac{\tau^2+\sigma^2}{\tau^2 \sigma^2}\right) -2\mu\left(\frac{x\tau^2+\xi\sigma^2}{\tau^2\sigma^2}\right)\right]}\\
\pi(\mu|x) & \propto e^{-\frac{1}{2\tau^2\sigma^2}[ \mu^2(\tau^2+\sigma^2) -2\mu(x\tau^2+\xi\sigma^2)]}\\
\pi(\mu|x) & \propto e^{-\frac{\tau^2+\sigma^2}{2\tau^2\sigma^2}\left[ \mu^2 -2\mu\frac{(x\tau^2+\xi\sigma^2)}{(\tau^2+\sigma^2)}\right]}\\
\pi(\mu|x) & \propto e^{-\frac{1}{2\tau^2\sigma^2}[ \mu^2(\tau^2+\sigma^2) -2\mu(x\tau^2+\xi\sigma^2)]}\\
\pi(\mu|x) & \propto e^{-\frac{\tau^2+\sigma^2}{2\tau^2\sigma^2}\left[ \mu^2 -2\mu\frac{x\tau^2+\xi\sigma^2}{\tau^2+\sigma^2}+\left( \frac{(x\tau^2+\xi\sigma^2)}{(\tau^2+\sigma^2)} \right)^2-\left( \frac{x\tau^2+\xi\sigma^2}{\tau^2+\sigma^2} \right)^2\right]}\\
\pi(\mu|x) & \propto e^{-\frac{\tau^2+\sigma^2}{2\tau^2\sigma^2}\left[ \mu -\frac{x\tau^2+\xi\sigma^2}{\tau^2+\sigma^2}\right]^2}e^{\frac{(\tau^2+\sigma^2)}{2\tau^2\sigma^2}\left( \frac{x\tau^2+\xi\sigma^2}{\tau^2+\sigma^2} \right)^2}\\
\pi(\mu|x) & \propto e^{-\frac{\tau^2+\sigma^2}{2\tau^2\sigma^2}\left[ \mu -\frac{x\tau^2+\xi\sigma^2}{\tau^2+\sigma^2}\right]^2}\\
\pi(\mu|x) & \propto \frac{\sqrt{2\pi\frac{\tau^2\sigma^2}{\tau^2+\sigma^2}}}{\sqrt{2\pi\frac{\tau^2\sigma^2}{\tau^2+\sigma^2}}} e^{-\frac{1}{2\frac{\tau^2\sigma^2}{\tau^2+\sigma^2}}\left[ \mu -\frac{x\tau^2+\xi\sigma^2}{\tau^2+\sigma^2}\right]^2}\\
\pi(\mu|x) & \propto \frac{1}{\sqrt{2\pi\frac{\tau^2\sigma^2}{\tau^2+\sigma^2}}}e^{-\frac{1}{2\frac{\tau^2\sigma^2}{\tau^2+\sigma^2}}\left[ \mu -\frac{x\tau^2+\xi\sigma^2}{\tau^2+\sigma^2}\right]^2}\\
\end{array}
$$

Obteniendo la dristribución normal con media $\xi(x)=\frac{\sigma^2\xi+\tau^2x}{\sigma^2+\tau^2}$ y la varianza $\omega^2=\frac{\sigma^2\tau^2}{\sigma^2+\tau^2}$.

Entonces:
$$
\begin{array}{rl}
\Phi\bigg(\frac{-\xi(x)}{\omega}\bigg)=&\Phi\Bigg(-\displaystyle \frac{\displaystyle \sigma^2\xi+\tau^2x}{\sigma^2+\tau^2}\Bigg/{\displaystyle\sqrt{\frac{\sigma^2\tau^2}{\sigma^2+\tau^2}}}\Bigg)\\
=&\Phi\Bigg(-\displaystyle \frac{\displaystyle \sigma^2\xi+\tau^2x}{\sqrt{\sigma^2+\tau^2} \sqrt{\sigma^2\tau^2}}\Bigg)\\
\end{array}
$$
ahora vamos a ver que pasa cuando $\xi\to 0$
$$
\lim_{\xi\to 0}-\displaystyle \frac{\displaystyle \sigma^2\xi+\tau^2x}{\sqrt{\sigma^2+\tau^2} \sqrt{\sigma^2\tau^2}}=-\displaystyle \frac{\displaystyle \tau^2x}{\sqrt{\sigma^2+\tau^2} \sqrt{\sigma^2\tau^2}}
$$
ahora vamor a ver que pasa cuando $\tau\to\infty$
$$
\begin{array}{rl}
\displaystyle \lim_{\tau\to\infty} -\displaystyle \frac{\displaystyle \tau^2x}{\sqrt{\sigma^2+\tau^2} \sqrt{\sigma^2\tau^2}}=& \displaystyle \lim_{\tau\to\infty}-\frac{\displaystyle \tau^2x}{\sqrt{\tau^2(\frac{\sigma^2}{\tau^2}+1}) \sigma\ \tau} \\
=&\displaystyle \lim_{\tau\to\infty}-\frac{\displaystyle \tau^2x}{\tau^2\ \sigma \sqrt{\frac{\sigma^2}{\tau^2}+1}}\\
=&\displaystyle \lim_{\tau\to\infty}-\frac{\displaystyle x}{\sigma \sqrt{\frac{\sigma^2}{\tau^2}+1}}=-\frac{x}{\sigma}
\end{array}
$$

-----

## 2.3.2 El *Factor de Bayes*

Una noción central a las pruebas Bayesianas es el *Factor de Bayes*
$$
B_{10}^\pi=\displaystyle \frac{\frac{P^\pi(\theta\in\Theta_1|x)}{P^\pi(\theta\in\Theta_0|x)}}{\frac{P^\pi(\theta\in\Theta_1)}{P^\pi(\theta\in\Theta_0)}}=\displaystyle \frac{\frac{P^\pi(\mathcal{M}_1|x)}{P^\pi(\mathcal{M}_0|x)}}{\frac{P^\pi(\mathcal{M}_1)}{P^\pi(\mathcal{M}_0)}}
$$
que corresponde a las posibilidades clásica (classical odds) o razón de verosimilitud, la diferencia está en que los parámetros se integran en lugar de maximizarce bajo cada modelo $\mathcal{M}_1$ versus $\mathcal{M}_0$.

Dado que es una simple transformada uno a uno de la probabilidad posterior, se puede usar para pruebas Bayesianas sin utilizar una función de pérdida específica, por ejemplo usando la escala de evidencia de Jeffreys:

- Si $0\le\log_{10}(B_{10}^\pi)<\frac{1}{2}$ la evidencia en contra $H_0$ es débil
- Si $\frac{1}{2}\le\log_{10}(B_{10}^\pi)<1$ la evidencia es sustancial
- Si $1\le\log_{10}(B_{10}^\pi)<2$ la evidencia es fuerte y 
- Si $\log_{10}(B_{10}^\pi)\ge2$ la evidencia es decisiva

A pesar de que esta escala está lejos de ser justificada bajo principios estrictos, provee una referencia para evaluar las hipótesis sin necesidad de definir las probabilidades *a priori* $H_0:\theta\in\Theta_0$ versus $H_1:\theta\in\Theta_1$, lo cual es una de las ventajas de utilizar el *Factor Bayesiano*.

En general, el *Factor de Bayes* obviamente depende de la información *a priori*, pero todavía se considera una respuesta Bayesiana objetiva en parte por que elimina la influencia del modelo *a priori* y enfatiza el rol de las observaciones.

Alternativamente, se puede notar (perceived) como una razón de verosimilitud Bayesiana dado que si $\pi_0$ y $\pi_1$ son distribuciones *a priori* bajo $H_0$ y $H_1$, respectivamente y si $P^\pi(\theta\in\Theta_0)=P^\pi(\theta\in\Theta_1)=\frac{1}{2}$, $B_{10}^\pi$ se puede expresar como
$$
B_{10}^\pi=\frac{\displaystyle\int_{\Theta_1}f_\theta(x)\pi_1(\theta)d\theta}{\displaystyle\int_{\Theta_0}f_\theta(x)\pi_0(\theta)d\theta}=\frac{m_1(x)}{m_0(x)}
$$
en la cual reemplazamos las verosimilitudes con las marginales bajo ambas hipótesis.

Cuando la hipótesis que se va a probar es una hipótesis nula puntual, $H_0:\theta=\theta_0$, hay dificultades en la construcción de un procedimiento Bayesiano, dado que, para una distribución *a priori* contínua $\pi$
$$
P^\pi(\theta=\theta_0)=0
$$

Obviamente, las hipótesis nulas puntuales se pueden criticar y considerar como artificiales e imposibles de probar (*¿Con cuanta frecuencia se puede distinguir entre $\theta=0$ de $\theta=0.0001$?!*), sin embargo también se deben realizar como parte de los requerimientos del análisis estadístico del día a día y también se pueden ver como una representación conveniente de algunos problemas de escogencia del modelo (model choice problems) que serán tratados luego.

La prueba de hipótesis nulas puntuales ameritan una modificación de la distribución *a priori* tal que cuando se prueba $H_0:\theta\in\Theta_0$ versus $H_1:\theta\in\Theta_1$, se cumple que
$$
\pi(\Theta_0)>0\ \ y \ \ \pi(\Theta_1)>0
$$
independientemente de los cálculos (measures) de $\Theta_0$ y $\Theta_1$ para la distribución *a priori* original, lo cual significa que la distribución *a priori* se puede descomponer como
$$
\pi(\theta)=P^\pi(\theta\in\Theta_0)\times\pi_0(\theta)+P^\pi(\theta\in\Theta_1)\times\pi_1(\theta)
$$
con pesos positivos en ambos conjuntos $\Theta_0$ y $\Theta_1$.

  > Note que esta modificación tiene sentido tanto desde el punto de vista de la información como operativo. Si $H_0:\theta=\theta_0$, el hecho de que la hipótesis es puesta a prueba implica que $\theta=\theta_0$ es una posibilidad y trae información *a priori* adicional sobre el parámetro $\theta$. Además, si se prueba $H_0$ y es aceptada, esto significa, en la mayoría de los casos, que el modelo (reducido) bajo $H_0$ se utilizará en lugar del modelo (completo) considerado anteriormente. Entonces, una distribución *a priori* bajo el modelo reducido debe estar disponible para inferencias posteriores potenciales. (Obviamente, el hecho de que esta inferencia posterior depende de la selección de la hipótesis nula $H_0$ también se debería tomar en consideración.)

En el caso especial $\Theta_0=\{\theta_0\}$, $\pi_0$ es la masa de *Dirac* en $\theta_0$, que significa que $P^{\pi_0}(\theta=\theta_0)=1$, y se requiere introducir un peso *a priori* separado de $H_0$, digamos,
$$
\rho=P(\theta=\theta_0)\ \text{ y }\ \pi(\theta)=\rho\ 1_{\theta_0}(\theta)\ +\ (1-\rho)\pi_1(\theta) 
$$
entonces,
$$
\begin{array}{rl}
\displaystyle \pi(\Theta_0|x)&=\displaystyle \frac{f_{\theta_0}(x)\rho}{\displaystyle\int f_\theta(x)\pi(\theta)d\theta} \\
&=\displaystyle \frac{f_{\theta_0}(x)\rho}{f_{\theta_0}(x)\rho\ +\ (1-\rho)m_1(x)}
\end{array}
$$

Para $x\sim\mathcal{N}(\mu,\sigma^2)$ y $\mu\sim\mathcal{N}(\xi,\tau^2)$, considere la prueba $H_0:\mu=0$. Podemos escoger $\xi$ igual a $0$ si no contamos con información *a priori* adicional. 
$$
\begin{array}{rl}
m_1(x)=&\displaystyle\int_{\Theta_1}f_\theta(x)\pi_1(\theta)d\theta\\
=&\displaystyle\int_{\Theta_1} \frac{1}{\sqrt{2\pi}\sigma}\exp\bigg\{-\frac{(x-\mu)^2}{2\sigma^2}\bigg\}
\frac{1}{\sqrt{2\pi}\tau}\exp\bigg\{-\frac{(\mu-\xi)^2}{2\tau^2}\bigg\}d\mu\\
=&\displaystyle\int_{\Theta_1} \frac{1}{\sqrt{2\pi}\sigma}\frac{1}{\sqrt{2\pi}\tau} \exp\bigg\{-\frac{(x-\mu)^2}{2\sigma^2}-\frac{(\mu-\xi)^2}{2\tau^2}\bigg\}d\mu\\
\end{array}
$$
vamos a ver el exponente de la exponencial
$$
\begin{array}{rl}
-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}-\frac{1}{2}\frac{(\mu-\xi)^2}{\tau^2}=&-\frac{1}{2}\frac{x^2-2x+\mu^2}{\sigma^2}-\frac{1}{2}\frac{\mu^2-2\mu\xi+\xi^2}{\tau^2}\\
=&-\frac{1}{2}\bigg[\frac{x^2}{\sigma^2}-\frac{2x\mu}{\sigma^2}+\frac{\mu^2}{\sigma^2}+\frac{\mu^2}{\tau^2}-\frac{2\mu\xi}{\tau^2}+\frac{\xi^2}{\tau^2}  \bigg]\\
=&-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)+\mu^2\bigg(\frac{1}{\sigma^2}+\frac{1}{\tau^2} \bigg) - 2\mu\bigg( \frac{x}{\sigma^2}+\frac{\xi}{\tau^2}  \bigg)  \bigg]\\
=&-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)+\bigg(\frac{1}{\sigma^2}+\frac{1}{\tau^2} \bigg)\bigg(\mu^2- 2\mu\bigg( \frac{x\tau^2+\xi\sigma^2}{\sigma^2\tau^2} \bigg)\bigg(\frac{\sigma^2\tau^2}{\sigma^2+\tau^2}\bigg)  \bigg]\\
=&-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)+\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg\{\mu^2- 2\mu\bigg( \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg) +\bigg( \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2-\bigg( \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2 \bigg\}  \bigg]\\
=&-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)+\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg\{\bigg(\mu -  \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2-\bigg( \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2 \bigg\}  \bigg]\\
=&-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)+\bigg\{\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg(\mu -  \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2-\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg( \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2 \bigg\}  \bigg]\\
=&-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)+\bigg\{\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg(\mu -  \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}  \bigg\}  \bigg]\\
\end{array}
$$

Ahora desarrollamos $m_1(x)$
$$
\begin{array}{rl}
m_1(x)=& \displaystyle\int_{\Theta_1} \frac{1}{\sqrt{2\pi}\sigma}\frac{1}{\sqrt{2\pi}\tau} e^{-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)+\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg(\mu -  \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}  \bigg]}d\mu\\
m_1(x)=& \displaystyle\int_{\Theta_1} \frac{1}{\sqrt{2\pi}\sigma}\frac{1}{\sqrt{2\pi}\tau} e^{-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}+\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg(\mu -  \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2\bigg]}d\mu\\
m_1(x)=& \displaystyle\int_{\Theta_1} \frac{1}{\sqrt{2\pi}\sigma}\frac{1}{\sqrt{2\pi}\tau} e^{-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}\bigg]}e^{\frac{1}{2}\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg(\mu -  \frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2}d\mu\\
m_1(x)=& \displaystyle \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}\bigg]} \int_{\Theta_1}\frac{1}{\sqrt{2\pi}\sigma\tau} e^{\frac{1}{2}\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg(\mu -\frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2}d\mu\\
m_1(x)=& \displaystyle \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}\bigg]} \int_{\Theta_1}\frac{\sqrt{\sigma^2+\tau^2}}{\sqrt{\sigma^2+\tau^2}}\frac{1}{\sqrt{2\pi}\sigma\tau} e^{\frac{1}{2}\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg(\mu -\frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2}d\mu\\
m_1(x)=& \displaystyle \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}\bigg]} \int_{\Theta_1}\frac{\sqrt{\sigma^2+\tau^2}}{\sqrt{2\pi\sigma^2\tau^2}} e^{\frac{1}{2}\bigg(\frac{\sigma^2+\tau^2}{\sigma^2\tau^2} \bigg)\bigg(\mu -\frac{x\tau^2+\xi\sigma^2}{\sigma^2+\tau^2} \bigg)^2}d\mu\\
m_1(x)=& \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\bigg[\bigg(\frac{x^2}{\sigma^2}+\frac{\xi^2}{\tau^2} \bigg)-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}\bigg]}\\
m_1(x)=& \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\bigg[\frac{x^2\tau^2+\xi^2\sigma^2}{\tau^2\sigma^2}-
\frac{(x\tau^2+\xi\sigma^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}\bigg]}
\end{array}
$$

Si $\xi=0$ tenemos:
$$
\begin{array}{rl}
m_1(x)=& \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\left[\frac{x^2\tau^2}{\tau^2\sigma^2}-
\frac{(x\tau^2)^2}{\sigma^2\tau^2(\sigma^2+\tau^2)}\right]}\\
m_1(x)=& \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\left[\frac{x^2}{\sigma^2}-
\frac{x^2\tau^2}{\sigma^2(\sigma^2+\tau^2)}\right]}\\
m_1(x)=& \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\frac{x^2}{\sigma^2}\left[1-
\frac{\tau^2}{\sigma^2+\tau^2}\right]}\\
m_1(x)=& \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\frac{x^2}{\sigma^2}\left[\frac{\sigma^2+\tau^2-\tau^2}{\sigma^2+\tau^2}\right]}\\
m_1(x)=& \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\frac{x^2}{\sigma^2}\left[\frac{\sigma^2}{\sigma^2+\tau^2}\right]}\\
m_1(x)=& \frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}} e^{-\frac{1}{2}\frac{x^2}{\sigma^2+\tau^2}}\\
m_1(x)=&\frac{1}{\sqrt{2\pi}\sqrt{\sigma^2+\tau^2}}e^{ -\frac{1}{2}\frac{x^2}{\sigma^2+\tau^2}}
\end{array}
$$

Como
$$
f_0(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\bigg\{-\frac{(x-0)^2}{2\sigma^2} \bigg\}=\frac{1}{\sqrt{2\pi}\sigma}\exp\bigg\{-\frac{x^2}{2\sigma^2} \bigg\}\sim\mathcal{N(\mu,\sigma^2)}\text{ con }\mu=0
$$

entonces $B_{10}^\pi$
$$
\begin{array}{rl}
B_{10}^\pi & =\frac{m_1(x)}{f_0(x)}\\
B_{10}^\pi & =\frac{ \frac{1}{\sqrt{2\pi}\sqrt{\sigma^2+\tau^2}}e^{ -\frac{1}{2}\frac{x^2}{\sigma^2+\tau^2}}}{\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^2}{2\sigma^2}}}\\
B_{10}^\pi & =\frac{\sqrt{\sigma^2}}{\sqrt{\sigma^2+\tau^2}} e^{ -\frac{1}{2}\frac{x^2}{\sigma^2+\tau^2}+\frac{x^2}{2\sigma^2}}\\
B_{10}^\pi & =\frac{\sqrt{\sigma^2}}{\sqrt{\sigma^2+\tau^2}} e^{ -\frac{1}{2}\left( \frac{x^2}{\sigma^2+\tau^2}-\frac{x^2}{\sigma^2}\right)}\\
B_{10}^\pi & =\frac{\sqrt{\sigma^2}}{\sqrt{\sigma^2+\tau^2}} e^{ -\frac{1}{2}\left( \frac{x^2\sigma^2 -x^2(\sigma^2+\tau^2)}{\sigma^2(\sigma^2+\tau^2)}\right)}\\
B_{10}^\pi & =\frac{\sqrt{\sigma^2}}{\sqrt{\sigma^2+\tau^2}} e^{-\frac{1}{2}\left( \frac{-x^2\tau^2}{\sigma^2(\sigma^2+\tau^2)}\right)}\\
B_{10}^\pi & =\frac{\sqrt{\sigma^2}}{\sqrt{\sigma^2+\tau^2}} e^{ \frac{x^2\tau^2}{2\sigma^2(\sigma^2+\tau^2)}}
\end{array}
$$

Entonces el *Factor de Bayes* es la razón de las marginales bajo ambas hipótesis, $\mu=0$ y $\mu\ne 0$,
$$
B_{10}^\pi=\frac{m_1(x)}{f_0(x)}=\sqrt{\frac{\sigma^2}{\sigma^2+\tau^2}}e^{\frac{\tau^2x^2}{2\sigma^2(\sigma^2+\tau^2)}}
$$
y si partimos de que $\rho=P(\theta=\theta_0)$ y $\pi(\theta)=\rho 1_{\Theta_0}(\theta)+(1-\rho)\pi_1(\theta)$
$$
\begin{array}{rl}
\pi(\Theta_0|x) & =\frac{f_{\Theta_0}(x)\rho}{\int f_{\Theta_0}(x)\pi(\theta)d\theta}\\
\pi(\Theta_0|x) & =\frac{f_{\Theta_0}(x)\rho}{f_{\Theta_0}(x)\rho+(1-\rho)m_1(x)}\\
\pi(\Theta_0|x) & =\frac{\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^2}{2\sigma^2} }\rho}{\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^2}{2\sigma^2} }\rho+(1-\rho)\frac{1}{\sqrt{2\pi}\sqrt{\sigma^2+\tau^2}}e^{ -\frac{1}{2}\frac{x^2}{\sigma^2+\tau^2}}}\\
\pi(\Theta_0|x) & =\frac{\frac{1}{\sigma}e^{-\frac{x^2}{2\sigma^2} }\rho}{\frac{1}{\sigma}e^{-\frac{x^2}{2\sigma^2} }\rho+(1-\rho)\frac{1}{\sqrt{\sigma^2+\tau^2}}e^{ -\frac{1}{2}\frac{x^2}{\sigma^2+\tau^2}}} \frac{\frac{1}{\frac{1}{\sigma}e^{-\frac{x^2}{2\sigma^2} }\rho}}{\frac{1}{\frac{1}{\sigma}e^{-\frac{x^2}{2\sigma^2} }\rho}}\\
\pi(\Theta_0|x) & =\frac{1}{1+(1-\rho)\frac{1}{\sqrt{\sigma^2+\tau^2}}\frac{\sigma}{\rho} e^{ -\frac{1}{2}\frac{x^2}{\sigma^2+\tau^2}}e^{\frac{1}{2}\frac{x^2}{\sigma^2}}}\\
\pi(\Theta_0|x) & =\frac{1}{1+(1-\rho)\frac{1}{\sqrt{\sigma^2+\tau^2}}\frac{\sigma}{\rho} e^{ -\frac{1}{2}\left(\frac{x^2}{\sigma^2+\tau^2}-\frac{x^2}{\sigma^2}\right)}}\\
\pi(\Theta_0|x) & =\frac{1}{1+(1-\rho)\frac{1}{\sqrt{\sigma^2+\tau^2}}\frac{\sigma}{\rho} e^{ -\frac{1}{2}\left(\frac{x^2\sigma^2-x^2(\sigma^2+\tau^2)}{\sigma^2(\sigma^2+\tau^2)}\right)}}\\
\pi(\Theta_0|x) & =\frac{1}{1+\frac{(1-\rho)}{\rho}\frac{\sqrt{\sigma^2}}{\sqrt{\sigma^2+\tau^2}} e^{ \frac{1}{2}\frac{x^2\tau^2}{\sigma^2(\sigma^2+\tau^2)}}}\\
\end{array}
$$

entonces $\pi(\mu=0|x)$
$$
\pi(\mu=0|x)=\bigg[1\ +\ \frac{1-\rho}{\rho} \displaystyle \sqrt{\frac{\sigma^2}{\sigma^2+\tau^2}}\exp\bigg\{\frac{\tau^2x^2}{2\sigma^2(\sigma^2+\tau^2)} \bigg\}\bigg]^{-1}
$$
es la probabilidad posterior de $H_0$. La Tabla 2.1 ofrece una idea de los valores de la probabilidad posterior cuando la cantidad normalizada $x/\sigma$ varía. Esta probabilidad posterior obviamente depende de la selección de la variancia *a priori* $\tau^2$: La dependencia es en realidad muy severa, como veremos más adelante con la paradoja de *Jeffreys Lindley*.

**Tabla 2.1.** Probabilidad posterior de $\mu=0$ para valores diferentes de $z=x/\sigma$, $\rho=1/2$ y para $\tau=\sigma$ (fila superior), $\tau^2=10\sigma^2$ (fila inferior).

$$
\begin{array}{ccccc}
\hline
z              & 0 & 0.68  & 1.28  & 1.96  \\ \hline
\pi(\mu=0|z)   & 0.586                  & 0.557 & 0.484 & 0.351 \\ \hline
\pi(\mu=0|z)   & 0.768                  & 0.729 & 0.612 & 0.366 \\ \hline
\end{array}
$$
Para **normaldata**, si escogemos $\tau$ igual a $0.1$, el *Factor de Bayes* contra $\mu=0$ sólo depende de $\overline{x}_n\sim\mathcal{N}(\mu,\hat\sigma^2/90)$ y es igual a
$$
B_{10}^\pi(\overline{x}_n)=\sqrt{\frac{\hat\sigma^2}{\hat\sigma^2+n\tau^2}}\exp\bigg\{\frac{n\tau^2\ \overline{x}^2_n}{2\hat\sigma^2(\hat\sigma^2+n\tau^2)} \bigg\}=0.1481
$$
El conjunto de datos, por tanto, favorece la hipótesis nula $H_0:\mu=0$. Sin embargo, otras elecciones de $\tau$ pueden resultar en evaluaciones numéricas diferentes, como se muestra en la Figura 2.6, como el *Factor de Bayes* varía entre $1$ y $0$ así como $\tau$ crece de $0$ a $\infty$.

```{r, echo=TRUE}
from<-10^-4
to<-10
step<-0.001

b10<-c()
sdsqred<-sd(larcenies)^2
n<-length(larcenies)
xbarsqred<-mean(larcenies)^2

#testing
tau<-0.1
value<-sqrt(sdsqred/(sdsqred+n*tau^2))*
  exp(n*tau^2*xbarsqred/(2*sdsqred*(sdsqred+n*tau^2)))
value

value<-0
tau<-from
while (tau<=to) {
  value<-sqrt(sdsqred/(sdsqred+n*tau^2))*
    exp(n*tau^2*xbarsqred/(2*sdsqred*(sdsqred+n*tau^2)))
  b10<-c(b10,value)
  tau<-tau+step
}
plot(seq(from,to,step),b10,type = "l",log = "x",xlim=c(from, to))
```


**Figura 2.6.** Datos **normaldata**: Rango del *Factor de Bayes* $B_{10}^\pi$ cuando $\tau$ va de $10^{-4}$ a 10. (El eje $x$ está en escala logarítmica)

## 2.3.3 El veto a las distribuciones *a priori* impropias

Desafortunadamente, esta descomposición de la distribución *a priori* en dos sub distribuciones *a priori* genera una seria dificultad relacionada con las distribuciones *a priori* imprópias, que implica en términos prácticos, el veto de su uso en situaciones de prueba (testing situations). De hecho, cuando se utiliza la representación
$$
\pi(\theta)=P(\theta\in\Theta_0)\times\pi_0(\theta)+P(\theta\in\Theta_1)\times\pi_1(\theta)
$$
los pesos $P(\theta\in\Theta_0)$ y $P(\theta\in\Theta_1)$ son significativos sólo si $\pi_0$ y $\pi_1$ son densidades de probabilidad normalizadas. De lo contrario, no se pueden interpretar como *pesos*.

Cuando $x\sim\mathcal{N}(\mu,1)$ y $H_0:\mu=0$, la distribución *a priori* imprópia (Jeffreys) es $\pi_1(\mu)=1$; si escribimos
$$
\pi(\mu)=\frac{1}{2} 1_{0}(\mu)+\frac{1}{2} \cdotp 1
$$

entonces la probabilidad posterior es
$$
\pi(\mu=0|x)=  \frac{\displaystyle e^{-\frac{x^2}{2}}}{e^{-\frac{x^2}{2}}\ +\ \displaystyle \int_{-\infty}^{\infty}e^{-\frac{(x-\theta)^2}{2}}d\theta}=\frac{1}{1+\sqrt{2\pi}\ e^{\frac{x^2}{2}}}
$$

Una primera consecuencia de esta elección la probabilidad posterior de $H_0$ es acotada superiormente por
$$
\pi(\mu=0|x)\le\frac{1}{(1+\sqrt{2\pi})}=0.285
$$
La Tabla 2.2 provee la evolución (el cambio) de esta probabilidad cuando $x$ se separa del $0$. Un punto interesante es que los valores numéricos, de alguna forma, coinciden con los *p*-valores utilizados en las pruebas de hipótesis clásicas (Casella and Berger, 2001).

**Tabla 2.2.** Probabilidad posterior de $H_0:\mu=0$ para la distribución *a priori* de Jeffrey $\pi_1(\mu)=1$ bajo $H_1$.

$$
\begin{array}{cccccc}
\hline
x              & 0     & 1.0   & 1.65  & 1.96  & 2.58  \\ \hline
\pi(\mu=0|x)   & 0.285 & 0.195 & 0.089 & 0.055 & 0.014 \\ \hline
\end{array}
$$

-----  
  
  > Si estamos interesados en probar $H_0: \theta\le 0$ versus $H_1:\theta>0$ entonces la probabilidad posterior es
  $$
    \pi(\theta\le 0|x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^0 e^{-\frac{(x-\theta)^2}{2}}d\theta=\Phi(-x)
  $$
  y la respuesta es ahora *exáctamente* el *p*-valor que encontramos en la estadística clásica.

-----

Para **normaldata**, si consideramos el párametro $\theta=(\mu,\sigma^2)$ con una distribución que proviene de una distribución *a priori* no informativa $\pi(\theta)=1/\sigma^2$, la probabilidad posterior de que $\mu$ sea positiva es una probabilidad de que una distribución t-Student sea positiva con $89$ grados de libertad, media $-0.0144$ y variancia $0.000206$. Esto es esencialmente una distribución normal $\mathcal{N}(-0.0144,0.000206)$ y la probabilidad correspondiente es $0.0021$, la cual es muy pequeña para la hipótesis $H_0:\mu>0$ se sostenga bajo cualquier verosimilitud.


-----

La dificultad en el uso de una distribución *a priori* imprópia se relaciona a la llamada paradoja de *Jeffrey Lindley*, un fenómeno que muestra que los argumentos limitantes (limiting arguments) no son válidos en situaciones de pruebas de hipótesis. Por el contrario en estimación, las distribuciones *a priori* no informativas no corresponden mas al límite de inferencias conjugadas. De hecho, para una distribución *a priori* conjugada, la probabilidad posterior
$$
\pi(\mu=0|x)=\bigg[1\ +\ \frac{1-\rho_0}{\rho_0} \displaystyle \sqrt{\frac{\sigma^2}{\sigma^2+\tau^2}}\exp\bigg\{\frac{\tau^2x^2}{2\sigma^2(\sigma^2+\tau^2)} \bigg\}\bigg]^{-1}
$$
converge a $1$ cuando $\tau$ va a $+\infty$, para cada valor de $x$, como se ilustró en la Figura 2.6.
Este procedimiento no informativo, obviamente difiere de la respuesta no informativa previa $[1+\sqrt{2\pi}\exp(x^2/2)]^{-1}$.

El principal obstáculo (bars us) que impide el uso de *a priori* imprópios en uno o dos conjuntos $\Theta_0$ y $\Theta_1$ es una dificultad para normalizar (normalizing difficulty): Si $g_0$ y $g_1$ son medidas (en lugar de probabilidades) sobre los subespacios $\Theta_0$ y $\Theta_1$, la selección de las constantes normalizadoras influencia en *Factor de Bayes*. De hecho, cuando $g_i$ es reemplazada por $c_i g_i (i=0,1)$, donde $c_i$ es una constante arbitraria, el *Factor de Bayes* es multiplicado por $c_0/c_1$. Entonces, por ejemplo, si el *a priori* de Jeffrey es uniforme y $g_0=c_0$,$g_1=c_1$, la probabilidad posterior
$$
\begin{array}{rr}
\displaystyle \pi(\theta\in\Theta_0|x)=&\frac{\displaystyle \rho_0c_0\int_{\Theta_0}f(x|\theta)d\theta}{\displaystyle \rho_0c_0\int_{\Theta_0}f(x|\theta)d\theta\ +\ (1-\rho_0)c_1\int_{\Theta_1}f(x|\theta)d\theta}\\
=&\frac{\displaystyle \rho_0\int_{\Theta_0}f(x|\theta)d\theta}{\displaystyle \rho_0\int_{\Theta_0}f(x|\theta)d\theta\ +\ (1-\rho_0)[c_1/c_0]\int_{\Theta_1}f(x|\theta)d\theta}
\end{array}
$$
es completamente determinado por la selección de $[c_0/c_1]$. Esto implica, por ejemplo, que la función $[1+\sqrt{2\pi}\exp(x^2/2)]^{-1}$ obtenida previamente no tiene ninguna validez.

  > Dado que los *a priori* imprópios son una parte esencial del enfoque Bayesiano, han habido muchas propuestas para superar este obstáculo. Muchos utilizan una herramienta (device) que transforma el *a priori* en una distribución de probabilidad utilizando una porción de los datos $\mathcal{D}$ y luego utilizan la otra parte de los datos para realizar una prueba de hipótesis en una situación estándar. La variedad de soluciones disponibles se debe a las muchas posibilidades de eliminar la dependencia en la selección de una porción de los datos utilizados en la primera parte. El procedimiento resultante es llamado *pseudo Factor de Bayes*, aunque algunos realmente sean *Factores de Bayes*. Ver Robert (2001, Capítulo 6) para más detalles.
  
-----

Para **CMBdata**, considere dos muestras construidas tomando dos segmentos sobre la imagen, como se ve en la Figura 2.7.. Podemos considerar esas muestras como, $(x_1,\ldots,x_n)$ y $(y_1,\ldots,y_n)$, ambas provenientes de una distribución normal, $\mathcal{N}(\mu_x, \sigma^2)$ y $\mathcal{N}(\mu_y, \sigma^2)$. La pregunta de interés es si ambas medias son iguales, $H_0:\mu_x=\mu_y$.

**FALTA LA GRAFICA**

**Figura 2.7.** Datos **CMBdata**: Muestras del conjunto de datos representadas por segmentos de recta.

Para tomar ventaja de la estructura de este modelo, podemos asumir que $\sigma^2$ es el mismo error de medición bajo ambos modelos y por tanto comparten la misma distribución *a priori*. Esto significa que el *Factor de Bayes*
$$
B_{10}^\pi=\frac{\int l(\mu_x,\mu_y,\sigma|\mathcal{D})\pi(\mu_x,\mu_y)\pi_\sigma(\sigma^2)d\sigma^2d\mu_x d\mu_y}{\int l(\mu,\sigma|\mathcal{D})\pi_\mu(\mu)\pi_\sigma(\sigma^2)d\sigma^2d\mu}
$$
no depende de la constante de normalización utilizada para $\pi_\sigma(\sigma^2)$ y por tanto todavía podemos utilizar *a priori* imprópios como $\pi_\sigma(\sigma^2)=1/\sigma^2$ en este caso. Adicionalmente, podemos reescribir $\mu_x$ y $\mu_y$ como $\mu_x=\mu-\xi$ y $\mu_y=\mu+\xi$ respectivamente y utilizar un *a priori* de la forma $\pi(\mu,\xi)=\pi_\mu(\mu)\pi_\xi(\xi)$ en la nueva parametrización, así, de nuevo, el mismo *a priori* $\pi_\mu$ se puede utilizar bajo ambas $H_0$ y su alternativa. La misma cancelación de la constante de normalización ocurre para $\pi_\mu$ y la selección del *a priori* $\pi_\mu(\mu)=1$ y $\xi\sim\mathcal{N}(0,1)$, es decir:
$$
\begin{array}{rl}
& \pi_\sigma(\sigma^2)  = \frac{1}{\sigma^2}\\
& \mu_x  = \mu-\epsilon\\
& \mu_y  = \mu+\epsilon\\
& \pi(\mu,\xi) = \pi_\mu(\mu)\pi_\xi(\xi)\\
&\pi_\mu(\mu)=1\\
& \xi \sim N(0,1)\implies \pi_\xi(\xi)=\frac{1}{\sqrt{2\pi}1}e^{-\frac{1}{2}\frac{(\xi-0)^2}{1^2}}=\frac{1}{\sqrt{2\pi}}e^{-\frac{\xi^2}{2}}
\end{array}
$$

Así
$$
\begin{array}{rl}
l(\mu_x,\mu_y,\sigma|\mathcal{D}) & = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-\mu_x)^2}{2\sigma^2}} \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-\mu_y)^2}{2\sigma^2}}\\
l(\mu_x,\mu_y,\sigma|\mathcal{D}) & = \left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n e^{-\frac{\sum_{i=1}^n(x_i-\mu_x)^2}{2\sigma^2}}\left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n e^{-\frac{\sum_{i=1}^n(y_i-\mu_y)^2}{2\sigma^2}}\\
l(\mu_x,\mu_y,\sigma|\mathcal{D}) & =\left( \frac{1}{\sqrt{2\pi}\sigma} \right)^{2n} e^{-\frac{\sum_{i=1}^n(x_i-\mu_x)^2}{2\sigma^2} -\frac{\sum_{i=1}^n(y_i-\mu_y)^2}{2\sigma^2}}\\
l(\mu_x,\mu_y,\sigma|\mathcal{D}) & =\left( \frac{1}{\sqrt{2\pi}\sigma} \right)^{2n} e^{-\frac{1}{2\sigma^2} (\sum_{i=1}^n(x_i-\mu_x)^2 +\sum_{i=1}^n(y_i-\mu_y)^2 ) }\\
\end{array}
$$

Sustituyendo $\mu_x  = \mu-\epsilon$ y $\mu_y  = \mu+\epsilon$
$$
\begin{array}{rl}
& \sum_{i=1}^n (x_i-\mu_x)^2 +\sum_{i=1}^n (y_i-\mu_y)^2 \\
& = \sum_{i=1}^n (x_i-(\mu-\epsilon))^2 +\sum_{i=1}^n (y_i-(\mu+\epsilon))^2\\
& = \sum_{i=1}^n (x_i^2-2x_i(\mu-\epsilon)+(\mu-\epsilon)^2) +\sum_{i=1}^n (y_i^2-2y_i(\mu+\epsilon)+(\mu+\epsilon)^2)\\
& = \sum_{i=1}^n x_i^2-2n\bar{x}(\mu-\epsilon)+n(\mu-\epsilon)^2 +\sum_{i=1}^n y_i^2-2n\bar{y}(\mu+\epsilon)+n(\mu+\epsilon)^2\\
& = \sum_{i=1}^n x_i^2-2n\bar{x}(\mu-\epsilon)+n\bar{x}^2-n\bar{x}^2+ n(\mu-\epsilon)^2 +\sum_{i=1}^n y_i^2-2n\bar{y}(\mu+\epsilon)\\
& \ \ \ +n\bar{y}^2-n\bar{y}^2+n(\mu+\epsilon)^2\\
& = \sum_{i=1}^n x_i^2-n\bar{x}^2 + n(\bar{x}-(\mu-\epsilon))^2 +\sum_{i=1}^n y_i^2-n\bar{y}^2 +n(\bar{y}-(\mu+\epsilon))^2
\end{array}
$$
Donde:
$$
\begin{array}{rl}
\sum_{i=1}^n x_i^2-n\bar{x}^2 & = \sum_{i=1}^n x_i^2-2n\bar{x}^2+n\bar{x}^2\\
\sum_{i=1}^n x_i^2-n\bar{x}^2 & = \sum_{i=1}^n x_i^2-2\bar{x}n\bar{x}+n\bar{x}^2\\
\sum_{i=1}^n x_i^2-n\bar{x}^2 & = \sum_{i=1}^n x_i^2-2\bar{x}n\frac{\sum_{i=1}^n x_i}{n}+n\bar{x}^2\\
\sum_{i=1}^n x_i^2-n\bar{x}^2 & = \sum_{i=1}^n x_i^2-2\bar{x}\sum_{i=1}^n x_i+n\bar{x}^2\\
\sum_{i=1}^n x_i^2-n\bar{x}^2 & = \sum_{i=1}^n x_i^2-2\bar{x} x_i+\bar{x}^2\\
\sum_{i=1}^n x_i^2-n\bar{x}^2 & = \sum_{i=1}^n (x_i-\bar{x})^2
\end{array}
$$

Obteniendo un resultado similar para $y_i$, es decir, $\sum_{i=1}^n y_i^2-n\bar{y}^2 = \sum_{i=1}^n (y_i-\bar{y})^2$. Sustituyendolos:
$$
\begin{array}{rl}
\sum_{i=1}^n (x_i-\mu_x)^2 +\sum_{i=1}^n (y_i-\mu_y)^2 & = \sum_{i=1}^n (x_i-\bar{x})^2 + n(\bar{x}-(\mu-\epsilon))^2 +\sum_{i=1}^n (y_i-\bar{y})^2 +n(\bar{y}-(\mu+\epsilon))^2\\
\sum_{i=1}^n (x_i-\mu_x)^2 +\sum_{i=1}^n (y_i-\mu_y)^2 & = \frac{n\sum_{i=1}^n (x_i-\bar{x})^2}{n} + n(\bar{x}-(\mu-\epsilon))^2 +\frac{n\sum_{i=1}^n (y_i-\bar{y})^2}{n} +n(\bar{y}-(\mu+\epsilon))^2\\
\sum_{i=1}^n (x_i-\mu_x)^2 +\sum_{i=1}^n (y_i-\mu_y)^2 & = n \left[ \frac{\sum_{i=1}^n (x_i-\bar{x})^2}{n}  +\frac{\sum_{i=1}^n (y_i-\bar{y})^2}{n} + (\bar{x}-\mu+\epsilon)^2 +(\bar{y}-\mu-\epsilon)^2\right]\\
\sum_{i=1}^n (x_i-\mu_x)^2 +\sum_{i=1}^n (y_i-\mu_y)^2 & = n \left[ S^2 + (\bar{x}-\mu+\epsilon)^2 +(\bar{y}-\mu-\epsilon)^2\right]\\
\end{array}
$$

Donde $S^2= \frac{\sum_{i=1}^n (x_i-\bar{x})^2}{n}  +\frac{\sum_{i=1}^n (y_i-\bar{y})^2}{n}$. Así tenemos:
$$
\begin{array}{rl}
l(\mu_x,\mu_y,\sigma|\mathcal{D}) & =\left( \frac{1}{\sqrt{2\pi}\sigma} \right)^{2n} e^{-\frac{1}{2\sigma^2} n \left[ S^2 + (\bar{x}-\mu+\epsilon)^2 +(\bar{y}-\mu-\epsilon)^2\right]}
\end{array}
$$

Con este resultado y las prioris supuestas tenemos:
$$
\begin{array}{rl}
B_{10}^\pi=&\frac{\displaystyle \int \left( \frac{1}{\sqrt{2\pi}\sigma} \right)^{2n} e^{-\frac{1}{2\sigma^2} n \left[ S^2 + (\bar{x}-\mu+\epsilon)^2 +(\bar{y}-\mu-\epsilon)^2\right]} \pi(\mu,\xi)\pi_\sigma(\sigma^2) d\sigma^2 d\mu \ d\xi}
{\displaystyle \int \left( \frac{1}{\sqrt{2\pi}\sigma} \right)^{2n} e^{-\frac{1}{2\sigma^2} n \left[ S^2 + (\mu-\bar{x})^2 +(\mu-\bar{y})^2\right]}\pi_\mu(\mu) \pi_\sigma(\sigma^2) d\sigma^2\ d\mu}\\
B_{10}^\pi=&\frac{\displaystyle \int \left( \frac{1}{\sqrt{2\pi}\sigma} \right)^{2n} e^{-\frac{1}{2\sigma^2} n \left[ S^2 + (\bar{x}-\mu+\epsilon)^2 +(\bar{y}-\mu-\epsilon)^2\right]} \pi(\mu)\pi(\xi)\pi_\sigma(\sigma^2) d\sigma^2 d\mu \ d\xi}
{\displaystyle \int \left( \frac{1}{\sqrt{2\pi}\sigma} \right)^{2n} e^{-\frac{1}{2\sigma^2} n \left[ S^2 + (\mu-\bar{x})^2 +(\mu-\bar{y})^2\right]}\pi_\mu(\mu) \pi_\sigma(\sigma^2) d\sigma^2\ d\mu}\\
B_{10}^\pi=&\frac{\displaystyle \int \frac{1}{\sigma^{2n}}e^{-\frac{1}{2\sigma^2} n \left[ S^2 + (\bar{x}-\mu+\epsilon)^2 +(\bar{y}-\mu-\epsilon)^2\right]} 1 \frac{1}{\sqrt{2\pi}}e^{-\frac{\xi^2}{2}}\frac{1}{\sigma^2}d\sigma^2 d\mu \ d\xi}
{\displaystyle \int \frac{1}{\sigma^{2n}} e^{-\frac{1}{2\sigma^2} n \left[ S^2 + (\mu-\bar{x})^2 +(\mu-\bar{y})^2\right]}1 \frac{1}{\sigma^2} d\sigma^2\ d\mu}\\
B_{10}^\pi=&\frac{\displaystyle \int \frac{1}{\sigma^{2(n+1)}}e^{-\frac{1}{2\sigma^2} n \left[ S^2 + (\bar{x}-\mu+\epsilon)^2 +(\bar{y}-\mu-\epsilon)^2\right]} \frac{1}{\sqrt{2\pi}}e^{-\frac{\xi^2}{2}} d\sigma^2 d\mu \ d\xi}
{\displaystyle \int \frac{1}{\sigma^{2(n+1)}} e^{-\frac{1}{2\sigma^2} n \left[ S^2 + (\mu-\bar{x})^2 +(\mu-\bar{y})^2\right]} d\sigma^2\ d\mu}
\end{array}
$$

Resolviendo ambas integrales con respecto a $\sigma^2$:
$$
\begin{array}{rl}
&\displaystyle \int\int\int \frac{1}{\sigma^{2(n+1)}}e^{-\frac{1}{2\sigma^2} n \left[ S^2 + (\bar{x}-\mu+\epsilon)^2 +(\bar{y}-\mu-\epsilon)^2\right]} \frac{1}{\sqrt{2\pi}}e^{-\frac{\xi^2}{2}} d\sigma^2 d\mu \ d\xi\\
& = \displaystyle \int\int B\int \frac{1}{\sigma^{2(n+1)}}e^{-\frac{1}{\sigma^2} A} d\sigma^2 d\mu \ d\xi
\end{array}
$$
Donde $A=\frac{n \left[ S^2 + (\bar{x}-\mu+\epsilon)^2 +(\bar{y}-\mu-\epsilon)^2\right]}{2}$ y $B=\frac{1}{\sqrt{2\pi}}e^{-\frac{\xi^2}{2}} d\sigma^2$

Haciendo el cambio de variable $u=\frac{1}{\sigma^2}\implies du=-\frac{1}{(\sigma^2)^2}d\sigma^2$
$$
\begin{array}{rl}
& = \displaystyle \int\int B\int \left( \frac{1}{\sigma^{2}}\right)^{n+1}e^{-\frac{1}{\sigma^2} A} d\sigma^2 d\mu \ d\xi\\
& = \displaystyle \int\int B\int \left( \frac{1}{\sigma^{2}}\right)^{n+1-2}e^{-\frac{1}{\sigma^2} A} \left( \frac{1}{\sigma^{2}}\right)^{2} d\sigma^2 d\mu \ d\xi\\
& = \displaystyle \int\int B\int \left( \frac{1}{\sigma^{2}}\right)^{n-1}e^{-\frac{1}{\sigma^2} A} \left( \frac{1}{\sigma^{2}}\right)^{2} d\sigma^2 d\mu \ d\xi\\
& = \displaystyle \int\int B\int u^{n-1}e^{-uA} d\sigma^2 d\mu \ d\xi\\
& = \displaystyle \int\int B\int u^{n-1}e^{-uA}\frac{A^{n}}{\Gamma(n)}\frac{\Gamma(n)}{A^{n}} d\sigma^2 d\mu \ d\xi\\
& = \displaystyle \int\int B\frac{\Gamma(n)}{A^{n}}\int u^{n-1}e^{-uA}\frac{A^{n}}{\Gamma(n)} d\sigma^2 d\mu \ d\xi\\
& = \displaystyle \int\int B\frac{\Gamma(n)}{A^{n}} d\mu \ d\xi\\
& = \displaystyle \int\int \frac{1}{\sqrt{2\pi}}e^{-\frac{\xi^2}{2}} \frac{\Gamma(n)}{\left(\frac{n \left[ S^2 + (\bar{x}-\mu+\epsilon)^2 +(\bar{y}-\mu-\epsilon)^2\right]}{2}\right)^{n}} d\mu \ d\xi\\
& = \displaystyle \Gamma(n)2^{n} \int \frac{e^{-\frac{\xi^2}{2}}}{{\sqrt{2\pi}}} \frac{1}{\left(n \left[ S^2 + (\bar{x}-\mu+\epsilon)^2 +(\bar{y}-\mu-\epsilon)^2\right] \right)^{n}} d\mu \ d\xi\\
& = \displaystyle \Gamma(n)2^{n} \int \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left(n \left[ S^2 + (\bar{x}-\mu+\epsilon)^2 +(\bar{y}-\mu-\epsilon)^2\right] \right)^{-n} d\mu \ d\xi\\
& = \displaystyle \Gamma(n)2^{n}n^{-n} \int \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left[ S^2 + (\bar{x}-\mu+\epsilon)^2 +(\bar{y}-\mu-\epsilon)^2\right]^{-n} d\mu \ d\xi
\end{array}
$$

En la otra integral tenemos:
$$
\begin{array}{rl}
& \displaystyle \int \frac{1}{\sigma^{2(n+1)}} e^{-\frac{1}{2\sigma^2} n \left[ S^2 + (\mu-\bar{x})^2 +(\mu-\bar{y})^2\right]} d\sigma^2\ d\mu\\
& = \displaystyle \int \int \frac{1}{\sigma^{2(n+1)}}e^{-\frac{1}{\sigma^2} C} d\sigma^2 d\mu\\
& = \displaystyle \int\int u^{n-1}e^{-uC}\frac{C^{n}}{\Gamma(n)}\frac{\Gamma(n)}{C^{n}} d\sigma^2 d\mu\\
& = \displaystyle \int\frac{\Gamma(n)}{C^{n}}\int u^{n-1}e^{-uC}\frac{C^{n}}{\Gamma(n)} d\sigma^2 d\mu\\
& = \displaystyle \int \Gamma(n)C^{-n}d\mu\\ 
& = \displaystyle \int \Gamma(n)\left(\frac{n \left[ S^2 + (\mu-\bar{x})^2 +(\mu-\bar{y})^2\right]}{2}\right)^{-n}d\mu\\
& = \Gamma(n) n^{-n}2^n\displaystyle \int \left[ S^2 + (\mu-\bar{x})^2 +(\mu-\bar{y})^2\right]^{-n}d\mu\\
\end{array}
$$

Sustituyendo el resultados de las integrales en el factor de Bayes nos queda:
$$
\begin{array}{rl}
B_{10}^\pi = & \frac{\displaystyle \Gamma(n)2^{n}n^{-n} \int \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left[ S^2 + (\bar{x}-\mu+\epsilon)^2 +(\bar{y}-\mu-\epsilon)^2\right]^{-n} d\mu \ d\xi}{\Gamma(n) n^{-n}2^n\displaystyle \int \left[ S^2 + (\mu-\bar{x})^2 +(\mu-\bar{y})^2\right]^{-n}d\mu}\\ 
B_{10}^\pi = & \frac{\displaystyle  \int \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left[ S^2 + (\bar{x}-\mu+\epsilon)^2 +(\bar{y}-\mu-\epsilon)^2\right]^{-n} d\mu \ d\xi}{\displaystyle \int \left[ S^2 + (\mu-\bar{x})^2 +(\mu-\bar{y})^2\right]^{-n}d\mu} 
\end{array}
$$

Donde $S^2$ denota la media
$$
S^2=\frac{1}{n}\sum_{i=1}^{n}(x_i-\bar{x})^2+\frac{1}{n}\sum_{i=1}^{n}(y_i-\bar{y})^2
$$

Mientras que el denominador se puede integrar, el numerador no. Una aproximación numérica de $B_{10}^\pi$ es necesaria. Este asunto será tratado en la Sección 2.4.

-----

### Ejercicio 2.21.

Recordemos que la constante de normalización para la distribución t-Student $\tau(\nu,\mu,\sigma^2)$ es
$$
\displaystyle \frac{\frac{\Gamma\bigg(\frac{\nu+1}{2}\bigg)}{\Gamma\big(\frac{\nu}{2}\big)}}{\sigma\sqrt{\nu\pi}}=\displaystyle \frac{\Gamma\bigg(\frac{\nu+1}{2}\bigg)}{\sigma\sqrt{\nu\pi}\ \Gamma\big(\frac{\nu}{2}\big)}
$$

Calcule el valor de la integral en el denominador de $B_{10}^\pi$ previo.

-----

Primero vamos a desarrollar
$$
\begin{array}{rl}
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&(\mu^2-2\mu\bar{x}+\bar{x}^2)+(\mu^2-2\mu\bar{y}+\bar{y}^2)\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2\mu^2-2\mu(\bar{x}+\bar{y})+\bar{x}^2+\bar{y}^2\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2(\mu^2-\mu(\bar{x}+\bar{y}))+\bar{x}^2+\bar{y}^2\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2(\mu^2-\mu(\bar{x}+\bar{y}) + \big(\frac{\bar{x}+\bar{y}}{2}\big)^2-\big(\frac{\bar{x}+\bar{y}}{2}\big)^2)+\bar{x}^2+\bar{y}^2\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2([\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2-\big(\frac{\bar{x}+\bar{y}}{2}\big)^2)+\bar{x}^2+\bar{y}^2\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2-2\big(\frac{\bar{x}+\bar{y}}{2}\big)^2+\bar{x}^2+\bar{y}^2\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2-\big(\frac{\bar{x}^2+2\bar{x}\bar{y}+\bar{y}^2}{2}\big)+2\frac{\bar{x}^2}{2}+2\frac{\bar{y}^2}{2}\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2-\frac{\bar{x}^2+2\bar{x}\bar{y}+\bar{y}^2-2\bar{x}^2-2\bar{y}^2}{2}\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2+\frac{\bar{x}^2-2\bar{x}\bar{y}+\bar{y}^2}{2}\\
(\mu-\bar{x})^2+(\mu-\bar{y})^2=&2[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2+\frac{(\bar{x}-\bar{y})^2}{2}
\end{array}
$$

Sustituyendo esta expresión en el denominador de la integral anterior, tenemos
$$
\begin{array}{rl}
\displaystyle \int[(\mu-\bar{x})^2+(\mu-\bar{y})^2+S^2]^{-n}d\mu=&\displaystyle \int \Bigg(2[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2+\frac{(\bar{x}-\bar{y})^2}{2}+S^2\Bigg)^{-n}d\mu\\
=&\displaystyle \int\bigg(2\bigg[ [\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2+\frac{(\bar{x}-\bar{y})^2}{4}+\frac{S^2}{2} \bigg]\bigg)^{-n}d\mu\\
=&2^{-n}\displaystyle \int\bigg( [\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2+\frac{(\bar{x}-\bar{y})^2}{4}+\frac{S^2}{2}\bigg)^{-n}d\mu
\end{array}
$$
Si $\nu=2n-1$ entonces $n=(\nu+1)/2$, sustituyendo
$$
\begin{array}{rl}
\displaystyle \int[(\mu-\bar{x})^2+(\mu-\bar{y})^2+S^2]^{-n}d\mu=&2^{-(\nu+1)/2}\displaystyle \int\bigg( [\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2+\frac{(\bar{x}-\bar{y})^2}{4}+\frac{S^2}{2}\bigg)^{-(\nu+1)/2}d\mu
\end{array}
$$

Si $\sigma^2=\frac{\frac{(\bar{x}-\bar{y})^2}{4}+\frac{S^2}{2}}{\nu}$ queda
$$
\begin{array}{rl}
& \displaystyle \int[(\mu-\bar{x})^2+(\mu-\bar{y})^2+S^2]^{-n}d\mu\\
=& 2^{-(\nu+1)/2}\displaystyle \int\left( [\mu- \left(\frac{\bar{x}+\bar{y}}{2}\right)]^2+\sigma^2\nu\right)^{-(\nu+1)/2}d\mu\\
=&2^{-(\nu+1)/2}\displaystyle \int\bigg( \sigma^2\nu\bigg[ \frac{[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2}{\sigma^2\nu}+1\bigg]\bigg)^{-(\nu+1)/2}d\mu\\
=&(2\sigma^2)^{-(\nu+1)/2}\displaystyle \int\nu^{-(\nu+1)/2}\bigg( \frac{[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2}{\sigma^2\nu}+1\bigg)^{-(\nu+1)/2}d\mu\\
=&(2\sigma^2)^{-(\nu+1)/2}\frac{\sigma\sqrt{\nu\pi}\ \Gamma\big(\frac{\nu}{2}\big)}{\Gamma\bigg(\frac{\nu+1}{2}\bigg)} \displaystyle \int \frac{\Gamma\bigg(\frac{\nu+1}{2}\bigg)}{\sigma\sqrt{\nu\pi}\ \Gamma\big(\frac{\nu}{2}\big)}\nu^{-(\nu+1)/2}\bigg( \frac{[\mu- \big(\frac{\bar{x}+\bar{y}}{2}\big)]^2}{\sigma^2\nu}+1\bigg)^{-(\nu+1)/2}d\mu\\
=&(2\sigma^2)^{-(\nu+1)/2}\frac{\sigma\sqrt{\nu\pi}\ \Gamma\big(\frac{\nu}{2}\big)}{\Gamma\bigg(\frac{\nu+1}{2}\bigg)}\\
=&(2\sigma^2)^{-n}\frac{\sigma\sqrt{\nu\pi}\ \Gamma\big(\frac{\nu}{2}\big)}{\Gamma\bigg(\frac{\nu+1}{2}\bigg)} \\
=&2^{-n}\sigma^{-2n+1}\frac{\sqrt{\nu\pi}\ \Gamma\big(\frac{\nu}{2}\big)}{\Gamma\bigg(\frac{\nu+1}{2}\bigg)} \\
\end{array}
$$

$$
\begin{array}{rl}
=&\displaystyle\frac{\sqrt{\nu\pi}\ \Gamma\big(\frac{\nu}{2}\big)}{2^{n}\sigma^{2n-1}\Gamma\bigg(\frac{\nu+1}{2}\bigg)}\\
=&\displaystyle\frac{\sqrt{\nu\pi}\ \Gamma\big(\frac{\nu}{2}\big)}{2^{n}\Bigg\{\sqrt{\frac{\frac{(\bar{x}-\bar{y})^2}{4}+\frac{S^2}{2}}{\nu}}   \Bigg\}^{2n-1}\Gamma\bigg(\frac{\nu+1}{2}\bigg)} \\
=&\displaystyle \frac{\nu^{n} \sqrt{\pi}\ \Gamma\big(\frac{\nu}{2}\big)}{2^{n}\Bigg\{\sqrt{\frac{(\bar{x}-\bar{y})^2}{4}+\frac{S^2}{2}}   \Bigg\}^{2n-1}\Gamma\bigg(\frac{\nu+1}{2}\bigg)} 
\end{array}
$$

-----

# 2.4 Método de Monte Carlos

Como se pudo observar en la sección anterior, el factor de Bayes y la probabilidad posterior son las únicas cantidades usadas en la evaluación de las hipótesis sobre el modelo, aunque su obtención analítica no siempre es posible. Ya que la misma requiere de la integración del parámetro ó sobre el conjunto $\Theta_0$ o su complemento $\Theta_0^c$, sobre las respectivas prioris $\pi_0$ y $\pi_1$. Sin embargo, existen técnicas numéricas especiales para calcular el factor de Bayes, las cuales se basan en razones de integrales.

Entre las técnicas más usadas para la aproximación de integrales, se encuentra el método de Monte Carlo, el cual se basa en la simulación de variables aleatorias para producir una aproximación de las integrales, cuyo valor converge a medida que el número de simulaciones aumenta. Su justificación se encuentra en la Ley de los grandes números, la cual establece que, dada un conjunto de variables aleatorias $x_1,...,x_n$ iid distribuidas según $g$, entonces el promedio empírico
$\hat{J} = \frac{(h(x_1) +... + h(x_n))}{n}$ converge (casi seguramente) a la integral
$$J = Eg [h(X)] = \int h(x)g(x)dx.$$

El algoritmo básico de este método se presenta acontinuación,

-Para $i = 1,...,n$

 Simular $x_i \sim g(x)$

-Calcular

 $\hat{J_n} = \frac{h(x_1) +...+ h(x_n)}{n}$

-Para aproximar el valor de $J$

Siempre y cuando la generación de los valores pseudo-aleatorios de $g$ sea factible, así como los valores de $h(x_i)$ sean calculables. Cuando la simulación mediante $g$ ocasiona problemas, ya sea porque dicha función es no estandar, las técnicas usuales como algoritmo de aceptación y rechazo son díciles de aplicar. Por tal razón otras técnicas más avanzadas son requeridas, tal es el caso de las Cadenas de Markov mediante el método de Monte Carlo (MCMC).
Por otra parte cuando la dícultad se centra en la intratabilidad de la función $h$, la solución usualmente es expandir las variables aleatorias $x_i$ en $(x_i,y_i)$, donde $y_i$ es una variable auxiliar.

Como se calculó en el ejercicio 2.21, el factor de Bayes $B_{01}^\pi = \frac{1}{B_{10}^\pi}$ se puede simplificar a la siguiente expresión:
$$
B_{01}^\pi = \frac{\displaystyle \int \left[ S^2 + (\mu-\bar{x})^2 +(\mu-\bar{y})^2\right]^{-n}d\mu}{\displaystyle  \int \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left[ S^2 + (\bar{x}-\mu+\xi)^2 +(\bar{y}-\mu-\xi)^2\right]^{-n} d\mu \ d\xi} 
$$
Resolviendo la integral del denominador:
$$
\begin{array}{rl}
\displaystyle  \int \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left[ S^2 + (\bar{x}-\mu+\xi)^2 +(\bar{y}-\mu-\xi)^2\right]^{-n} d\mu \ d\xi
\end{array}
$$

Primero desarrollaremos:
$$
\begin{array}{rl}
& (\bar{x}-\mu+\xi)^2 +(\bar{y}-\mu-\xi)^2\\
= & (\mu-\bar{x}-\xi)^2 +(\mu-\bar{y}+\xi)^2\\
= & (\mu-(\bar{x}+\xi))^2 +(\mu-(\bar{y}-\xi))^2\\
= & \mu^2-2\mu(\bar{x}+\xi)+(\bar{x}+\xi)^2 +\mu^2-2\mu(\bar{y}-\xi)+(\bar{y}-\xi)^2\\
= & 2\mu^2-2\mu(\bar{x}+\xi)-2\mu(\bar{y}-\xi)+(\bar{y}-\xi)^2+(\bar{x}+\xi)^2\\
= & 2\left( \mu^2-\mu(\bar{x}+\xi + \bar{y}-\xi)\right)+(\bar{y}-\xi)^2+(\bar{x}+\xi)^2\\
= & 2\left( \mu^2 - 2\mu\frac{\bar{x} + \bar{y}}{2} + \left(\frac{\bar{x} + \bar{y}}{2}\right)^2-\left(\frac{\bar{x} + \bar{y}}{2}\right)^2\right)+(\bar{y}-\xi)^2+(\bar{x}+\xi)^2\\
= & 2\left( \mu^2 - 2\mu\frac{\bar{x}+\bar{y}}{2} + \left(\frac{\bar{x}+ \bar{y}}{2} \right)^2\right)-2\left(\frac{\bar{x}+\bar{y}}{2}\right)^2 +(\bar{y}-\xi)^2+(\bar{x}+\xi)^2\\
= & 2\left( \mu -\frac{\bar{x}+\bar{y}}{2} \right)^2 -2\frac{(\bar{x}+\xi+\bar{y}+\xi)^2}{4} +(\bar{y}-\xi)^2+(\bar{x}+\xi)^2\\
= & 2\left( \mu -\frac{\bar{x}+\bar{y}}{2} \right)^2 -\frac{(\bar{x}+\bar{y})^2}{2} +(\bar{y}-\xi)^2+(\bar{x}+\xi)^2\\
= & 2\left( \mu -\frac{\bar{x}+\bar{y}}{2} \right)^2 -\frac{(\bar{x}+\bar{y})^2}{2} +(\bar{y}-\xi)^2+(\bar{x}+\xi)^2\\
\end{array}
$$

$$
\begin{array}{rl}
= & 2\left( \mu -\frac{\bar{x}+\bar{y}}{2} \right)^2 -\frac{(\bar{x}+\xi)^2+2(\bar{x}+\xi)(\bar{y}+\xi)+(\bar{y}+\xi)^2}{2} +(\bar{y}-\xi)^2+(\bar{x}+\xi)^2\\
= & 2\left( \mu -\frac{\bar{x}+\bar{y}}{2} \right)^2 -\frac{(\bar{x}+\xi)^2}{2}-2\frac{(\bar{x}+\xi)(\bar{y}+\xi)}{2}-\frac{(\bar{y}+\xi)^2}{2} +(\bar{y}-\xi)^2+(\bar{x}+\xi)^2\\
= & 2\left( \mu -\frac{\bar{x}+\bar{y}}{2} \right)^2 +\frac{(\bar{x}+\xi)^2}{2}-(\bar{x}+\xi)(\bar{y}+\xi)+\frac{(\bar{y}+\xi)^2}{2}\\
= & 2\left( \mu -\frac{\bar{x}+\bar{y}}{2} \right)^2 +\frac{1}{2}\left( (\bar{x}+\xi)^2-2(\bar{x}+\xi)(\bar{y}+\xi)+(\bar{y}+\xi)^2\right)\\
= & 2\left( \mu -\frac{\bar{x}+\bar{y}}{2} \right)^2 +\frac{1}{2}\left( \bar{x}+\xi+\bar{y}+\xi\right)^2\\
= & 2\left( \mu -\frac{\bar{x}+\bar{y}}{2} \right)^2 +\frac{1}{2}\left( \bar{x}+\bar{y}+2\xi\right)^2\\
\end{array}
$$

Sustituyendo ésto tenemos:
$$
\begin{array}{rl}
& \displaystyle \int\int \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left[ S^2 + 2\left( \mu -\frac{\bar{x}+\bar{y}+2\xi}{2} \right)^2 +\frac{1}{2}\left( \bar{x}+\bar{y}+2\xi\right)^2 \right]^{-n} d\mu \ d\xi\\
= & \displaystyle \int\int \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left[ 2\left( \left( \mu -\frac{\bar{x}+\bar{y}+2\xi}{2} \right)^2 +\frac{1}{4}\left( \bar{x}+\bar{y}+2\xi\right)^2 +\frac{S^2}{2}\right)\right]^{-n} d\mu \ d\xi\\
= & \displaystyle \int\int \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} 2^{-n} \left( \left( \mu -\frac{\bar{x}+\bar{y}+2\xi}{2} \right)^2 +\frac{1}{4}\left( \bar{x}+\bar{y}+2\xi\right)^2 +\frac{S^2}{2}\right)^{-n} d\mu \ d\xi\\
= & \displaystyle \int\frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} 2^{-n} \int  \left( \left( \mu -\frac{\bar{x}+\bar{y}+2\xi}{2} \right)^2 +\frac{1}{4}\left( \bar{x}+\bar{y}+2\xi\right)^2 +\frac{S^2}{2}\right)^{-n} d\mu \ d\xi\\
\end{array}
$$

Haciendo $\nu=2n-1$ y $\sigma^2=\frac{\frac{1}{4}\left( \bar{x}+\bar{y}+2\xi\right)^2 +\frac{S^2}{2}}{\nu}$
$$
\begin{array}{rl}
= & \displaystyle \int\frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} 2^{-n} \int  \left( \left( \mu -\frac{\bar{x}+\bar{y}+2\xi}{2} \right)^2 +\sigma^2\nu\right)^{-\frac{\nu+1}{2}} d\mu \ d\xi\\
= & \displaystyle \int\frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} 2^{-n} \int  \left( \sigma^2\nu \left( \frac{\left( \mu -\frac{\bar{x}+\bar{y}+2\xi}{2} \right)^2}{\sigma^2\nu} +1\right)\right)^{-\frac{\nu+1}{2}} d\mu \ d\xi\\
= & \displaystyle \int\frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} 2^{-n} \int  (\sigma^2\nu)^{-\frac{\nu+1}{2}}  \left(\frac{\left( \mu -\frac{\bar{x}+\bar{y}+2\xi}{2} \right)^2}{\sigma^2\nu} +1\right)^{-\frac{\nu+1}{2}} d\mu \ d\xi\\
= & \displaystyle \int\frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} 2^{-n}{\sigma^2}^{-\frac{\nu+1}{2}} \int  \nu^{-\frac{\nu+1}{2}}  \left(\frac{\left( \mu -\frac{\bar{x}+\bar{y}+2\xi}{2} \right)^2}{\sigma^2\nu} +1\right)^{-\frac{\nu+1}{2}} d\mu \ d\xi\\
= & \displaystyle \int\frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} 2^{-n}{\sigma^2}^{-\frac{\nu+1}{2}}  \frac{\Gamma(\frac{\nu}{2})\sigma\sqrt{\nu\pi}}{\Gamma(\frac{\nu+1}{2})} \int \frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\frac{\nu}{2})\sigma\sqrt{\nu\pi}} \nu^{-\frac{\nu+1}{2}}  \left(\frac{\left( \mu -\frac{\bar{x}+\bar{y}+2\xi}{2} \right)^2}{\sigma^2\nu} +1\right)^{-\frac{\nu+1}{2}} d\mu \ d\xi\\
= & \displaystyle \int\frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} 2^{-n}{\sigma^2}^{-\frac{\nu+1}{2}}  \frac{\Gamma(\frac{\nu}{2})\sigma\sqrt{\nu\pi}}{\Gamma(\frac{-\nu+1}{2})} d\xi\\
= & \frac{\Gamma(\frac{\nu}{2})\sqrt{\nu\pi}}{2^{n}\Gamma(\frac{\nu+1}{2})} \displaystyle \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \sigma {\sigma^2}^{-\frac{\nu+1}{2}} d\xi\\
= & \frac{\Gamma(\frac{\nu}{2})\sqrt{\nu\pi}}{2^{n}\Gamma(\frac{\nu+1}{2})} \displaystyle \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \sigma {\sigma}^{-\nu-1} d\xi\\
= & \frac{\Gamma(\frac{\nu}{2})\sqrt{\nu\pi}}{2^{n}\Gamma(\frac{\nu+1}{2})} \displaystyle \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \sigma^{-\nu} d\xi\\
\end{array}
$$
$$
\begin{array}{rl}
= & \frac{\Gamma(\frac{\nu}{2})\sqrt{\nu\pi}}{2^{n}\Gamma(\frac{\nu+1}{2})} \displaystyle \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \sqrt{\frac{\frac{1}{4}\left( \bar{x}+\bar{y}+2\xi\right)^2 +\frac{S^2}{2}}{\nu}}^{-\nu} d\xi\\
= & \frac{\Gamma(\frac{\nu}{2})\sqrt{\nu\pi}}{2^{n}\Gamma(\frac{\nu+1}{2})} \displaystyle \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left(\frac{\frac{1}{4}\left( \bar{x}+\bar{y}+2\xi\right)^2 +\frac{S^2}{2}}{\nu}\right)^{-\frac{\nu}{2}} d\xi\\
= & \frac{\Gamma(\frac{\nu}{2})\sqrt{\nu\pi}}{2^{n}\Gamma(\frac{\nu+1}{2})\nu^{-\frac{\nu}{2}}} \displaystyle \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left(\frac{1}{4}\left( \bar{x}+\bar{y}+2\xi\right)^2 +\frac{S^2}{2}\right)^{-\frac{\nu}{2}} d\xi\\
= & \frac{\Gamma(\frac{\nu}{2})\sqrt{\nu\pi}}{2^{n}\Gamma(\frac{\nu+1}{2})\nu^{-\frac{\nu}{2}}} \displaystyle \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left(\frac{1}{4}\left( \bar{x}+\bar{y}+2\xi\right)^2 +\frac{S^2}{2}\right)^{\frac{-2n+1}{2}} d\xi\\
= & \frac{\Gamma(\frac{\nu}{2})\sqrt{\nu\pi}}{2^{n}\Gamma(\frac{\nu+1}{2})\nu^{-\frac{\nu}{2}}} \displaystyle \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left(\frac{1}{4}\left( \bar{x}+\bar{y}+2\xi\right)^2 +\frac{S^2}{2}\right)^{-n+\frac{1}{2}} d\xi\\
= & \frac{\Gamma(\frac{\nu}{2})\sqrt{\nu\pi}}{2^{n}\Gamma(\frac{\nu+1}{2})\nu^{-\frac{\nu}{2}}} \displaystyle \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left(\frac{1}{4}\left( \bar{x}+\bar{y}+2\xi\right)^2 +\frac{S^2}{2}\right)^{-n+\frac{1}{2}} d\xi\\
= & \frac{\Gamma(\frac{\nu}{2})\sqrt{\nu\pi}}{2^{n}\Gamma(\frac{\nu+1}{2})\nu^{-\frac{2n-1}{2}}} \displaystyle \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left(\frac{1}{4}\left( \bar{x}+\bar{y}+2\xi\right)^2 +\frac{S^2}{2}\right)^{-n+\frac{1}{2}} d\xi\\
= & \frac{\Gamma(\frac{\nu}{2})\sqrt{\nu\pi}}{2^{n}\Gamma(\frac{\nu+1}{2})\nu^{-n+\frac{1}{2}}} \displaystyle \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left(\frac{1}{4}\left( \bar{x}+\bar{y}+2\xi\right)^2 +\frac{S^2}{2}\right)^{-n+\frac{1}{2}} d\xi\\
= & \frac{\Gamma(\frac{\nu}{2})\nu^n\sqrt{\pi}}{2^{n}\Gamma(\frac{\nu+1}{2})} \displaystyle \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left(\frac{1}{4}\left( \bar{x}+\bar{y}+2\xi\right)^2 +\frac{S^2}{2}\right)^{-n+\frac{1}{2}} d\xi\\
\end{array}
$$

Ahora usaremos el resultado obtenido en el ejercicio 2.21, sustituyendolo con el último resultado en el factor de bayes, tenemos:
$$
\begin{array}{rl}
B_{01}^\pi & = \frac{\frac{\nu^{n} \sqrt{\pi}\ \Gamma\big(\frac{\nu}{2}\big)}{2^{n}\Bigg\{\sqrt{\frac{(\bar{x}-\bar{y})^2}{4}+\frac{S^2}{2}}   \Bigg\}^{2n-1}\Gamma\bigg(\frac{\nu+1}{2}\bigg)}}{\frac{\Gamma(\frac{\nu}{2})\nu^n\sqrt{\pi}}{2^{n}\Gamma(\frac{\nu+1}{2})} \displaystyle \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left(\frac{1}{4}\left( \bar{x}+\bar{y}+2\xi\right)^2 +\frac{S^2}{2}\right)^{-n+\frac{1}{2}} d\xi}\\ 
B_{01}^\pi & = \frac{\frac{1}{\left(\sqrt{\frac{(\bar{x}-\bar{y})^2}{4}+\frac{S^2}{2}}   \right)^{2n-1}}}{\displaystyle \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left(\frac{1}{4}\left( \bar{x}+\bar{y}+2\xi\right)^2 +\frac{S^2}{2}\right)^{-n+\frac{1}{2}} d\xi}\\ 
B_{01}^\pi & = \frac{\left(\sqrt{\frac{(\bar{x}-\bar{y})^2}{4}+\frac{S^2}{2}} \right)^{-2n+1}}{\displaystyle \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left(\frac{1}{4}\left( \bar{x}+\bar{y}+2\xi\right)^2 +\frac{S^2}{2}\right)^{-n+\frac{1}{2}} d\xi}
\end{array}
$$

$$
\begin{array}{rl}
B_{01}^\pi & = \frac{\left(\frac{(\bar{x}-\bar{y})^2}{4}+\frac{S^2}{2} \right)^{-n+\frac{1}{2}}}{\displaystyle \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left(\frac{1}{4}\left( \bar{x}+\bar{y}+2\xi\right)^2 +\frac{S^2}{2}\right)^{-n+\frac{1}{2}} d\xi}\\
B_{01}^\pi & = \frac{\left(\frac{1}{4}\right)^{-n+\frac{1}{2}} \left((\bar{x}-\bar{y})^2+2S^2 \right)^{-n+\frac{1}{2}}}{\displaystyle \left(\frac{1}{4}\right)^{-n+\frac{1}{2}} \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left(\left( \bar{x}+\bar{y}+2\xi\right)^2 +2S^2\right)^{-n+\frac{1}{2}} d\xi}\\ 
B_{01}^\pi & = \displaystyle\frac{\left((\bar{x}-\bar{y})^2+2S^2 \right)^{-n+\frac{1}{2}}}{ \displaystyle \int  \frac{e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}} \left(\left( \bar{x}+\bar{y}+2\xi\right)^2 +2S^2\right)^{-n+\frac{1}{2}} d\xi}
\end{array} 
$$

Así considerando la integral en el denominador de la expresión anterior, podemos usar el método de Monte Carlo para selecionar las funciones $g$ y $h$, la primera está asociada con una distribución normal estandar, es decir $g \sim N(0, 1)$.





